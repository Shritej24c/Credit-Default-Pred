{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlWi0R627ieM",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuqEjjy2C5KI"
   },
   "source": [
    "# In this project we will build a model to estimate current credit status of a customer. \n",
    "\n",
    "In most applications, we would like to *predict* probability of default for a customer in the future, but we don't have data for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AhZLUazmfIKC",
    "outputId": "96fd47ac-2481-4fb1-9cf1-a86a0cc78718",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# this is to read data on Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXI_v-uE7qcg",
    "outputId": "e2f30f34-12e7-4199-de83-a33011ec093c",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# You can find the data here: https://www.kaggle.com/wordsforthewise/lending-club\n",
    "data=pd.read_csv(\"drive/My Drive/accepted.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZc7XjSF3szq"
   },
   "source": [
    "# Steps - compare the order of steps with those for XGBoost:\n",
    "1. Data Exploration - understand the data\n",
    "2. Preliminary feature exclusion - remove features that do not make sense, or can not be used\n",
    "3. Observation exclusion - to creat an unbiased sample that represents the target population and serves model's goal\n",
    "4. One-Hot Encoding\n",
    "5. Feature Engineering (not needed for this model)\n",
    "6. Test/Train split\n",
    "7. Normalization (not needed for tree-based models)\n",
    "8. Outlier Treatment (not needed for tree-based models)\n",
    "9. Missing Value Imputation (not needed for the XGBoost package we use)\n",
    "10. Feature reduction\n",
    "11. Grid search, and Bias/Variance analysis - Choose the final model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Im3NepEN7xUk"
   },
   "source": [
    "## 1. Data Exploration - the goal here is to know the data better\n",
    "\n",
    "**Note: This is a demo. Analysis has been done on only some of the attributes. In an actual project, all attributes, that make sense, should be analyzed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kDwozpFljSp",
    "outputId": "c7afe195-d9cd-4c07-eae9-e050e4644a70",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "zNh6UMXaC6UO",
    "outputId": "7c9139d6-7dcf-46df-db1f-76ba40e9733d",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADKIe7xOHu3y",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# remove invalid observations\n",
    "data = data[0:2260699]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "xYlP28k28Tb-",
    "outputId": "e3fff7e3-579b-4278-e96c-e0ee9a4e4b1d",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# frequently check your steps\n",
    "print (data.shape[0])\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zuG77HoYKZAL",
    "outputId": "676f84ec-c0a1-434b-d26e-3f0ee5d7d879",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check data types - objects imply non-numeric\n",
    "# note that sometimes numeric columns appear as Object, because of few non-numeric observations. Such as a character that may represent a special value.\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  \n",
    "    print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G3nL6yY-ziJ"
   },
   "source": [
    "## 2. Preliminary feature exclusion\n",
    "\n",
    "Exclude features that do not make sense or can not be used. For example, some features such as Gender can not be used in a Credit Risk model (fair lending practices).  What other features you can think of that can not be used?\n",
    "\n",
    "Here we will use a small subsample of features. In an actual project, more features would have been selected.\n",
    "\n",
    "ID will be used for data merge (needed in an actual project), loan_status will be used to define dependent variable, pymnt_plan and hardship_flag will be used to define exclusions. The rest of the variables will be used as independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gorhqJ3OAFz",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "final_data = data[[\"id\", \"emp_length\", \"loan_status\", \"pymnt_plan\", \"dti\", \"delinq_2yrs\"\n",
    ",\"fico_range_low\", \"fico_range_high\", \"inq_last_6mths\", \"mths_since_last_delinq\", \"revol_bal\",\n",
    "\"revol_util\", \"total_acc\", \"avg_cur_bal\", \"chargeoff_within_12_mths\", \"hardship_flag\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUIkeYt0OBrI",
    "outputId": "035548db-e69a-4513-d4b5-3ac96ae87a5f",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check the data frequently\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "HEvNeXHIOEdt",
    "outputId": "68302dbf-5752-481d-cbb3-520a85df92a4",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check the data frequently\n",
    "final_data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUenpv91A1E7"
   },
   "source": [
    "## 3. Observation Exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ej2seounB9hH",
    "outputId": "6562927f-8216-4abe-d2b0-c10ebdb0e206",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "final_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "sNIeE37_A9bY",
    "outputId": "93fdb7ac-3853-4194-8c08-57964070f160",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Remove observations under payment plan. Cases that are under payment plan, do not follow normal delinquency process. They often have weak profiles but are not tagged as\n",
    "# delinquent because they are under payment plan.\n",
    "\n",
    "final_data[[\"pymnt_plan\", \"id\"]].groupby([\"pymnt_plan\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yx6uCSjia-zQ",
    "outputId": "8bb22020-4c53-4d1b-8a97-a912a6634988",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "final_data = final_data[final_data.pymnt_plan != \"y\"]\n",
    "final_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "OcSaY1LTBEsY",
    "outputId": "90049473-1f86-470d-a63f-9fae08e613b5",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Remove observations under hardship flag, for the same reason as hardship flag.\n",
    "\n",
    "final_data[[\"hardship_flag\", \"id\"]].groupby([\"hardship_flag\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "welk1EsKCKdy",
    "outputId": "cdc2f81e-9a8e-485a-c60a-3aaa088de1d2",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "final_data = final_data[final_data.hardship_flag != \"Y\"]\n",
    "final_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "VwpNaLN2O2iH",
    "outputId": "cb86501f-1b96-40e7-96b7-4463a3ef2e16",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Last exclusions are related to the target variable. We intend to analyze the current credit status of customer, so we exclude inactive accounts.\n",
    "# One category of inactive is \"charged off\" accounts. These are customers who have defaulted previously, and so we have stopped tracking their credit\n",
    "# status (target variable). Their profile (independent variables) may have improved since they have been charged off, but the target variable \n",
    "# shows \"charged off\". Therefore for these customers, dependent and independent variables do not show the correct relationship.\n",
    "\n",
    "final_data[[\"loan_status\", \"id\"]].groupby([\"loan_status\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aFWMUoL7UmJN",
    "outputId": "3241b6d8-49fa-493c-d29a-1e549bd76d20",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Remove cases with missing loan status as well as inactive accounts. \n",
    "# Note that we often don't do missing imputation on Y variable. rather exclude those observations.\n",
    "final_data = final_data[final_data.loan_status != \"Charged Off\"]\n",
    "final_data = final_data[final_data.loan_status != \"Default\"]\n",
    "final_data = final_data[final_data.loan_status != \"Does not meet the credit policy. Status:Charged Off\"]\n",
    "final_data = final_data[final_data.loan_status != \"Does not meet the credit policy. Status:Fully Paid\"]\n",
    "final_data = final_data[final_data.loan_status != \"Fully Paid\"]\n",
    "final_data = final_data[final_data.loan_status.notnull()]\n",
    "\n",
    "final_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Yqb-qiGKXjl3",
    "outputId": "723c7e1f-904a-416d-eca1-283e766235f4",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check\n",
    "final_data[[\"loan_status\", \"id\"]].groupby([\"loan_status\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzrrAZdDXo6W",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define target variable based on \"loan_status\". We define everyone who is current or in grace priod as good (0), and others as bad (1).\n",
    "final_data['30+ Delinquent'] = np.where((final_data.loan_status == \"Current\") | \n",
    "                                        (final_data.loan_status == \"In Grace Period\"),0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "IuZfLB0N5Ace",
    "outputId": "24092fcf-734d-42f3-bb18-d089356dfeff",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check\n",
    "final_data[[\"30+ Delinquent\", \"id\"]].groupby([\"30+ Delinquent\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JRDeCqUXsdP",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# remove attributes that are not neede anymore\n",
    "final_data.drop(['loan_status', 'hardship_flag', \n",
    "                'pymnt_plan'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3Qqk3KhZ9_S"
   },
   "source": [
    "# Missing value imputation\n",
    "\n",
    "Missing value imputation should be done after Normalization. We are going to replace missing values by 0 (This is an \"ok\" approach for Neural Networks). We will also need to Normalize the data (Normalization is a necessary step for Neural Networks). Replacing missings with 0, before normalization, affects the normalization process. We prefer missing value imputation impact \"no other steps\" as much as possible. So we will normalize the data, then impute missing values. Note that normalization process leave missing values unchanged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0bvd8FIaDlg"
   },
   "source": [
    "# 4. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PE1eANecTkbb",
    "outputId": "7ae6859b-d4cb-4a91-818f-f9bfecf1a4e5",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# there is only one independent non-numerical variable we need to take care of: emp_length\n",
    "final_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "AQzTk9HRYY7M",
    "outputId": "63a01b2f-4c5c-4ffa-b5c3-e2283a770ffa",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check categories\n",
    "final_data[[\"emp_length\", \"id\"]].groupby([\"emp_length\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zJ4E5_HYcWa",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# we can do one-hot encoding on \"employment length\", but it is an ordinal, not categorical, variable. So we can just convert it to ordinal numbers.\n",
    "final_data['Employment_Length'] = np.nan\n",
    "final_data['Employment_Length'] = np.where(final_data.emp_length == \"< 1 year\", 0, final_data.Employment_Length)\n",
    "final_data['Employment_Length'] = np.where(final_data.emp_length == \"1 year\", 1, final_data.Employment_Length)\n",
    "final_data['Employment_Length'] = np.where(final_data.emp_length == \"2 years\", 2, final_data.Employment_Length)\n",
    "final_data['Employment_Length'] = np.where(final_data.emp_length == \"3 years\", 3, final_data.Employment_Length)\n",
    "final_data['Employment_Length'] = np.where(final_data.emp_length == \"4 years\", 4, final_data.Employment_Length)\n",
    "final_data['Employment_Length'] = np.where(final_data.emp_length == \"5 years\", 5, final_data.Employment_Length)\n",
    "final_data['Employment_Length'] = np.where(final_data.emp_length == \"6 years\", 6, final_data.Employment_Length)\n",
    "final_data['Employment_Length'] = np.where(final_data.emp_length == \"7 years\", 7, final_data.Employment_Length)\n",
    "final_data['Employment_Length'] = np.where(final_data.emp_length == \"8 years\", 8, final_data.Employment_Length)\n",
    "final_data['Employment_Length'] = np.where(final_data.emp_length == \"9 years\", 9, final_data.Employment_Length)\n",
    "final_data['Employment_Length'] = np.where(final_data.emp_length == \"10+ years\", 10, final_data.Employment_Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "l07rFQwdYgcZ",
    "outputId": "83353547-c6d9-4f24-a29a-236afcb9a6d9",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check\n",
    "final_data[[\"Employment_Length\", \"id\"]].groupby([\"Employment_Length\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idTxpMUsYjtO",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "final_data.drop(['emp_length'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRiNW1GDb8L7"
   },
   "source": [
    "# Outlier treatment - should be done after test/train split, and should be done based on the train sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKmsazHN7wsf"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# 5. Feature Engineering - not needed fro this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlSoBYkCZboq"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# 6. Test-Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw6fqomSZfLC",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# put 30% in test. This is a random split which is not ideal. Ideally we would like to split based on another variable, for example time. \n",
    "# Note that both test and train should be unbiased samples of the whole population.\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(final_data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkoZ_7lPa9l6",
    "outputId": "549c8cb3-fbab-4656-b7e7-6d2f9f5f021b",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SlLgIlzBfkIt",
    "outputId": "de6eef27-a821-4454-8973-2283ea4413b7",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check - it is a good practice to compare test and train samples to make sure they are not fundamentally different. \n",
    "# If so, we will get high variance even with a non-overfitted model.\n",
    "# Here we compare bad rate in both samples.\n",
    "print (sum(train[\"30+ Delinquent\"])/len(train[\"30+ Delinquent\"]))\n",
    "print (sum(test[\"30+ Delinquent\"])/len(test[\"30+ Delinquent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3T6WF6aaQFz",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define X and Y varibales to build the ensemble model. \n",
    "X_train = train.drop([\"id\", '30+ Delinquent'], axis = 1)\n",
    "Y_train = train['30+ Delinquent']\n",
    "\n",
    "X_test = test.drop([\"id\", '30+ Delinquent'], axis = 1)\n",
    "Y_test = test['30+ Delinquent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwJlw-Wgdg6A"
   },
   "source": [
    "# 7. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EC5Vu6dg5NUE",
    "outputId": "570f6734-f160-4054-9377-43483b068b7b",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-veVss7CPah",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_normalized = sc.transform(X_train)\n",
    "X_test_normalized = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMB9XtSCDwak",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# convert to Pandas DF\n",
    "X_train_normalized = pd.DataFrame(X_train_normalized, columns=X_train.columns)\n",
    "X_test_normalized = pd.DataFrame(X_test_normalized, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wzey3_FK-N5W"
   },
   "source": [
    "# 8. Outlier treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "HS0JkkFn1qKI",
    "outputId": "a0d9f70b-f630-4cc1-c8e8-cf8125f81d37",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# looking at the following table, seems like there are some outliers. One popular approach is to use 1 percentile as floor and 99 percentile as cap. \n",
    "# Howver it is not a written rule and depends on modeler's decision. Here we will cap \"dti\", \"delinq_2yrs\", \"revol_bal\", and \"avg_cur_bal\" at 99 percentile.\n",
    "X_train_normalized.describe(percentiles=[0.01, 0.99]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "MciW0z1r2-_m",
    "outputId": "b0ce72bc-39c3-47b6-bf89-150b7389bfd9",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_normalized['dti'] = np.where((X_train_normalized['dti'] > 2.294135), 2.294135, X_train_normalized['dti'])  \n",
    "X_train_normalized['delinq_2yrs'] = np.where((X_train_normalized['delinq_2yrs'] > 4.390682), 4.390682, X_train_normalized['delinq_2yrs'])  \n",
    "X_train_normalized['revol_bal'] = np.where((X_train_normalized['revol_bal'] > 3.583833), 3.583833\t, X_train_normalized['revol_bal'])  \n",
    "X_train_normalized['avg_cur_bal'] = np.where((X_train_normalized['avg_cur_bal'] > 3.710172), 3.710172, X_train_normalized['avg_cur_bal'])  \n",
    "\n",
    "\n",
    "X_train_normalized.describe(percentiles=[0.01, 0.99]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WbaJWkA4n0c"
   },
   "source": [
    "**Note: Any step you do during modeling process needs to be done on any future data to be passed to the model. This includes for example the above capping process. So for any future datasets, we will use the above tresholds to cap values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "RUK7m6NIAzOu",
    "outputId": "e34d4cc9-9e56-4071-b053-ba44053bf2c9",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_test_normalized['dti'] = np.where((X_test_normalized['dti'] > 2.294135), 2.294135, X_test_normalized['dti'])  \n",
    "X_test_normalized['delinq_2yrs'] = np.where((X_test_normalized['delinq_2yrs'] > 4.390682), 4.390682, X_test_normalized['delinq_2yrs'])  \n",
    "X_test_normalized['revol_bal'] = np.where((X_test_normalized['revol_bal'] > 3.583833), 3.583833\t, X_test_normalized['revol_bal'])  \n",
    "X_test_normalized['avg_cur_bal'] = np.where((X_test_normalized['avg_cur_bal'] > 3.710172), 3.710172, X_test_normalized['avg_cur_bal'])  \n",
    "\n",
    "\n",
    "X_test_normalized.describe(percentiles=[0.01, 0.99]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tY89LNawFXNV"
   },
   "source": [
    "# 9. Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pi69jyQHFV3W",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_normalized.fillna(0,inplace=True)\n",
    "X_test_normalized.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb4hAwiydpJ_"
   },
   "source": [
    "# 10. Feature Selection\n",
    "\n",
    "Before grid search, we should choose only a sub-sample of features that have predictive power. This will significantly increase speed of grid search, while we don't lose a lot of information. \n",
    "An effect approach is to buid a simple Ensemble model, and choose only features with feature importance higher than say 1%. There is no written prescription here, and it is up to modeler to choose the treshold.\n",
    "\n",
    "Note: Here we have few X variables. Feature selection is really not needed. It is done only for illustration.\n",
    "\n",
    "Note: For linear models, there are automated feature selection techniques (forward, backward, and stepwise), But even for those, it is beneficial to remove non-important features first, using this approach.\n",
    "\n",
    "Note: There are several techniques for feature selection (like for all other steps we discussed here). The discussion here is just an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJmeNHodib3K",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azO_55CFb0a-",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# run XGBoost\n",
    "\n",
    "xgb_instance = xgb.XGBClassifier(n_estimators=50) # nothing inside paranthesis, meaning we are using default parameters, with 100 trees.\n",
    "\n",
    "model_for_feature_selection = xgb_instance.fit(X_train_normalized, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "9TcYVdCYgtFd",
    "outputId": "4d628160-caac-4456-901e-908f2d6aafbb",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check the importances - you can also use SHAP values\n",
    "feature_importance = {'Feature':X_train_normalized.columns,'Importance':model_for_feature_selection.feature_importances_}\n",
    "feature_importance = pd.DataFrame(feature_importance)\n",
    "feature_importance.sort_values(\"Importance\", inplace=True,ascending=False)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSvfnfLHhiGH",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# choose featires with FI higher than 1%\n",
    "final_features = feature_importance[\"Feature\"][feature_importance.Importance > 0.01]\n",
    "\n",
    "X_train_normalized = X_train_normalized[final_features]\n",
    "X_test_normalized = X_test_normalized[final_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "yDV3Z7BhiKJK",
    "outputId": "086c39b8-df34-4ab9-cb85-ebedd365a21a",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check \n",
    "X_train_normalized.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "PtBgYoqNiVpy",
    "outputId": "b842573e-8133-416c-cf4b-26e292801cf5",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check\n",
    "X_test_normalized.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiEchrQ3Gnhq"
   },
   "source": [
    "# Build the model\n",
    "\n",
    "We wil build a sample NN, and will give a sample code for Grid Search. Figure out Grid Search and use it for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXmDiOObG-nU",
    "outputId": "003b5582-8b00-4169-c526-bb242655cee3",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Applications/anaconda3/lib/python3.8/site-packages (2.6.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: wheel~=0.35 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: clang~=5.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (4.22.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.53.0)\n",
      "Requirement already satisfied: keras~=2.6 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Applications/anaconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (6.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Applications/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vP7m3_n7HB3H",
    "outputId": "3952f4d8-e004-4eba-9d43-d55b5b1eaea2",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Applications/anaconda3/lib/python3.8/site-packages (2.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ryQAyePbHFL9",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-c69e9bb5db87>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mkeras\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodels\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSequential\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDense\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msys\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0m_sys\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 41\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtools\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmodule_util\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0m_module_util\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     42\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlazy_loader\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mLazyLoader\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0m_LazyLoader\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[0;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meager\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpywrap_tensorflow\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0m_pywrap_tensorflow\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msix\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfunction_pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotobuf\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mconfig_pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotobuf\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrewriter_config_pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/function_pb2.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mattr_value_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_attr__value__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnode_def_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_node__def__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mop_def_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_op__def__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/attr_value_pb2.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtensor_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_tensor__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtensor_shape_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtypes_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/tensor_pb2.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mresource_handle_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_resource__handle__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtensor_shape_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtypes_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/resource_handle_pb2.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtensor_shape_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtypes_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     34\u001B[0m   \u001B[0mcontaining_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m   fields=[\n\u001B[0;32m---> 36\u001B[0;31m     _descriptor.FieldDescriptor(\n\u001B[0m\u001B[1;32m     37\u001B[0m       \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'size'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfull_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'tensorflow.TensorShapeProto.Dim.size'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m       \u001B[0mnumber\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcpp_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Applications/anaconda3/lib/python3.8/site-packages/google/protobuf/descriptor.py\u001B[0m in \u001B[0;36m__new__\u001B[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001B[0m\n\u001B[1;32m    559\u001B[0m                 \u001B[0mhas_default_value\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontaining_oneof\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mjson_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    560\u001B[0m                 file=None, create_key=None):  # pylint: disable=redefined-builtin\n\u001B[0;32m--> 561\u001B[0;31m       \u001B[0m_message\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMessage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_CheckCalledFromGeneratedFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    562\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mis_extension\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    563\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_message\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdefault_pool\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFindExtensionByName\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfull_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z06oN63BHJJJ",
    "outputId": "1c6c10b2-c22f-44fe-ae72-8a64c80c4a70",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# We build a NN with two hidden layers, and 6 nodes in each hidden layer.\n",
    "\n",
    "# first step: create a Sequential object, as a sequence of layers. B/C NN is a sequence of layers.\n",
    "classifier = Sequential()\n",
    "\n",
    "# add the first hidden layer\n",
    "classifier.add(Dense(units=6,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))\n",
    "\n",
    "# add the second hidden layer\n",
    "classifier.add(Dense(units=6,kernel_initializer='glorot_uniform',\n",
    "                activation = 'relu'))\n",
    "\n",
    "# add the output layer\n",
    "classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'sigmoid'))\n",
    "\n",
    "# add additional parameters\n",
    "classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'FalseNegatives'])\n",
    "\n",
    "# train the model\n",
    "classifier.fit(X_train_normalized,Y_train,batch_size=1000,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ILFniXFH_Sy",
    "outputId": "fd723235-43a9-471d-e007-46c2ee53878a",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(Y_test, classifier.predict(X_test_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HeGwMhcvVZhn",
    "outputId": "da1e0035-83ee-492f-8fe3-ea6f8d5fc228",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(Y_train, classifier.predict(X_train_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyqa1qusiy1k"
   },
   "source": [
    "# Grid Search - Read this part, or look up grid search for NN on internet. Use it in your models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqLKzPbKi1RO",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# fine tuning with Grid Search\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def build_classifier(optimizer):\n",
    "    # first step: create a Sequential object, as a sequence of layers. B/C NN is a sequence of layers.\n",
    "    classifier = Sequential()\n",
    "    # add the first hidden layer\n",
    "    classifier.add(Dense(units=6,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))\n",
    "    # add the second hidden layer\n",
    "    classifier.add(Dense(units=6,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'relu'))\n",
    "    # add the output layer\n",
    "    classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',\n",
    "                    activation = 'sigmoid'))\n",
    "    # compiling the NN\n",
    "    classifier.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "classifier = KerasClassifier(build_fn=build_classifier)\n",
    "\n",
    "# create a dictionary of hyper-parameters to optimize\n",
    "parameters = {'batch_size':[1000,2000], 'nb_epoch':[20,10],'optimizer':['adam']}\n",
    "grid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = 'accuracy', cv=10)\n",
    "grid_search = grid_search.fit(X_train_normalized,Y_train)\n",
    "\n",
    "best_parameters = grid_search.best_params_ \n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}