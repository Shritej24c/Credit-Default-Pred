{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shritej24c/Credit-Risk/blob/kriti/XGB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-Pmg8Su6kEm",
        "outputId": "005eb58e-9604-460c-a125-96586b2fd69d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /Applications/anaconda3/lib/python3.8/site-packages (2.6.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.20.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.15.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: keras~=2.6 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: clang~=5.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (5.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.53.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (52.0.0.post20210125)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Applications/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Applications/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /Applications/anaconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (6.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Applications/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /Applications/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Applications/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Applications/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: keras in /Applications/anaconda3/lib/python3.8/site-packages (2.12.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKFgPk0O6kEp",
        "outputId": "b876d61b-3236-418a-cd81-c857fb035701"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: protobuf==3.20 in /Applications/anaconda3/lib/python3.8/site-packages (3.20.0)\r\n"
          ]
        }
      ],
      "source": [
        "!pip install protobuf==3.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOXGuPa36kEq"
      },
      "outputs": [],
      "source": [
        "!export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHMqGjkQ6kEr",
        "outputId": "2ee4d18c-1951-484a-f378-33cda23c5f5f"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'protobuf'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7a8f51706131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mprotobuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'protobuf'"
          ]
        }
      ],
      "source": [
        "import protobuf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaP-HSPz6kEr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCTw2seH6kEr",
        "outputId": "9d44f068-ef19-437f-c885-2ef591b08424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy==1.7 in /Applications/anaconda3/lib/python3.8/site-packages (1.7.0)\r\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /Applications/anaconda3/lib/python3.8/site-packages (from scipy==1.7) (1.19.5)\r\n"
          ]
        }
      ],
      "source": [
        "|!pip install scipy==1.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udsEQbL56kEs",
        "outputId": "77e9ac99-98dd-4c94-8d89-61e96540a0b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /Applications/anaconda3/lib/python3.8/site-packages (1.23.5)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.24.2-cp38-cp38-macosx_10_9_x86_64.whl (19.8 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.2\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "pctQ94WI6kEs"
      },
      "outputs": [],
      "source": [
        "!pip install dask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDq-RUmO6kEs"
      },
      "source": [
        "### Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeOvwhkA6kEt",
        "outputId": "c18c31a0-a22b-48c3-cba8-2c071094d82d"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "initialization failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;31mSystemError\u001b[0m: <built-in method __contains__ of dict object at 0x7f89daa165c0> returned a result with an error set",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-158-d6579f534729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonitoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/pywrap_tf_session.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pywrap_tf_session\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pywrap_tf_session\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_TF_SetTarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pywrap_tf_session\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_TF_SetConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: initialization failed"
          ]
        }
      ],
      "source": [
        "import tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eylDZ6526kEu",
        "outputId": "d3227f8c-628d-484c-91a3-b6bfdd587828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement protoc (from versions: none)\u001b[0m\u001b[31m\r\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for protoc\u001b[0m\u001b[31m\r\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install protoc >= 3.19.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "BmQyGITN6kEu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import time as t\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# fine tuning with Grid Search\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "Q4i2eAU96kEu"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCr_eOQm6kEv"
      },
      "source": [
        "### Reading CSVs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "XhcnCiIN6kEv",
        "outputId": "2559ee4c-319b-459f-cd44-5eaa91ef9566"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customer_ID</th>\n",
              "      <th>S_2</th>\n",
              "      <th>P_2</th>\n",
              "      <th>D_39</th>\n",
              "      <th>B_1</th>\n",
              "      <th>B_2</th>\n",
              "      <th>R_1</th>\n",
              "      <th>S_3</th>\n",
              "      <th>D_41</th>\n",
              "      <th>B_3</th>\n",
              "      <th>D_42</th>\n",
              "      <th>D_43</th>\n",
              "      <th>D_44</th>\n",
              "      <th>B_4</th>\n",
              "      <th>D_45</th>\n",
              "      <th>B_5</th>\n",
              "      <th>R_2</th>\n",
              "      <th>D_46</th>\n",
              "      <th>D_47</th>\n",
              "      <th>D_48</th>\n",
              "      <th>D_49</th>\n",
              "      <th>B_6</th>\n",
              "      <th>B_7</th>\n",
              "      <th>B_8</th>\n",
              "      <th>D_50</th>\n",
              "      <th>D_51</th>\n",
              "      <th>B_9</th>\n",
              "      <th>R_3</th>\n",
              "      <th>D_52</th>\n",
              "      <th>P_3</th>\n",
              "      <th>B_10</th>\n",
              "      <th>D_53</th>\n",
              "      <th>S_5</th>\n",
              "      <th>B_11</th>\n",
              "      <th>S_6</th>\n",
              "      <th>D_54</th>\n",
              "      <th>R_4</th>\n",
              "      <th>S_7</th>\n",
              "      <th>B_12</th>\n",
              "      <th>S_8</th>\n",
              "      <th>D_55</th>\n",
              "      <th>D_56</th>\n",
              "      <th>B_13</th>\n",
              "      <th>R_5</th>\n",
              "      <th>D_58</th>\n",
              "      <th>S_9</th>\n",
              "      <th>B_14</th>\n",
              "      <th>D_59</th>\n",
              "      <th>D_60</th>\n",
              "      <th>D_61</th>\n",
              "      <th>B_15</th>\n",
              "      <th>S_11</th>\n",
              "      <th>D_62</th>\n",
              "      <th>D_63</th>\n",
              "      <th>D_64</th>\n",
              "      <th>D_65</th>\n",
              "      <th>B_16</th>\n",
              "      <th>B_17</th>\n",
              "      <th>B_18</th>\n",
              "      <th>B_19</th>\n",
              "      <th>D_66</th>\n",
              "      <th>B_20</th>\n",
              "      <th>D_68</th>\n",
              "      <th>S_12</th>\n",
              "      <th>R_6</th>\n",
              "      <th>S_13</th>\n",
              "      <th>B_21</th>\n",
              "      <th>D_69</th>\n",
              "      <th>B_22</th>\n",
              "      <th>D_70</th>\n",
              "      <th>D_71</th>\n",
              "      <th>D_72</th>\n",
              "      <th>S_15</th>\n",
              "      <th>B_23</th>\n",
              "      <th>D_73</th>\n",
              "      <th>P_4</th>\n",
              "      <th>D_74</th>\n",
              "      <th>D_75</th>\n",
              "      <th>D_76</th>\n",
              "      <th>B_24</th>\n",
              "      <th>R_7</th>\n",
              "      <th>D_77</th>\n",
              "      <th>B_25</th>\n",
              "      <th>B_26</th>\n",
              "      <th>D_78</th>\n",
              "      <th>D_79</th>\n",
              "      <th>R_8</th>\n",
              "      <th>R_9</th>\n",
              "      <th>S_16</th>\n",
              "      <th>D_80</th>\n",
              "      <th>R_10</th>\n",
              "      <th>R_11</th>\n",
              "      <th>B_27</th>\n",
              "      <th>D_81</th>\n",
              "      <th>D_82</th>\n",
              "      <th>S_17</th>\n",
              "      <th>R_12</th>\n",
              "      <th>B_28</th>\n",
              "      <th>R_13</th>\n",
              "      <th>D_83</th>\n",
              "      <th>R_14</th>\n",
              "      <th>R_15</th>\n",
              "      <th>D_84</th>\n",
              "      <th>R_16</th>\n",
              "      <th>B_29</th>\n",
              "      <th>B_30</th>\n",
              "      <th>S_18</th>\n",
              "      <th>D_86</th>\n",
              "      <th>D_87</th>\n",
              "      <th>R_17</th>\n",
              "      <th>R_18</th>\n",
              "      <th>D_88</th>\n",
              "      <th>B_31</th>\n",
              "      <th>S_19</th>\n",
              "      <th>R_19</th>\n",
              "      <th>B_32</th>\n",
              "      <th>S_20</th>\n",
              "      <th>R_20</th>\n",
              "      <th>R_21</th>\n",
              "      <th>B_33</th>\n",
              "      <th>D_89</th>\n",
              "      <th>R_22</th>\n",
              "      <th>R_23</th>\n",
              "      <th>D_91</th>\n",
              "      <th>D_92</th>\n",
              "      <th>D_93</th>\n",
              "      <th>D_94</th>\n",
              "      <th>R_24</th>\n",
              "      <th>R_25</th>\n",
              "      <th>D_96</th>\n",
              "      <th>S_22</th>\n",
              "      <th>S_23</th>\n",
              "      <th>S_24</th>\n",
              "      <th>S_25</th>\n",
              "      <th>S_26</th>\n",
              "      <th>D_102</th>\n",
              "      <th>D_103</th>\n",
              "      <th>D_104</th>\n",
              "      <th>D_105</th>\n",
              "      <th>D_106</th>\n",
              "      <th>D_107</th>\n",
              "      <th>B_36</th>\n",
              "      <th>B_37</th>\n",
              "      <th>R_26</th>\n",
              "      <th>R_27</th>\n",
              "      <th>B_38</th>\n",
              "      <th>D_108</th>\n",
              "      <th>D_109</th>\n",
              "      <th>D_110</th>\n",
              "      <th>D_111</th>\n",
              "      <th>B_39</th>\n",
              "      <th>D_112</th>\n",
              "      <th>B_40</th>\n",
              "      <th>S_27</th>\n",
              "      <th>D_113</th>\n",
              "      <th>D_114</th>\n",
              "      <th>D_115</th>\n",
              "      <th>D_116</th>\n",
              "      <th>D_117</th>\n",
              "      <th>D_118</th>\n",
              "      <th>D_119</th>\n",
              "      <th>D_120</th>\n",
              "      <th>D_121</th>\n",
              "      <th>D_122</th>\n",
              "      <th>D_123</th>\n",
              "      <th>D_124</th>\n",
              "      <th>D_125</th>\n",
              "      <th>D_126</th>\n",
              "      <th>D_127</th>\n",
              "      <th>D_128</th>\n",
              "      <th>D_129</th>\n",
              "      <th>B_41</th>\n",
              "      <th>B_42</th>\n",
              "      <th>D_130</th>\n",
              "      <th>D_131</th>\n",
              "      <th>D_132</th>\n",
              "      <th>D_133</th>\n",
              "      <th>R_28</th>\n",
              "      <th>D_134</th>\n",
              "      <th>D_135</th>\n",
              "      <th>D_136</th>\n",
              "      <th>D_137</th>\n",
              "      <th>D_138</th>\n",
              "      <th>D_139</th>\n",
              "      <th>D_140</th>\n",
              "      <th>D_141</th>\n",
              "      <th>D_142</th>\n",
              "      <th>D_143</th>\n",
              "      <th>D_144</th>\n",
              "      <th>D_145</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
              "      <td>2017-03-09</td>\n",
              "      <td>0.938469</td>\n",
              "      <td>0.001733</td>\n",
              "      <td>0.008724</td>\n",
              "      <td>1.006838</td>\n",
              "      <td>0.009228</td>\n",
              "      <td>0.124035</td>\n",
              "      <td>0.008771</td>\n",
              "      <td>0.004709</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.080986</td>\n",
              "      <td>0.708906</td>\n",
              "      <td>0.170600</td>\n",
              "      <td>0.006204</td>\n",
              "      <td>0.358587</td>\n",
              "      <td>0.525351</td>\n",
              "      <td>0.255736</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.063902</td>\n",
              "      <td>0.059416</td>\n",
              "      <td>0.006466</td>\n",
              "      <td>0.148698</td>\n",
              "      <td>1.335856</td>\n",
              "      <td>0.008207</td>\n",
              "      <td>0.001423</td>\n",
              "      <td>0.207334</td>\n",
              "      <td>0.736463</td>\n",
              "      <td>0.096219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.023381</td>\n",
              "      <td>0.002768</td>\n",
              "      <td>0.008322</td>\n",
              "      <td>1.001519</td>\n",
              "      <td>0.008298</td>\n",
              "      <td>0.161345</td>\n",
              "      <td>0.148266</td>\n",
              "      <td>0.922998</td>\n",
              "      <td>0.354596</td>\n",
              "      <td>0.152025</td>\n",
              "      <td>0.118075</td>\n",
              "      <td>0.001882</td>\n",
              "      <td>0.158612</td>\n",
              "      <td>0.065728</td>\n",
              "      <td>0.018385</td>\n",
              "      <td>0.063646</td>\n",
              "      <td>0.199617</td>\n",
              "      <td>0.308233</td>\n",
              "      <td>0.016361</td>\n",
              "      <td>0.401619</td>\n",
              "      <td>0.091071</td>\n",
              "      <td>CR</td>\n",
              "      <td>O</td>\n",
              "      <td>0.007126</td>\n",
              "      <td>0.007665</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.652984</td>\n",
              "      <td>0.008520</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004730</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.272008</td>\n",
              "      <td>0.008363</td>\n",
              "      <td>0.515222</td>\n",
              "      <td>0.002644</td>\n",
              "      <td>0.009013</td>\n",
              "      <td>0.004808</td>\n",
              "      <td>0.008342</td>\n",
              "      <td>0.119403</td>\n",
              "      <td>0.004802</td>\n",
              "      <td>0.108271</td>\n",
              "      <td>0.050882</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007554</td>\n",
              "      <td>0.080422</td>\n",
              "      <td>0.069067</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004327</td>\n",
              "      <td>0.007562</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007729</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.001576</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>0.001434</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002271</td>\n",
              "      <td>0.004061</td>\n",
              "      <td>0.007121</td>\n",
              "      <td>0.002456</td>\n",
              "      <td>0.002310</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>0.506612</td>\n",
              "      <td>0.008033</td>\n",
              "      <td>1.009825</td>\n",
              "      <td>0.084683</td>\n",
              "      <td>0.003820</td>\n",
              "      <td>0.007043</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.006452</td>\n",
              "      <td>0.000830</td>\n",
              "      <td>0.005055</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005720</td>\n",
              "      <td>0.007084</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.008907</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002537</td>\n",
              "      <td>0.005177</td>\n",
              "      <td>0.006626</td>\n",
              "      <td>0.009705</td>\n",
              "      <td>0.007782</td>\n",
              "      <td>0.002450</td>\n",
              "      <td>1.001101</td>\n",
              "      <td>0.002665</td>\n",
              "      <td>0.007479</td>\n",
              "      <td>0.006893</td>\n",
              "      <td>1.503673</td>\n",
              "      <td>1.006133</td>\n",
              "      <td>0.003569</td>\n",
              "      <td>0.008871</td>\n",
              "      <td>0.003950</td>\n",
              "      <td>0.003647</td>\n",
              "      <td>0.004950</td>\n",
              "      <td>0.894090</td>\n",
              "      <td>0.135561</td>\n",
              "      <td>0.911191</td>\n",
              "      <td>0.974539</td>\n",
              "      <td>0.001243</td>\n",
              "      <td>0.766688</td>\n",
              "      <td>1.008691</td>\n",
              "      <td>1.004587</td>\n",
              "      <td>0.893734</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.670041</td>\n",
              "      <td>0.009968</td>\n",
              "      <td>0.004572</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.008949</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004326</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.007336</td>\n",
              "      <td>0.210060</td>\n",
              "      <td>0.676922</td>\n",
              "      <td>0.007871</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.238250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.232120</td>\n",
              "      <td>0.236266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.702280</td>\n",
              "      <td>0.434345</td>\n",
              "      <td>0.003057</td>\n",
              "      <td>0.686516</td>\n",
              "      <td>0.008740</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.003319</td>\n",
              "      <td>1.007819</td>\n",
              "      <td>1.000080</td>\n",
              "      <td>0.006805</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002052</td>\n",
              "      <td>0.005972</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004345</td>\n",
              "      <td>0.001535</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002427</td>\n",
              "      <td>0.003706</td>\n",
              "      <td>0.003818</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000569</td>\n",
              "      <td>0.000610</td>\n",
              "      <td>0.002674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
              "      <td>2017-04-07</td>\n",
              "      <td>0.936665</td>\n",
              "      <td>0.005775</td>\n",
              "      <td>0.004923</td>\n",
              "      <td>1.000653</td>\n",
              "      <td>0.006151</td>\n",
              "      <td>0.126750</td>\n",
              "      <td>0.000798</td>\n",
              "      <td>0.002714</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002526</td>\n",
              "      <td>0.069419</td>\n",
              "      <td>0.712795</td>\n",
              "      <td>0.113239</td>\n",
              "      <td>0.006206</td>\n",
              "      <td>0.353630</td>\n",
              "      <td>0.521311</td>\n",
              "      <td>0.223329</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.065261</td>\n",
              "      <td>0.057744</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>0.149723</td>\n",
              "      <td>1.339794</td>\n",
              "      <td>0.008373</td>\n",
              "      <td>0.001984</td>\n",
              "      <td>0.202778</td>\n",
              "      <td>0.720886</td>\n",
              "      <td>0.099804</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.030599</td>\n",
              "      <td>0.002749</td>\n",
              "      <td>0.002482</td>\n",
              "      <td>1.009033</td>\n",
              "      <td>0.005136</td>\n",
              "      <td>0.140951</td>\n",
              "      <td>0.143530</td>\n",
              "      <td>0.919414</td>\n",
              "      <td>0.326757</td>\n",
              "      <td>0.156201</td>\n",
              "      <td>0.118737</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.148459</td>\n",
              "      <td>0.093935</td>\n",
              "      <td>0.013035</td>\n",
              "      <td>0.065501</td>\n",
              "      <td>0.151387</td>\n",
              "      <td>0.265026</td>\n",
              "      <td>0.017688</td>\n",
              "      <td>0.406326</td>\n",
              "      <td>0.086805</td>\n",
              "      <td>CR</td>\n",
              "      <td>O</td>\n",
              "      <td>0.002413</td>\n",
              "      <td>0.007148</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.647093</td>\n",
              "      <td>0.002238</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.003879</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.188970</td>\n",
              "      <td>0.004030</td>\n",
              "      <td>0.509048</td>\n",
              "      <td>0.004193</td>\n",
              "      <td>0.007842</td>\n",
              "      <td>0.001283</td>\n",
              "      <td>0.006524</td>\n",
              "      <td>0.140611</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.101018</td>\n",
              "      <td>0.040469</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004832</td>\n",
              "      <td>0.081413</td>\n",
              "      <td>0.074166</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004203</td>\n",
              "      <td>0.005304</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001864</td>\n",
              "      <td>0.000979</td>\n",
              "      <td>0.009896</td>\n",
              "      <td>0.007597</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009810</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.005966</td>\n",
              "      <td>0.000395</td>\n",
              "      <td>0.001327</td>\n",
              "      <td>0.007773</td>\n",
              "      <td>0.500855</td>\n",
              "      <td>0.000760</td>\n",
              "      <td>1.009461</td>\n",
              "      <td>0.081843</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>0.007789</td>\n",
              "      <td>0.004311</td>\n",
              "      <td>0.002332</td>\n",
              "      <td>0.009469</td>\n",
              "      <td>0.003753</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007584</td>\n",
              "      <td>0.006677</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001142</td>\n",
              "      <td>0.005907</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.008427</td>\n",
              "      <td>0.008979</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>0.009924</td>\n",
              "      <td>0.005987</td>\n",
              "      <td>0.002247</td>\n",
              "      <td>1.006779</td>\n",
              "      <td>0.002508</td>\n",
              "      <td>0.006827</td>\n",
              "      <td>0.002837</td>\n",
              "      <td>1.503577</td>\n",
              "      <td>1.005791</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>0.008351</td>\n",
              "      <td>0.008850</td>\n",
              "      <td>0.003180</td>\n",
              "      <td>0.902135</td>\n",
              "      <td>0.136333</td>\n",
              "      <td>0.919876</td>\n",
              "      <td>0.975624</td>\n",
              "      <td>0.004561</td>\n",
              "      <td>0.786007</td>\n",
              "      <td>1.000084</td>\n",
              "      <td>1.004118</td>\n",
              "      <td>0.906841</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.668647</td>\n",
              "      <td>0.003921</td>\n",
              "      <td>0.004654</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.003205</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.008707</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.007653</td>\n",
              "      <td>0.184093</td>\n",
              "      <td>0.822281</td>\n",
              "      <td>0.003444</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.247217</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.243532</td>\n",
              "      <td>0.241885</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.707017</td>\n",
              "      <td>0.430501</td>\n",
              "      <td>0.001306</td>\n",
              "      <td>0.686414</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.008394</td>\n",
              "      <td>1.004333</td>\n",
              "      <td>1.008344</td>\n",
              "      <td>0.004407</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001034</td>\n",
              "      <td>0.004838</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007495</td>\n",
              "      <td>0.004931</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.003954</td>\n",
              "      <td>0.003167</td>\n",
              "      <td>0.005032</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009576</td>\n",
              "      <td>0.005492</td>\n",
              "      <td>0.009217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
              "      <td>2017-05-28</td>\n",
              "      <td>0.954180</td>\n",
              "      <td>0.091505</td>\n",
              "      <td>0.021655</td>\n",
              "      <td>1.009672</td>\n",
              "      <td>0.006815</td>\n",
              "      <td>0.123977</td>\n",
              "      <td>0.007598</td>\n",
              "      <td>0.009423</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007605</td>\n",
              "      <td>0.068839</td>\n",
              "      <td>0.720884</td>\n",
              "      <td>0.060492</td>\n",
              "      <td>0.003259</td>\n",
              "      <td>0.334650</td>\n",
              "      <td>0.524568</td>\n",
              "      <td>0.189424</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.066982</td>\n",
              "      <td>0.056647</td>\n",
              "      <td>0.005126</td>\n",
              "      <td>0.151955</td>\n",
              "      <td>1.337179</td>\n",
              "      <td>0.009355</td>\n",
              "      <td>0.007426</td>\n",
              "      <td>0.206629</td>\n",
              "      <td>0.738044</td>\n",
              "      <td>0.134073</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.048367</td>\n",
              "      <td>0.010077</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>1.009184</td>\n",
              "      <td>0.006961</td>\n",
              "      <td>0.112229</td>\n",
              "      <td>0.137014</td>\n",
              "      <td>1.001977</td>\n",
              "      <td>0.304124</td>\n",
              "      <td>0.153795</td>\n",
              "      <td>0.114534</td>\n",
              "      <td>0.006328</td>\n",
              "      <td>0.139504</td>\n",
              "      <td>0.084757</td>\n",
              "      <td>0.056653</td>\n",
              "      <td>0.070607</td>\n",
              "      <td>0.305883</td>\n",
              "      <td>0.212165</td>\n",
              "      <td>0.063955</td>\n",
              "      <td>0.406768</td>\n",
              "      <td>0.094001</td>\n",
              "      <td>CR</td>\n",
              "      <td>O</td>\n",
              "      <td>0.001878</td>\n",
              "      <td>0.003636</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.645819</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004578</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.495308</td>\n",
              "      <td>0.006838</td>\n",
              "      <td>0.679257</td>\n",
              "      <td>0.001337</td>\n",
              "      <td>0.006025</td>\n",
              "      <td>0.009393</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>0.075868</td>\n",
              "      <td>0.007152</td>\n",
              "      <td>0.103239</td>\n",
              "      <td>0.047454</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.006561</td>\n",
              "      <td>0.078891</td>\n",
              "      <td>0.076510</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001782</td>\n",
              "      <td>0.001422</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005419</td>\n",
              "      <td>0.006149</td>\n",
              "      <td>0.009629</td>\n",
              "      <td>0.003094</td>\n",
              "      <td>0.008295</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009362</td>\n",
              "      <td>0.000954</td>\n",
              "      <td>0.005447</td>\n",
              "      <td>0.007345</td>\n",
              "      <td>0.007624</td>\n",
              "      <td>0.008811</td>\n",
              "      <td>0.504606</td>\n",
              "      <td>0.004056</td>\n",
              "      <td>1.004291</td>\n",
              "      <td>0.081954</td>\n",
              "      <td>0.002709</td>\n",
              "      <td>0.004093</td>\n",
              "      <td>0.007139</td>\n",
              "      <td>0.008358</td>\n",
              "      <td>0.002325</td>\n",
              "      <td>0.007381</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005901</td>\n",
              "      <td>0.001185</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.008013</td>\n",
              "      <td>0.008882</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.007327</td>\n",
              "      <td>0.002016</td>\n",
              "      <td>0.008686</td>\n",
              "      <td>0.008446</td>\n",
              "      <td>0.007291</td>\n",
              "      <td>0.007794</td>\n",
              "      <td>1.001014</td>\n",
              "      <td>0.009634</td>\n",
              "      <td>0.009820</td>\n",
              "      <td>0.005080</td>\n",
              "      <td>1.503359</td>\n",
              "      <td>1.005801</td>\n",
              "      <td>0.007425</td>\n",
              "      <td>0.009234</td>\n",
              "      <td>0.002471</td>\n",
              "      <td>0.009769</td>\n",
              "      <td>0.005433</td>\n",
              "      <td>0.939654</td>\n",
              "      <td>0.134938</td>\n",
              "      <td>0.958699</td>\n",
              "      <td>0.974067</td>\n",
              "      <td>0.011736</td>\n",
              "      <td>0.806840</td>\n",
              "      <td>1.003014</td>\n",
              "      <td>1.009285</td>\n",
              "      <td>0.928719</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.670901</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>0.019176</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000754</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004092</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.004312</td>\n",
              "      <td>0.154837</td>\n",
              "      <td>0.853498</td>\n",
              "      <td>0.003269</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.239867</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.240768</td>\n",
              "      <td>0.239710</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.704843</td>\n",
              "      <td>0.434409</td>\n",
              "      <td>0.003954</td>\n",
              "      <td>0.690101</td>\n",
              "      <td>0.009617</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.009307</td>\n",
              "      <td>1.007831</td>\n",
              "      <td>1.006878</td>\n",
              "      <td>0.003221</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005681</td>\n",
              "      <td>0.005497</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009227</td>\n",
              "      <td>0.009123</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.003269</td>\n",
              "      <td>0.007329</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.003429</td>\n",
              "      <td>0.006986</td>\n",
              "      <td>0.002603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
              "      <td>2017-06-13</td>\n",
              "      <td>0.960384</td>\n",
              "      <td>0.002455</td>\n",
              "      <td>0.013683</td>\n",
              "      <td>1.002700</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.117169</td>\n",
              "      <td>0.000685</td>\n",
              "      <td>0.005531</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.006406</td>\n",
              "      <td>0.055630</td>\n",
              "      <td>0.723997</td>\n",
              "      <td>0.166782</td>\n",
              "      <td>0.009918</td>\n",
              "      <td>0.323271</td>\n",
              "      <td>0.530929</td>\n",
              "      <td>0.135586</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.083720</td>\n",
              "      <td>0.049253</td>\n",
              "      <td>0.001418</td>\n",
              "      <td>0.151219</td>\n",
              "      <td>1.339909</td>\n",
              "      <td>0.006782</td>\n",
              "      <td>0.003515</td>\n",
              "      <td>0.208214</td>\n",
              "      <td>0.741813</td>\n",
              "      <td>0.134437</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.030063</td>\n",
              "      <td>0.009667</td>\n",
              "      <td>0.000783</td>\n",
              "      <td>1.007456</td>\n",
              "      <td>0.008706</td>\n",
              "      <td>0.102838</td>\n",
              "      <td>0.129017</td>\n",
              "      <td>0.704016</td>\n",
              "      <td>0.275055</td>\n",
              "      <td>0.155772</td>\n",
              "      <td>0.120740</td>\n",
              "      <td>0.004980</td>\n",
              "      <td>0.138100</td>\n",
              "      <td>0.048382</td>\n",
              "      <td>0.012498</td>\n",
              "      <td>0.065926</td>\n",
              "      <td>0.273553</td>\n",
              "      <td>0.204300</td>\n",
              "      <td>0.022732</td>\n",
              "      <td>0.405175</td>\n",
              "      <td>0.094854</td>\n",
              "      <td>CR</td>\n",
              "      <td>O</td>\n",
              "      <td>0.005899</td>\n",
              "      <td>0.005896</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.654358</td>\n",
              "      <td>0.005897</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005207</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.508670</td>\n",
              "      <td>0.008183</td>\n",
              "      <td>0.515282</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>0.005271</td>\n",
              "      <td>0.004554</td>\n",
              "      <td>0.002052</td>\n",
              "      <td>0.150209</td>\n",
              "      <td>0.005364</td>\n",
              "      <td>0.206394</td>\n",
              "      <td>0.031705</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009559</td>\n",
              "      <td>0.077490</td>\n",
              "      <td>0.071547</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005595</td>\n",
              "      <td>0.006363</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000646</td>\n",
              "      <td>0.009193</td>\n",
              "      <td>0.008568</td>\n",
              "      <td>0.003895</td>\n",
              "      <td>0.005153</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004876</td>\n",
              "      <td>0.005665</td>\n",
              "      <td>0.001888</td>\n",
              "      <td>0.004961</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.004652</td>\n",
              "      <td>0.508998</td>\n",
              "      <td>0.006969</td>\n",
              "      <td>1.004728</td>\n",
              "      <td>0.060634</td>\n",
              "      <td>0.009982</td>\n",
              "      <td>0.008817</td>\n",
              "      <td>0.008690</td>\n",
              "      <td>0.007364</td>\n",
              "      <td>0.005924</td>\n",
              "      <td>0.008802</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>0.003324</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009455</td>\n",
              "      <td>0.008348</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.007053</td>\n",
              "      <td>0.003909</td>\n",
              "      <td>0.002478</td>\n",
              "      <td>0.006614</td>\n",
              "      <td>0.009977</td>\n",
              "      <td>0.007686</td>\n",
              "      <td>1.002775</td>\n",
              "      <td>0.007791</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.007320</td>\n",
              "      <td>1.503701</td>\n",
              "      <td>1.007036</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>0.008507</td>\n",
              "      <td>0.004858</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.913205</td>\n",
              "      <td>0.140058</td>\n",
              "      <td>0.926341</td>\n",
              "      <td>0.975499</td>\n",
              "      <td>0.007571</td>\n",
              "      <td>0.808214</td>\n",
              "      <td>1.001517</td>\n",
              "      <td>1.004514</td>\n",
              "      <td>0.935383</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.672620</td>\n",
              "      <td>0.002729</td>\n",
              "      <td>0.011720</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.005338</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009703</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.002538</td>\n",
              "      <td>0.153939</td>\n",
              "      <td>0.844667</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.240910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.239400</td>\n",
              "      <td>0.240727</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.711546</td>\n",
              "      <td>0.436903</td>\n",
              "      <td>0.005135</td>\n",
              "      <td>0.687779</td>\n",
              "      <td>0.004649</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.001671</td>\n",
              "      <td>1.003460</td>\n",
              "      <td>1.007573</td>\n",
              "      <td>0.007703</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007108</td>\n",
              "      <td>0.008261</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007206</td>\n",
              "      <td>0.002409</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.006117</td>\n",
              "      <td>0.004516</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.008419</td>\n",
              "      <td>0.006527</td>\n",
              "      <td>0.009600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
              "      <td>2017-07-16</td>\n",
              "      <td>0.947248</td>\n",
              "      <td>0.002483</td>\n",
              "      <td>0.015193</td>\n",
              "      <td>1.000727</td>\n",
              "      <td>0.007605</td>\n",
              "      <td>0.117325</td>\n",
              "      <td>0.004653</td>\n",
              "      <td>0.009312</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007731</td>\n",
              "      <td>0.038862</td>\n",
              "      <td>0.720619</td>\n",
              "      <td>0.143630</td>\n",
              "      <td>0.006667</td>\n",
              "      <td>0.231009</td>\n",
              "      <td>0.529305</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.075900</td>\n",
              "      <td>0.048918</td>\n",
              "      <td>0.001199</td>\n",
              "      <td>0.154026</td>\n",
              "      <td>1.341735</td>\n",
              "      <td>0.000519</td>\n",
              "      <td>0.001362</td>\n",
              "      <td>0.205468</td>\n",
              "      <td>0.691986</td>\n",
              "      <td>0.121518</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.054221</td>\n",
              "      <td>0.009484</td>\n",
              "      <td>0.006698</td>\n",
              "      <td>1.003738</td>\n",
              "      <td>0.003846</td>\n",
              "      <td>0.094311</td>\n",
              "      <td>0.129539</td>\n",
              "      <td>0.917133</td>\n",
              "      <td>0.231110</td>\n",
              "      <td>0.154914</td>\n",
              "      <td>0.095178</td>\n",
              "      <td>0.001653</td>\n",
              "      <td>0.126443</td>\n",
              "      <td>0.039259</td>\n",
              "      <td>0.027897</td>\n",
              "      <td>0.063697</td>\n",
              "      <td>0.233103</td>\n",
              "      <td>0.175655</td>\n",
              "      <td>0.031171</td>\n",
              "      <td>0.487460</td>\n",
              "      <td>0.093915</td>\n",
              "      <td>CR</td>\n",
              "      <td>O</td>\n",
              "      <td>0.009479</td>\n",
              "      <td>0.001714</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.650112</td>\n",
              "      <td>0.007773</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005851</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.216507</td>\n",
              "      <td>0.008605</td>\n",
              "      <td>0.507712</td>\n",
              "      <td>0.006821</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>0.001419</td>\n",
              "      <td>0.096441</td>\n",
              "      <td>0.007972</td>\n",
              "      <td>0.106020</td>\n",
              "      <td>0.032733</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.008156</td>\n",
              "      <td>0.076561</td>\n",
              "      <td>0.074432</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004933</td>\n",
              "      <td>0.004831</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001833</td>\n",
              "      <td>0.005738</td>\n",
              "      <td>0.003289</td>\n",
              "      <td>0.002608</td>\n",
              "      <td>0.007338</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007447</td>\n",
              "      <td>0.004465</td>\n",
              "      <td>0.006111</td>\n",
              "      <td>0.002246</td>\n",
              "      <td>0.002109</td>\n",
              "      <td>0.001141</td>\n",
              "      <td>0.506213</td>\n",
              "      <td>0.001770</td>\n",
              "      <td>1.000904</td>\n",
              "      <td>0.062492</td>\n",
              "      <td>0.005860</td>\n",
              "      <td>0.001845</td>\n",
              "      <td>0.007816</td>\n",
              "      <td>0.002470</td>\n",
              "      <td>0.005516</td>\n",
              "      <td>0.007166</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.001504</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002019</td>\n",
              "      <td>0.002678</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.007728</td>\n",
              "      <td>0.003432</td>\n",
              "      <td>0.002199</td>\n",
              "      <td>0.005511</td>\n",
              "      <td>0.004105</td>\n",
              "      <td>0.009656</td>\n",
              "      <td>1.006536</td>\n",
              "      <td>0.005158</td>\n",
              "      <td>0.003341</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>1.509905</td>\n",
              "      <td>1.002915</td>\n",
              "      <td>0.003079</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.007190</td>\n",
              "      <td>0.002983</td>\n",
              "      <td>0.000535</td>\n",
              "      <td>0.921026</td>\n",
              "      <td>0.131620</td>\n",
              "      <td>0.933479</td>\n",
              "      <td>0.978027</td>\n",
              "      <td>0.018200</td>\n",
              "      <td>0.822281</td>\n",
              "      <td>1.006125</td>\n",
              "      <td>1.005735</td>\n",
              "      <td>0.953363</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.673869</td>\n",
              "      <td>0.009998</td>\n",
              "      <td>0.017598</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.003175</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009120</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000130</td>\n",
              "      <td>0.120717</td>\n",
              "      <td>0.811199</td>\n",
              "      <td>0.008724</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.247939</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.244199</td>\n",
              "      <td>0.242325</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.705343</td>\n",
              "      <td>0.437433</td>\n",
              "      <td>0.002849</td>\n",
              "      <td>0.688774</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.009886</td>\n",
              "      <td>1.005053</td>\n",
              "      <td>1.008132</td>\n",
              "      <td>0.009823</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009680</td>\n",
              "      <td>0.004848</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.006312</td>\n",
              "      <td>0.004462</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.003671</td>\n",
              "      <td>0.004946</td>\n",
              "      <td>0.008889</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001670</td>\n",
              "      <td>0.008126</td>\n",
              "      <td>0.009827</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         customer_ID         S_2       P_2  \\\n",
              "0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-03-09  0.938469   \n",
              "1  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-04-07  0.936665   \n",
              "2  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-05-28  0.954180   \n",
              "3  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-06-13  0.960384   \n",
              "4  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-07-16  0.947248   \n",
              "\n",
              "       D_39       B_1       B_2       R_1       S_3      D_41       B_3  D_42  \\\n",
              "0  0.001733  0.008724  1.006838  0.009228  0.124035  0.008771  0.004709   NaN   \n",
              "1  0.005775  0.004923  1.000653  0.006151  0.126750  0.000798  0.002714   NaN   \n",
              "2  0.091505  0.021655  1.009672  0.006815  0.123977  0.007598  0.009423   NaN   \n",
              "3  0.002455  0.013683  1.002700  0.001373  0.117169  0.000685  0.005531   NaN   \n",
              "4  0.002483  0.015193  1.000727  0.007605  0.117325  0.004653  0.009312   NaN   \n",
              "\n",
              "   D_43      D_44       B_4      D_45       B_5       R_2      D_46      D_47  \\\n",
              "0   NaN  0.000630  0.080986  0.708906  0.170600  0.006204  0.358587  0.525351   \n",
              "1   NaN  0.002526  0.069419  0.712795  0.113239  0.006206  0.353630  0.521311   \n",
              "2   NaN  0.007605  0.068839  0.720884  0.060492  0.003259  0.334650  0.524568   \n",
              "3   NaN  0.006406  0.055630  0.723997  0.166782  0.009918  0.323271  0.530929   \n",
              "4   NaN  0.007731  0.038862  0.720619  0.143630  0.006667  0.231009  0.529305   \n",
              "\n",
              "       D_48  D_49       B_6       B_7       B_8      D_50      D_51       B_9  \\\n",
              "0  0.255736   NaN  0.063902  0.059416  0.006466  0.148698  1.335856  0.008207   \n",
              "1  0.223329   NaN  0.065261  0.057744  0.001614  0.149723  1.339794  0.008373   \n",
              "2  0.189424   NaN  0.066982  0.056647  0.005126  0.151955  1.337179  0.009355   \n",
              "3  0.135586   NaN  0.083720  0.049253  0.001418  0.151219  1.339909  0.006782   \n",
              "4       NaN   NaN  0.075900  0.048918  0.001199  0.154026  1.341735  0.000519   \n",
              "\n",
              "        R_3      D_52       P_3      B_10  D_53       S_5      B_11       S_6  \\\n",
              "0  0.001423  0.207334  0.736463  0.096219   NaN  0.023381  0.002768  0.008322   \n",
              "1  0.001984  0.202778  0.720886  0.099804   NaN  0.030599  0.002749  0.002482   \n",
              "2  0.007426  0.206629  0.738044  0.134073   NaN  0.048367  0.010077  0.000530   \n",
              "3  0.003515  0.208214  0.741813  0.134437   NaN  0.030063  0.009667  0.000783   \n",
              "4  0.001362  0.205468  0.691986  0.121518   NaN  0.054221  0.009484  0.006698   \n",
              "\n",
              "       D_54       R_4       S_7      B_12       S_8      D_55      D_56  \\\n",
              "0  1.001519  0.008298  0.161345  0.148266  0.922998  0.354596  0.152025   \n",
              "1  1.009033  0.005136  0.140951  0.143530  0.919414  0.326757  0.156201   \n",
              "2  1.009184  0.006961  0.112229  0.137014  1.001977  0.304124  0.153795   \n",
              "3  1.007456  0.008706  0.102838  0.129017  0.704016  0.275055  0.155772   \n",
              "4  1.003738  0.003846  0.094311  0.129539  0.917133  0.231110  0.154914   \n",
              "\n",
              "       B_13       R_5      D_58       S_9      B_14      D_59      D_60  \\\n",
              "0  0.118075  0.001882  0.158612  0.065728  0.018385  0.063646  0.199617   \n",
              "1  0.118737  0.001610  0.148459  0.093935  0.013035  0.065501  0.151387   \n",
              "2  0.114534  0.006328  0.139504  0.084757  0.056653  0.070607  0.305883   \n",
              "3  0.120740  0.004980  0.138100  0.048382  0.012498  0.065926  0.273553   \n",
              "4  0.095178  0.001653  0.126443  0.039259  0.027897  0.063697  0.233103   \n",
              "\n",
              "       D_61      B_15      S_11      D_62 D_63 D_64      D_65      B_16  B_17  \\\n",
              "0  0.308233  0.016361  0.401619  0.091071   CR    O  0.007126  0.007665   NaN   \n",
              "1  0.265026  0.017688  0.406326  0.086805   CR    O  0.002413  0.007148   NaN   \n",
              "2  0.212165  0.063955  0.406768  0.094001   CR    O  0.001878  0.003636   NaN   \n",
              "3  0.204300  0.022732  0.405175  0.094854   CR    O  0.005899  0.005896   NaN   \n",
              "4  0.175655  0.031171  0.487460  0.093915   CR    O  0.009479  0.001714   NaN   \n",
              "\n",
              "       B_18      B_19  D_66      B_20  D_68      S_12       R_6      S_13  \\\n",
              "0  0.652984  0.008520   NaN  0.004730   6.0  0.272008  0.008363  0.515222   \n",
              "1  0.647093  0.002238   NaN  0.003879   6.0  0.188970  0.004030  0.509048   \n",
              "2  0.645819  0.000408   NaN  0.004578   6.0  0.495308  0.006838  0.679257   \n",
              "3  0.654358  0.005897   NaN  0.005207   6.0  0.508670  0.008183  0.515282   \n",
              "4  0.650112  0.007773   NaN  0.005851   6.0  0.216507  0.008605  0.507712   \n",
              "\n",
              "       B_21      D_69      B_22      D_70      D_71      D_72      S_15  \\\n",
              "0  0.002644  0.009013  0.004808  0.008342  0.119403  0.004802  0.108271   \n",
              "1  0.004193  0.007842  0.001283  0.006524  0.140611  0.000094  0.101018   \n",
              "2  0.001337  0.006025  0.009393  0.002615  0.075868  0.007152  0.103239   \n",
              "3  0.008716  0.005271  0.004554  0.002052  0.150209  0.005364  0.206394   \n",
              "4  0.006821  0.000152  0.000104  0.001419  0.096441  0.007972  0.106020   \n",
              "\n",
              "       B_23  D_73       P_4      D_74      D_75  D_76      B_24       R_7  \\\n",
              "0  0.050882   NaN  0.007554  0.080422  0.069067   NaN  0.004327  0.007562   \n",
              "1  0.040469   NaN  0.004832  0.081413  0.074166   NaN  0.004203  0.005304   \n",
              "2  0.047454   NaN  0.006561  0.078891  0.076510   NaN  0.001782  0.001422   \n",
              "3  0.031705   NaN  0.009559  0.077490  0.071547   NaN  0.005595  0.006363   \n",
              "4  0.032733   NaN  0.008156  0.076561  0.074432   NaN  0.004933  0.004831   \n",
              "\n",
              "   D_77      B_25      B_26      D_78      D_79       R_8  R_9      S_16  \\\n",
              "0   NaN  0.007729  0.000272  0.001576  0.004239  0.001434  NaN  0.002271   \n",
              "1   NaN  0.001864  0.000979  0.009896  0.007597  0.000509  NaN  0.009810   \n",
              "2   NaN  0.005419  0.006149  0.009629  0.003094  0.008295  NaN  0.009362   \n",
              "3   NaN  0.000646  0.009193  0.008568  0.003895  0.005153  NaN  0.004876   \n",
              "4   NaN  0.001833  0.005738  0.003289  0.002608  0.007338  NaN  0.007447   \n",
              "\n",
              "       D_80      R_10      R_11      B_27      D_81      D_82      S_17  \\\n",
              "0  0.004061  0.007121  0.002456  0.002310  0.003532  0.506612  0.008033   \n",
              "1  0.000127  0.005966  0.000395  0.001327  0.007773  0.500855  0.000760   \n",
              "2  0.000954  0.005447  0.007345  0.007624  0.008811  0.504606  0.004056   \n",
              "3  0.005665  0.001888  0.004961  0.000034  0.004652  0.508998  0.006969   \n",
              "4  0.004465  0.006111  0.002246  0.002109  0.001141  0.506213  0.001770   \n",
              "\n",
              "       R_12      B_28      R_13      D_83      R_14      R_15      D_84  \\\n",
              "0  1.009825  0.084683  0.003820  0.007043  0.000438  0.006452  0.000830   \n",
              "1  1.009461  0.081843  0.000347  0.007789  0.004311  0.002332  0.009469   \n",
              "2  1.004291  0.081954  0.002709  0.004093  0.007139  0.008358  0.002325   \n",
              "3  1.004728  0.060634  0.009982  0.008817  0.008690  0.007364  0.005924   \n",
              "4  1.000904  0.062492  0.005860  0.001845  0.007816  0.002470  0.005516   \n",
              "\n",
              "       R_16  B_29  B_30      S_18      D_86  D_87      R_17      R_18  D_88  \\\n",
              "0  0.005055   NaN   0.0  0.005720  0.007084   NaN  0.000198  0.008907   NaN   \n",
              "1  0.003753   NaN   0.0  0.007584  0.006677   NaN  0.001142  0.005907   NaN   \n",
              "2  0.007381   NaN   0.0  0.005901  0.001185   NaN  0.008013  0.008882   NaN   \n",
              "3  0.008802   NaN   0.0  0.002520  0.003324   NaN  0.009455  0.008348   NaN   \n",
              "4  0.007166   NaN   0.0  0.000155  0.001504   NaN  0.002019  0.002678   NaN   \n",
              "\n",
              "   B_31      S_19      R_19      B_32      S_20      R_20      R_21      B_33  \\\n",
              "0     1  0.002537  0.005177  0.006626  0.009705  0.007782  0.002450  1.001101   \n",
              "1     1  0.008427  0.008979  0.001854  0.009924  0.005987  0.002247  1.006779   \n",
              "2     1  0.007327  0.002016  0.008686  0.008446  0.007291  0.007794  1.001014   \n",
              "3     1  0.007053  0.003909  0.002478  0.006614  0.009977  0.007686  1.002775   \n",
              "4     1  0.007728  0.003432  0.002199  0.005511  0.004105  0.009656  1.006536   \n",
              "\n",
              "       D_89      R_22      R_23      D_91      D_92      D_93      D_94  \\\n",
              "0  0.002665  0.007479  0.006893  1.503673  1.006133  0.003569  0.008871   \n",
              "1  0.002508  0.006827  0.002837  1.503577  1.005791  0.000571  0.000391   \n",
              "2  0.009634  0.009820  0.005080  1.503359  1.005801  0.007425  0.009234   \n",
              "3  0.007791  0.000458  0.007320  1.503701  1.007036  0.000664  0.003200   \n",
              "4  0.005158  0.003341  0.000264  1.509905  1.002915  0.003079  0.003845   \n",
              "\n",
              "       R_24      R_25      D_96      S_22      S_23      S_24      S_25  \\\n",
              "0  0.003950  0.003647  0.004950  0.894090  0.135561  0.911191  0.974539   \n",
              "1  0.008351  0.008850  0.003180  0.902135  0.136333  0.919876  0.975624   \n",
              "2  0.002471  0.009769  0.005433  0.939654  0.134938  0.958699  0.974067   \n",
              "3  0.008507  0.004858  0.000063  0.913205  0.140058  0.926341  0.975499   \n",
              "4  0.007190  0.002983  0.000535  0.921026  0.131620  0.933479  0.978027   \n",
              "\n",
              "       S_26     D_102     D_103     D_104     D_105  D_106     D_107  \\\n",
              "0  0.001243  0.766688  1.008691  1.004587  0.893734    NaN  0.670041   \n",
              "1  0.004561  0.786007  1.000084  1.004118  0.906841    NaN  0.668647   \n",
              "2  0.011736  0.806840  1.003014  1.009285  0.928719    NaN  0.670901   \n",
              "3  0.007571  0.808214  1.001517  1.004514  0.935383    NaN  0.672620   \n",
              "4  0.018200  0.822281  1.006125  1.005735  0.953363    NaN  0.673869   \n",
              "\n",
              "       B_36      B_37  R_26      R_27  B_38  D_108     D_109  D_110  D_111  \\\n",
              "0  0.009968  0.004572   NaN  1.008949   2.0    NaN  0.004326    NaN    NaN   \n",
              "1  0.003921  0.004654   NaN  1.003205   2.0    NaN  0.008707    NaN    NaN   \n",
              "2  0.001264  0.019176   NaN  1.000754   2.0    NaN  0.004092    NaN    NaN   \n",
              "3  0.002729  0.011720   NaN  1.005338   2.0    NaN  0.009703    NaN    NaN   \n",
              "4  0.009998  0.017598   NaN  1.003175   2.0    NaN  0.009120    NaN    NaN   \n",
              "\n",
              "   B_39     D_112      B_40      S_27     D_113  D_114     D_115  D_116  \\\n",
              "0   NaN  1.007336  0.210060  0.676922  0.007871    1.0  0.238250    0.0   \n",
              "1   NaN  1.007653  0.184093  0.822281  0.003444    1.0  0.247217    0.0   \n",
              "2   NaN  1.004312  0.154837  0.853498  0.003269    1.0  0.239867    0.0   \n",
              "3   NaN  1.002538  0.153939  0.844667  0.000053    1.0  0.240910    0.0   \n",
              "4   NaN  1.000130  0.120717  0.811199  0.008724    1.0  0.247939    0.0   \n",
              "\n",
              "   D_117     D_118     D_119  D_120     D_121     D_122     D_123     D_124  \\\n",
              "0    4.0  0.232120  0.236266    0.0  0.702280  0.434345  0.003057  0.686516   \n",
              "1    4.0  0.243532  0.241885    0.0  0.707017  0.430501  0.001306  0.686414   \n",
              "2    4.0  0.240768  0.239710    0.0  0.704843  0.434409  0.003954  0.690101   \n",
              "3    4.0  0.239400  0.240727    0.0  0.711546  0.436903  0.005135  0.687779   \n",
              "4    4.0  0.244199  0.242325    0.0  0.705343  0.437433  0.002849  0.688774   \n",
              "\n",
              "      D_125  D_126     D_127     D_128     D_129      B_41  B_42     D_130  \\\n",
              "0  0.008740    1.0  1.003319  1.007819  1.000080  0.006805   NaN  0.002052   \n",
              "1  0.000755    1.0  1.008394  1.004333  1.008344  0.004407   NaN  0.001034   \n",
              "2  0.009617    1.0  1.009307  1.007831  1.006878  0.003221   NaN  0.005681   \n",
              "3  0.004649    1.0  1.001671  1.003460  1.007573  0.007703   NaN  0.007108   \n",
              "4  0.000097    1.0  1.009886  1.005053  1.008132  0.009823   NaN  0.009680   \n",
              "\n",
              "      D_131  D_132     D_133      R_28  D_134  D_135  D_136  D_137  D_138  \\\n",
              "0  0.005972    NaN  0.004345  0.001535    NaN    NaN    NaN    NaN    NaN   \n",
              "1  0.004838    NaN  0.007495  0.004931    NaN    NaN    NaN    NaN    NaN   \n",
              "2  0.005497    NaN  0.009227  0.009123    NaN    NaN    NaN    NaN    NaN   \n",
              "3  0.008261    NaN  0.007206  0.002409    NaN    NaN    NaN    NaN    NaN   \n",
              "4  0.004848    NaN  0.006312  0.004462    NaN    NaN    NaN    NaN    NaN   \n",
              "\n",
              "      D_139     D_140     D_141  D_142     D_143     D_144     D_145  \n",
              "0  0.002427  0.003706  0.003818    NaN  0.000569  0.000610  0.002674  \n",
              "1  0.003954  0.003167  0.005032    NaN  0.009576  0.005492  0.009217  \n",
              "2  0.003269  0.007329  0.000427    NaN  0.003429  0.006986  0.002603  \n",
              "3  0.006117  0.004516  0.003200    NaN  0.008419  0.006527  0.009600  \n",
              "4  0.003671  0.004946  0.008889    NaN  0.001670  0.008126  0.009827  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv(\"train_data.csv\")\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DewnXLya6kEv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "Z96k6fO36kEv",
        "outputId": "6311adeb-15b6-4c1b-f76b-e9d5cbc9e345"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5531451, 190)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ7tAsa86kEw",
        "outputId": "742d40fb-c7fe-454a-e725-392f459ddefd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customer_ID</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a2...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e8...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         customer_ID  target\n",
              "0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...       0\n",
              "1  00000fd6641609c6ece5454664794f0340ad84dddce9a2...       0\n",
              "2  00001b22f846c82c51f6e3958ccd81970162bae8b007e8...       0\n",
              "3  000041bdba6ecadd89a52d11886e8eaaec9325906c9723...       0\n",
              "4  00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...       0"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = pd.read_csv(\"train_labels.csv\")\n",
        "labels.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "89WiVADa6kEw"
      },
      "outputs": [],
      "source": [
        "#convert to datetime from string\n",
        "train['S_2'] = pd.to_datetime(train['S_2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8X0dIGf6kEw",
        "outputId": "a478f760-a537-44b6-85e5-e2d565ca81f9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customer_ID</th>\n",
              "      <th>S_2</th>\n",
              "      <th>P_2</th>\n",
              "      <th>D_39</th>\n",
              "      <th>B_1</th>\n",
              "      <th>B_2</th>\n",
              "      <th>R_1</th>\n",
              "      <th>S_3</th>\n",
              "      <th>D_41</th>\n",
              "      <th>B_3</th>\n",
              "      <th>D_42</th>\n",
              "      <th>D_43</th>\n",
              "      <th>D_44</th>\n",
              "      <th>B_4</th>\n",
              "      <th>D_45</th>\n",
              "      <th>B_5</th>\n",
              "      <th>R_2</th>\n",
              "      <th>D_46</th>\n",
              "      <th>D_47</th>\n",
              "      <th>D_48</th>\n",
              "      <th>D_49</th>\n",
              "      <th>B_6</th>\n",
              "      <th>B_7</th>\n",
              "      <th>B_8</th>\n",
              "      <th>D_50</th>\n",
              "      <th>D_51</th>\n",
              "      <th>B_9</th>\n",
              "      <th>R_3</th>\n",
              "      <th>D_52</th>\n",
              "      <th>P_3</th>\n",
              "      <th>B_10</th>\n",
              "      <th>D_53</th>\n",
              "      <th>S_5</th>\n",
              "      <th>B_11</th>\n",
              "      <th>S_6</th>\n",
              "      <th>D_54</th>\n",
              "      <th>R_4</th>\n",
              "      <th>S_7</th>\n",
              "      <th>B_12</th>\n",
              "      <th>S_8</th>\n",
              "      <th>D_55</th>\n",
              "      <th>D_56</th>\n",
              "      <th>B_13</th>\n",
              "      <th>R_5</th>\n",
              "      <th>D_58</th>\n",
              "      <th>S_9</th>\n",
              "      <th>B_14</th>\n",
              "      <th>D_59</th>\n",
              "      <th>D_60</th>\n",
              "      <th>D_61</th>\n",
              "      <th>B_15</th>\n",
              "      <th>S_11</th>\n",
              "      <th>D_62</th>\n",
              "      <th>D_63</th>\n",
              "      <th>D_64</th>\n",
              "      <th>D_65</th>\n",
              "      <th>B_16</th>\n",
              "      <th>B_17</th>\n",
              "      <th>B_18</th>\n",
              "      <th>B_19</th>\n",
              "      <th>D_66</th>\n",
              "      <th>B_20</th>\n",
              "      <th>D_68</th>\n",
              "      <th>S_12</th>\n",
              "      <th>R_6</th>\n",
              "      <th>S_13</th>\n",
              "      <th>B_21</th>\n",
              "      <th>D_69</th>\n",
              "      <th>B_22</th>\n",
              "      <th>D_70</th>\n",
              "      <th>D_71</th>\n",
              "      <th>D_72</th>\n",
              "      <th>S_15</th>\n",
              "      <th>B_23</th>\n",
              "      <th>D_73</th>\n",
              "      <th>P_4</th>\n",
              "      <th>D_74</th>\n",
              "      <th>D_75</th>\n",
              "      <th>D_76</th>\n",
              "      <th>B_24</th>\n",
              "      <th>R_7</th>\n",
              "      <th>D_77</th>\n",
              "      <th>B_25</th>\n",
              "      <th>B_26</th>\n",
              "      <th>D_78</th>\n",
              "      <th>D_79</th>\n",
              "      <th>R_8</th>\n",
              "      <th>R_9</th>\n",
              "      <th>S_16</th>\n",
              "      <th>D_80</th>\n",
              "      <th>R_10</th>\n",
              "      <th>R_11</th>\n",
              "      <th>B_27</th>\n",
              "      <th>D_81</th>\n",
              "      <th>D_82</th>\n",
              "      <th>S_17</th>\n",
              "      <th>R_12</th>\n",
              "      <th>B_28</th>\n",
              "      <th>R_13</th>\n",
              "      <th>D_83</th>\n",
              "      <th>R_14</th>\n",
              "      <th>R_15</th>\n",
              "      <th>D_84</th>\n",
              "      <th>R_16</th>\n",
              "      <th>B_29</th>\n",
              "      <th>B_30</th>\n",
              "      <th>S_18</th>\n",
              "      <th>D_86</th>\n",
              "      <th>D_87</th>\n",
              "      <th>R_17</th>\n",
              "      <th>R_18</th>\n",
              "      <th>D_88</th>\n",
              "      <th>B_31</th>\n",
              "      <th>S_19</th>\n",
              "      <th>R_19</th>\n",
              "      <th>B_32</th>\n",
              "      <th>S_20</th>\n",
              "      <th>R_20</th>\n",
              "      <th>R_21</th>\n",
              "      <th>B_33</th>\n",
              "      <th>D_89</th>\n",
              "      <th>R_22</th>\n",
              "      <th>R_23</th>\n",
              "      <th>D_91</th>\n",
              "      <th>D_92</th>\n",
              "      <th>D_93</th>\n",
              "      <th>D_94</th>\n",
              "      <th>R_24</th>\n",
              "      <th>R_25</th>\n",
              "      <th>D_96</th>\n",
              "      <th>S_22</th>\n",
              "      <th>S_23</th>\n",
              "      <th>S_24</th>\n",
              "      <th>S_25</th>\n",
              "      <th>S_26</th>\n",
              "      <th>D_102</th>\n",
              "      <th>D_103</th>\n",
              "      <th>D_104</th>\n",
              "      <th>D_105</th>\n",
              "      <th>D_106</th>\n",
              "      <th>D_107</th>\n",
              "      <th>B_36</th>\n",
              "      <th>B_37</th>\n",
              "      <th>R_26</th>\n",
              "      <th>R_27</th>\n",
              "      <th>B_38</th>\n",
              "      <th>D_108</th>\n",
              "      <th>D_109</th>\n",
              "      <th>D_110</th>\n",
              "      <th>D_111</th>\n",
              "      <th>B_39</th>\n",
              "      <th>D_112</th>\n",
              "      <th>B_40</th>\n",
              "      <th>S_27</th>\n",
              "      <th>D_113</th>\n",
              "      <th>D_114</th>\n",
              "      <th>D_115</th>\n",
              "      <th>D_116</th>\n",
              "      <th>D_117</th>\n",
              "      <th>D_118</th>\n",
              "      <th>D_119</th>\n",
              "      <th>D_120</th>\n",
              "      <th>D_121</th>\n",
              "      <th>D_122</th>\n",
              "      <th>D_123</th>\n",
              "      <th>D_124</th>\n",
              "      <th>D_125</th>\n",
              "      <th>D_126</th>\n",
              "      <th>D_127</th>\n",
              "      <th>D_128</th>\n",
              "      <th>D_129</th>\n",
              "      <th>B_41</th>\n",
              "      <th>B_42</th>\n",
              "      <th>D_130</th>\n",
              "      <th>D_131</th>\n",
              "      <th>D_132</th>\n",
              "      <th>D_133</th>\n",
              "      <th>R_28</th>\n",
              "      <th>D_134</th>\n",
              "      <th>D_135</th>\n",
              "      <th>D_136</th>\n",
              "      <th>D_137</th>\n",
              "      <th>D_138</th>\n",
              "      <th>D_139</th>\n",
              "      <th>D_140</th>\n",
              "      <th>D_141</th>\n",
              "      <th>D_142</th>\n",
              "      <th>D_143</th>\n",
              "      <th>D_144</th>\n",
              "      <th>D_145</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
              "      <td>2017-03-09</td>\n",
              "      <td>0.938469</td>\n",
              "      <td>0.001733</td>\n",
              "      <td>0.008724</td>\n",
              "      <td>1.006838</td>\n",
              "      <td>0.009228</td>\n",
              "      <td>0.124035</td>\n",
              "      <td>0.008771</td>\n",
              "      <td>0.004709</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.080986</td>\n",
              "      <td>0.708906</td>\n",
              "      <td>0.170600</td>\n",
              "      <td>0.006204</td>\n",
              "      <td>0.358587</td>\n",
              "      <td>0.525351</td>\n",
              "      <td>0.255736</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.063902</td>\n",
              "      <td>0.059416</td>\n",
              "      <td>0.006466</td>\n",
              "      <td>0.148698</td>\n",
              "      <td>1.335856</td>\n",
              "      <td>0.008207</td>\n",
              "      <td>0.001423</td>\n",
              "      <td>0.207334</td>\n",
              "      <td>0.736463</td>\n",
              "      <td>0.096219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.023381</td>\n",
              "      <td>0.002768</td>\n",
              "      <td>0.008322</td>\n",
              "      <td>1.001519</td>\n",
              "      <td>0.008298</td>\n",
              "      <td>0.161345</td>\n",
              "      <td>0.148266</td>\n",
              "      <td>0.922998</td>\n",
              "      <td>0.354596</td>\n",
              "      <td>0.152025</td>\n",
              "      <td>0.118075</td>\n",
              "      <td>0.001882</td>\n",
              "      <td>0.158612</td>\n",
              "      <td>0.065728</td>\n",
              "      <td>0.018385</td>\n",
              "      <td>0.063646</td>\n",
              "      <td>0.199617</td>\n",
              "      <td>0.308233</td>\n",
              "      <td>0.016361</td>\n",
              "      <td>0.401619</td>\n",
              "      <td>0.091071</td>\n",
              "      <td>CR</td>\n",
              "      <td>O</td>\n",
              "      <td>0.007126</td>\n",
              "      <td>0.007665</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.652984</td>\n",
              "      <td>0.008520</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004730</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.272008</td>\n",
              "      <td>0.008363</td>\n",
              "      <td>0.515222</td>\n",
              "      <td>0.002644</td>\n",
              "      <td>0.009013</td>\n",
              "      <td>0.004808</td>\n",
              "      <td>0.008342</td>\n",
              "      <td>0.119403</td>\n",
              "      <td>0.004802</td>\n",
              "      <td>0.108271</td>\n",
              "      <td>0.050882</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007554</td>\n",
              "      <td>0.080422</td>\n",
              "      <td>0.069067</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004327</td>\n",
              "      <td>0.007562</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007729</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.001576</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>0.001434</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002271</td>\n",
              "      <td>0.004061</td>\n",
              "      <td>0.007121</td>\n",
              "      <td>0.002456</td>\n",
              "      <td>0.002310</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>0.506612</td>\n",
              "      <td>0.008033</td>\n",
              "      <td>1.009825</td>\n",
              "      <td>0.084683</td>\n",
              "      <td>0.003820</td>\n",
              "      <td>0.007043</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.006452</td>\n",
              "      <td>0.000830</td>\n",
              "      <td>0.005055</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005720</td>\n",
              "      <td>0.007084</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.008907</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.002537</td>\n",
              "      <td>0.005177</td>\n",
              "      <td>0.006626</td>\n",
              "      <td>0.009705</td>\n",
              "      <td>0.007782</td>\n",
              "      <td>0.002450</td>\n",
              "      <td>1.001101</td>\n",
              "      <td>0.002665</td>\n",
              "      <td>0.007479</td>\n",
              "      <td>0.006893</td>\n",
              "      <td>1.503673</td>\n",
              "      <td>1.006133</td>\n",
              "      <td>0.003569</td>\n",
              "      <td>0.008871</td>\n",
              "      <td>0.003950</td>\n",
              "      <td>0.003647</td>\n",
              "      <td>0.004950</td>\n",
              "      <td>0.894090</td>\n",
              "      <td>0.135561</td>\n",
              "      <td>0.911191</td>\n",
              "      <td>0.974539</td>\n",
              "      <td>0.001243</td>\n",
              "      <td>0.766688</td>\n",
              "      <td>1.008691</td>\n",
              "      <td>1.004587</td>\n",
              "      <td>0.893734</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.670041</td>\n",
              "      <td>0.009968</td>\n",
              "      <td>0.004572</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.008949</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004326</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.007336</td>\n",
              "      <td>0.210060</td>\n",
              "      <td>0.676922</td>\n",
              "      <td>0.007871</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.238250</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.232120</td>\n",
              "      <td>0.236266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.702280</td>\n",
              "      <td>0.434345</td>\n",
              "      <td>0.003057</td>\n",
              "      <td>0.686516</td>\n",
              "      <td>0.008740</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.003319</td>\n",
              "      <td>1.007819</td>\n",
              "      <td>1.000080</td>\n",
              "      <td>0.006805</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002052</td>\n",
              "      <td>0.005972</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004345</td>\n",
              "      <td>0.001535</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002427</td>\n",
              "      <td>0.003706</td>\n",
              "      <td>0.003818</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000569</td>\n",
              "      <td>0.000610</td>\n",
              "      <td>0.002674</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
              "      <td>2017-04-07</td>\n",
              "      <td>0.936665</td>\n",
              "      <td>0.005775</td>\n",
              "      <td>0.004923</td>\n",
              "      <td>1.000653</td>\n",
              "      <td>0.006151</td>\n",
              "      <td>0.126750</td>\n",
              "      <td>0.000798</td>\n",
              "      <td>0.002714</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002526</td>\n",
              "      <td>0.069419</td>\n",
              "      <td>0.712795</td>\n",
              "      <td>0.113239</td>\n",
              "      <td>0.006206</td>\n",
              "      <td>0.353630</td>\n",
              "      <td>0.521311</td>\n",
              "      <td>0.223329</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.065261</td>\n",
              "      <td>0.057744</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>0.149723</td>\n",
              "      <td>1.339794</td>\n",
              "      <td>0.008373</td>\n",
              "      <td>0.001984</td>\n",
              "      <td>0.202778</td>\n",
              "      <td>0.720886</td>\n",
              "      <td>0.099804</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.030599</td>\n",
              "      <td>0.002749</td>\n",
              "      <td>0.002482</td>\n",
              "      <td>1.009033</td>\n",
              "      <td>0.005136</td>\n",
              "      <td>0.140951</td>\n",
              "      <td>0.143530</td>\n",
              "      <td>0.919414</td>\n",
              "      <td>0.326757</td>\n",
              "      <td>0.156201</td>\n",
              "      <td>0.118737</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.148459</td>\n",
              "      <td>0.093935</td>\n",
              "      <td>0.013035</td>\n",
              "      <td>0.065501</td>\n",
              "      <td>0.151387</td>\n",
              "      <td>0.265026</td>\n",
              "      <td>0.017688</td>\n",
              "      <td>0.406326</td>\n",
              "      <td>0.086805</td>\n",
              "      <td>CR</td>\n",
              "      <td>O</td>\n",
              "      <td>0.002413</td>\n",
              "      <td>0.007148</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.647093</td>\n",
              "      <td>0.002238</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.003879</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.188970</td>\n",
              "      <td>0.004030</td>\n",
              "      <td>0.509048</td>\n",
              "      <td>0.004193</td>\n",
              "      <td>0.007842</td>\n",
              "      <td>0.001283</td>\n",
              "      <td>0.006524</td>\n",
              "      <td>0.140611</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.101018</td>\n",
              "      <td>0.040469</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004832</td>\n",
              "      <td>0.081413</td>\n",
              "      <td>0.074166</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004203</td>\n",
              "      <td>0.005304</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001864</td>\n",
              "      <td>0.000979</td>\n",
              "      <td>0.009896</td>\n",
              "      <td>0.007597</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009810</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.005966</td>\n",
              "      <td>0.000395</td>\n",
              "      <td>0.001327</td>\n",
              "      <td>0.007773</td>\n",
              "      <td>0.500855</td>\n",
              "      <td>0.000760</td>\n",
              "      <td>1.009461</td>\n",
              "      <td>0.081843</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>0.007789</td>\n",
              "      <td>0.004311</td>\n",
              "      <td>0.002332</td>\n",
              "      <td>0.009469</td>\n",
              "      <td>0.003753</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007584</td>\n",
              "      <td>0.006677</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001142</td>\n",
              "      <td>0.005907</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.008427</td>\n",
              "      <td>0.008979</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>0.009924</td>\n",
              "      <td>0.005987</td>\n",
              "      <td>0.002247</td>\n",
              "      <td>1.006779</td>\n",
              "      <td>0.002508</td>\n",
              "      <td>0.006827</td>\n",
              "      <td>0.002837</td>\n",
              "      <td>1.503577</td>\n",
              "      <td>1.005791</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>0.008351</td>\n",
              "      <td>0.008850</td>\n",
              "      <td>0.003180</td>\n",
              "      <td>0.902135</td>\n",
              "      <td>0.136333</td>\n",
              "      <td>0.919876</td>\n",
              "      <td>0.975624</td>\n",
              "      <td>0.004561</td>\n",
              "      <td>0.786007</td>\n",
              "      <td>1.000084</td>\n",
              "      <td>1.004118</td>\n",
              "      <td>0.906841</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.668647</td>\n",
              "      <td>0.003921</td>\n",
              "      <td>0.004654</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.003205</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.008707</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.007653</td>\n",
              "      <td>0.184093</td>\n",
              "      <td>0.822281</td>\n",
              "      <td>0.003444</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.247217</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.243532</td>\n",
              "      <td>0.241885</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.707017</td>\n",
              "      <td>0.430501</td>\n",
              "      <td>0.001306</td>\n",
              "      <td>0.686414</td>\n",
              "      <td>0.000755</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.008394</td>\n",
              "      <td>1.004333</td>\n",
              "      <td>1.008344</td>\n",
              "      <td>0.004407</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001034</td>\n",
              "      <td>0.004838</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007495</td>\n",
              "      <td>0.004931</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.003954</td>\n",
              "      <td>0.003167</td>\n",
              "      <td>0.005032</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009576</td>\n",
              "      <td>0.005492</td>\n",
              "      <td>0.009217</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
              "      <td>2017-05-28</td>\n",
              "      <td>0.954180</td>\n",
              "      <td>0.091505</td>\n",
              "      <td>0.021655</td>\n",
              "      <td>1.009672</td>\n",
              "      <td>0.006815</td>\n",
              "      <td>0.123977</td>\n",
              "      <td>0.007598</td>\n",
              "      <td>0.009423</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007605</td>\n",
              "      <td>0.068839</td>\n",
              "      <td>0.720884</td>\n",
              "      <td>0.060492</td>\n",
              "      <td>0.003259</td>\n",
              "      <td>0.334650</td>\n",
              "      <td>0.524568</td>\n",
              "      <td>0.189424</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.066982</td>\n",
              "      <td>0.056647</td>\n",
              "      <td>0.005126</td>\n",
              "      <td>0.151955</td>\n",
              "      <td>1.337179</td>\n",
              "      <td>0.009355</td>\n",
              "      <td>0.007426</td>\n",
              "      <td>0.206629</td>\n",
              "      <td>0.738044</td>\n",
              "      <td>0.134073</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.048367</td>\n",
              "      <td>0.010077</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>1.009184</td>\n",
              "      <td>0.006961</td>\n",
              "      <td>0.112229</td>\n",
              "      <td>0.137014</td>\n",
              "      <td>1.001977</td>\n",
              "      <td>0.304124</td>\n",
              "      <td>0.153795</td>\n",
              "      <td>0.114534</td>\n",
              "      <td>0.006328</td>\n",
              "      <td>0.139504</td>\n",
              "      <td>0.084757</td>\n",
              "      <td>0.056653</td>\n",
              "      <td>0.070607</td>\n",
              "      <td>0.305883</td>\n",
              "      <td>0.212165</td>\n",
              "      <td>0.063955</td>\n",
              "      <td>0.406768</td>\n",
              "      <td>0.094001</td>\n",
              "      <td>CR</td>\n",
              "      <td>O</td>\n",
              "      <td>0.001878</td>\n",
              "      <td>0.003636</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.645819</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004578</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.495308</td>\n",
              "      <td>0.006838</td>\n",
              "      <td>0.679257</td>\n",
              "      <td>0.001337</td>\n",
              "      <td>0.006025</td>\n",
              "      <td>0.009393</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>0.075868</td>\n",
              "      <td>0.007152</td>\n",
              "      <td>0.103239</td>\n",
              "      <td>0.047454</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.006561</td>\n",
              "      <td>0.078891</td>\n",
              "      <td>0.076510</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001782</td>\n",
              "      <td>0.001422</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005419</td>\n",
              "      <td>0.006149</td>\n",
              "      <td>0.009629</td>\n",
              "      <td>0.003094</td>\n",
              "      <td>0.008295</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009362</td>\n",
              "      <td>0.000954</td>\n",
              "      <td>0.005447</td>\n",
              "      <td>0.007345</td>\n",
              "      <td>0.007624</td>\n",
              "      <td>0.008811</td>\n",
              "      <td>0.504606</td>\n",
              "      <td>0.004056</td>\n",
              "      <td>1.004291</td>\n",
              "      <td>0.081954</td>\n",
              "      <td>0.002709</td>\n",
              "      <td>0.004093</td>\n",
              "      <td>0.007139</td>\n",
              "      <td>0.008358</td>\n",
              "      <td>0.002325</td>\n",
              "      <td>0.007381</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005901</td>\n",
              "      <td>0.001185</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.008013</td>\n",
              "      <td>0.008882</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.007327</td>\n",
              "      <td>0.002016</td>\n",
              "      <td>0.008686</td>\n",
              "      <td>0.008446</td>\n",
              "      <td>0.007291</td>\n",
              "      <td>0.007794</td>\n",
              "      <td>1.001014</td>\n",
              "      <td>0.009634</td>\n",
              "      <td>0.009820</td>\n",
              "      <td>0.005080</td>\n",
              "      <td>1.503359</td>\n",
              "      <td>1.005801</td>\n",
              "      <td>0.007425</td>\n",
              "      <td>0.009234</td>\n",
              "      <td>0.002471</td>\n",
              "      <td>0.009769</td>\n",
              "      <td>0.005433</td>\n",
              "      <td>0.939654</td>\n",
              "      <td>0.134938</td>\n",
              "      <td>0.958699</td>\n",
              "      <td>0.974067</td>\n",
              "      <td>0.011736</td>\n",
              "      <td>0.806840</td>\n",
              "      <td>1.003014</td>\n",
              "      <td>1.009285</td>\n",
              "      <td>0.928719</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.670901</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>0.019176</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000754</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004092</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.004312</td>\n",
              "      <td>0.154837</td>\n",
              "      <td>0.853498</td>\n",
              "      <td>0.003269</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.239867</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.240768</td>\n",
              "      <td>0.239710</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.704843</td>\n",
              "      <td>0.434409</td>\n",
              "      <td>0.003954</td>\n",
              "      <td>0.690101</td>\n",
              "      <td>0.009617</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.009307</td>\n",
              "      <td>1.007831</td>\n",
              "      <td>1.006878</td>\n",
              "      <td>0.003221</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005681</td>\n",
              "      <td>0.005497</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009227</td>\n",
              "      <td>0.009123</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.003269</td>\n",
              "      <td>0.007329</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.003429</td>\n",
              "      <td>0.006986</td>\n",
              "      <td>0.002603</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
              "      <td>2017-06-13</td>\n",
              "      <td>0.960384</td>\n",
              "      <td>0.002455</td>\n",
              "      <td>0.013683</td>\n",
              "      <td>1.002700</td>\n",
              "      <td>0.001373</td>\n",
              "      <td>0.117169</td>\n",
              "      <td>0.000685</td>\n",
              "      <td>0.005531</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.006406</td>\n",
              "      <td>0.055630</td>\n",
              "      <td>0.723997</td>\n",
              "      <td>0.166782</td>\n",
              "      <td>0.009918</td>\n",
              "      <td>0.323271</td>\n",
              "      <td>0.530929</td>\n",
              "      <td>0.135586</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.083720</td>\n",
              "      <td>0.049253</td>\n",
              "      <td>0.001418</td>\n",
              "      <td>0.151219</td>\n",
              "      <td>1.339909</td>\n",
              "      <td>0.006782</td>\n",
              "      <td>0.003515</td>\n",
              "      <td>0.208214</td>\n",
              "      <td>0.741813</td>\n",
              "      <td>0.134437</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.030063</td>\n",
              "      <td>0.009667</td>\n",
              "      <td>0.000783</td>\n",
              "      <td>1.007456</td>\n",
              "      <td>0.008706</td>\n",
              "      <td>0.102838</td>\n",
              "      <td>0.129017</td>\n",
              "      <td>0.704016</td>\n",
              "      <td>0.275055</td>\n",
              "      <td>0.155772</td>\n",
              "      <td>0.120740</td>\n",
              "      <td>0.004980</td>\n",
              "      <td>0.138100</td>\n",
              "      <td>0.048382</td>\n",
              "      <td>0.012498</td>\n",
              "      <td>0.065926</td>\n",
              "      <td>0.273553</td>\n",
              "      <td>0.204300</td>\n",
              "      <td>0.022732</td>\n",
              "      <td>0.405175</td>\n",
              "      <td>0.094854</td>\n",
              "      <td>CR</td>\n",
              "      <td>O</td>\n",
              "      <td>0.005899</td>\n",
              "      <td>0.005896</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.654358</td>\n",
              "      <td>0.005897</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005207</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.508670</td>\n",
              "      <td>0.008183</td>\n",
              "      <td>0.515282</td>\n",
              "      <td>0.008716</td>\n",
              "      <td>0.005271</td>\n",
              "      <td>0.004554</td>\n",
              "      <td>0.002052</td>\n",
              "      <td>0.150209</td>\n",
              "      <td>0.005364</td>\n",
              "      <td>0.206394</td>\n",
              "      <td>0.031705</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009559</td>\n",
              "      <td>0.077490</td>\n",
              "      <td>0.071547</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005595</td>\n",
              "      <td>0.006363</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000646</td>\n",
              "      <td>0.009193</td>\n",
              "      <td>0.008568</td>\n",
              "      <td>0.003895</td>\n",
              "      <td>0.005153</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004876</td>\n",
              "      <td>0.005665</td>\n",
              "      <td>0.001888</td>\n",
              "      <td>0.004961</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.004652</td>\n",
              "      <td>0.508998</td>\n",
              "      <td>0.006969</td>\n",
              "      <td>1.004728</td>\n",
              "      <td>0.060634</td>\n",
              "      <td>0.009982</td>\n",
              "      <td>0.008817</td>\n",
              "      <td>0.008690</td>\n",
              "      <td>0.007364</td>\n",
              "      <td>0.005924</td>\n",
              "      <td>0.008802</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>0.003324</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009455</td>\n",
              "      <td>0.008348</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.007053</td>\n",
              "      <td>0.003909</td>\n",
              "      <td>0.002478</td>\n",
              "      <td>0.006614</td>\n",
              "      <td>0.009977</td>\n",
              "      <td>0.007686</td>\n",
              "      <td>1.002775</td>\n",
              "      <td>0.007791</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.007320</td>\n",
              "      <td>1.503701</td>\n",
              "      <td>1.007036</td>\n",
              "      <td>0.000664</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>0.008507</td>\n",
              "      <td>0.004858</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.913205</td>\n",
              "      <td>0.140058</td>\n",
              "      <td>0.926341</td>\n",
              "      <td>0.975499</td>\n",
              "      <td>0.007571</td>\n",
              "      <td>0.808214</td>\n",
              "      <td>1.001517</td>\n",
              "      <td>1.004514</td>\n",
              "      <td>0.935383</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.672620</td>\n",
              "      <td>0.002729</td>\n",
              "      <td>0.011720</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.005338</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009703</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.002538</td>\n",
              "      <td>0.153939</td>\n",
              "      <td>0.844667</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.240910</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.239400</td>\n",
              "      <td>0.240727</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.711546</td>\n",
              "      <td>0.436903</td>\n",
              "      <td>0.005135</td>\n",
              "      <td>0.687779</td>\n",
              "      <td>0.004649</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.001671</td>\n",
              "      <td>1.003460</td>\n",
              "      <td>1.007573</td>\n",
              "      <td>0.007703</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007108</td>\n",
              "      <td>0.008261</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007206</td>\n",
              "      <td>0.002409</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.006117</td>\n",
              "      <td>0.004516</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.008419</td>\n",
              "      <td>0.006527</td>\n",
              "      <td>0.009600</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
              "      <td>2017-07-16</td>\n",
              "      <td>0.947248</td>\n",
              "      <td>0.002483</td>\n",
              "      <td>0.015193</td>\n",
              "      <td>1.000727</td>\n",
              "      <td>0.007605</td>\n",
              "      <td>0.117325</td>\n",
              "      <td>0.004653</td>\n",
              "      <td>0.009312</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007731</td>\n",
              "      <td>0.038862</td>\n",
              "      <td>0.720619</td>\n",
              "      <td>0.143630</td>\n",
              "      <td>0.006667</td>\n",
              "      <td>0.231009</td>\n",
              "      <td>0.529305</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.075900</td>\n",
              "      <td>0.048918</td>\n",
              "      <td>0.001199</td>\n",
              "      <td>0.154026</td>\n",
              "      <td>1.341735</td>\n",
              "      <td>0.000519</td>\n",
              "      <td>0.001362</td>\n",
              "      <td>0.205468</td>\n",
              "      <td>0.691986</td>\n",
              "      <td>0.121518</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.054221</td>\n",
              "      <td>0.009484</td>\n",
              "      <td>0.006698</td>\n",
              "      <td>1.003738</td>\n",
              "      <td>0.003846</td>\n",
              "      <td>0.094311</td>\n",
              "      <td>0.129539</td>\n",
              "      <td>0.917133</td>\n",
              "      <td>0.231110</td>\n",
              "      <td>0.154914</td>\n",
              "      <td>0.095178</td>\n",
              "      <td>0.001653</td>\n",
              "      <td>0.126443</td>\n",
              "      <td>0.039259</td>\n",
              "      <td>0.027897</td>\n",
              "      <td>0.063697</td>\n",
              "      <td>0.233103</td>\n",
              "      <td>0.175655</td>\n",
              "      <td>0.031171</td>\n",
              "      <td>0.487460</td>\n",
              "      <td>0.093915</td>\n",
              "      <td>CR</td>\n",
              "      <td>O</td>\n",
              "      <td>0.009479</td>\n",
              "      <td>0.001714</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.650112</td>\n",
              "      <td>0.007773</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005851</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.216507</td>\n",
              "      <td>0.008605</td>\n",
              "      <td>0.507712</td>\n",
              "      <td>0.006821</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>0.001419</td>\n",
              "      <td>0.096441</td>\n",
              "      <td>0.007972</td>\n",
              "      <td>0.106020</td>\n",
              "      <td>0.032733</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.008156</td>\n",
              "      <td>0.076561</td>\n",
              "      <td>0.074432</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004933</td>\n",
              "      <td>0.004831</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001833</td>\n",
              "      <td>0.005738</td>\n",
              "      <td>0.003289</td>\n",
              "      <td>0.002608</td>\n",
              "      <td>0.007338</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007447</td>\n",
              "      <td>0.004465</td>\n",
              "      <td>0.006111</td>\n",
              "      <td>0.002246</td>\n",
              "      <td>0.002109</td>\n",
              "      <td>0.001141</td>\n",
              "      <td>0.506213</td>\n",
              "      <td>0.001770</td>\n",
              "      <td>1.000904</td>\n",
              "      <td>0.062492</td>\n",
              "      <td>0.005860</td>\n",
              "      <td>0.001845</td>\n",
              "      <td>0.007816</td>\n",
              "      <td>0.002470</td>\n",
              "      <td>0.005516</td>\n",
              "      <td>0.007166</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.001504</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002019</td>\n",
              "      <td>0.002678</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.007728</td>\n",
              "      <td>0.003432</td>\n",
              "      <td>0.002199</td>\n",
              "      <td>0.005511</td>\n",
              "      <td>0.004105</td>\n",
              "      <td>0.009656</td>\n",
              "      <td>1.006536</td>\n",
              "      <td>0.005158</td>\n",
              "      <td>0.003341</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>1.509905</td>\n",
              "      <td>1.002915</td>\n",
              "      <td>0.003079</td>\n",
              "      <td>0.003845</td>\n",
              "      <td>0.007190</td>\n",
              "      <td>0.002983</td>\n",
              "      <td>0.000535</td>\n",
              "      <td>0.921026</td>\n",
              "      <td>0.131620</td>\n",
              "      <td>0.933479</td>\n",
              "      <td>0.978027</td>\n",
              "      <td>0.018200</td>\n",
              "      <td>0.822281</td>\n",
              "      <td>1.006125</td>\n",
              "      <td>1.005735</td>\n",
              "      <td>0.953363</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.673869</td>\n",
              "      <td>0.009998</td>\n",
              "      <td>0.017598</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.003175</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009120</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000130</td>\n",
              "      <td>0.120717</td>\n",
              "      <td>0.811199</td>\n",
              "      <td>0.008724</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.247939</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.244199</td>\n",
              "      <td>0.242325</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.705343</td>\n",
              "      <td>0.437433</td>\n",
              "      <td>0.002849</td>\n",
              "      <td>0.688774</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.009886</td>\n",
              "      <td>1.005053</td>\n",
              "      <td>1.008132</td>\n",
              "      <td>0.009823</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009680</td>\n",
              "      <td>0.004848</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.006312</td>\n",
              "      <td>0.004462</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.003671</td>\n",
              "      <td>0.004946</td>\n",
              "      <td>0.008889</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001670</td>\n",
              "      <td>0.008126</td>\n",
              "      <td>0.009827</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         customer_ID        S_2       P_2  \\\n",
              "0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-03-09  0.938469   \n",
              "1  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-04-07  0.936665   \n",
              "2  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-05-28  0.954180   \n",
              "3  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-06-13  0.960384   \n",
              "4  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f... 2017-07-16  0.947248   \n",
              "\n",
              "       D_39       B_1       B_2       R_1       S_3      D_41       B_3  D_42  \\\n",
              "0  0.001733  0.008724  1.006838  0.009228  0.124035  0.008771  0.004709   NaN   \n",
              "1  0.005775  0.004923  1.000653  0.006151  0.126750  0.000798  0.002714   NaN   \n",
              "2  0.091505  0.021655  1.009672  0.006815  0.123977  0.007598  0.009423   NaN   \n",
              "3  0.002455  0.013683  1.002700  0.001373  0.117169  0.000685  0.005531   NaN   \n",
              "4  0.002483  0.015193  1.000727  0.007605  0.117325  0.004653  0.009312   NaN   \n",
              "\n",
              "   D_43      D_44       B_4      D_45       B_5       R_2      D_46      D_47  \\\n",
              "0   NaN  0.000630  0.080986  0.708906  0.170600  0.006204  0.358587  0.525351   \n",
              "1   NaN  0.002526  0.069419  0.712795  0.113239  0.006206  0.353630  0.521311   \n",
              "2   NaN  0.007605  0.068839  0.720884  0.060492  0.003259  0.334650  0.524568   \n",
              "3   NaN  0.006406  0.055630  0.723997  0.166782  0.009918  0.323271  0.530929   \n",
              "4   NaN  0.007731  0.038862  0.720619  0.143630  0.006667  0.231009  0.529305   \n",
              "\n",
              "       D_48  D_49       B_6       B_7       B_8      D_50      D_51       B_9  \\\n",
              "0  0.255736   NaN  0.063902  0.059416  0.006466  0.148698  1.335856  0.008207   \n",
              "1  0.223329   NaN  0.065261  0.057744  0.001614  0.149723  1.339794  0.008373   \n",
              "2  0.189424   NaN  0.066982  0.056647  0.005126  0.151955  1.337179  0.009355   \n",
              "3  0.135586   NaN  0.083720  0.049253  0.001418  0.151219  1.339909  0.006782   \n",
              "4       NaN   NaN  0.075900  0.048918  0.001199  0.154026  1.341735  0.000519   \n",
              "\n",
              "        R_3      D_52       P_3      B_10  D_53       S_5      B_11       S_6  \\\n",
              "0  0.001423  0.207334  0.736463  0.096219   NaN  0.023381  0.002768  0.008322   \n",
              "1  0.001984  0.202778  0.720886  0.099804   NaN  0.030599  0.002749  0.002482   \n",
              "2  0.007426  0.206629  0.738044  0.134073   NaN  0.048367  0.010077  0.000530   \n",
              "3  0.003515  0.208214  0.741813  0.134437   NaN  0.030063  0.009667  0.000783   \n",
              "4  0.001362  0.205468  0.691986  0.121518   NaN  0.054221  0.009484  0.006698   \n",
              "\n",
              "       D_54       R_4       S_7      B_12       S_8      D_55      D_56  \\\n",
              "0  1.001519  0.008298  0.161345  0.148266  0.922998  0.354596  0.152025   \n",
              "1  1.009033  0.005136  0.140951  0.143530  0.919414  0.326757  0.156201   \n",
              "2  1.009184  0.006961  0.112229  0.137014  1.001977  0.304124  0.153795   \n",
              "3  1.007456  0.008706  0.102838  0.129017  0.704016  0.275055  0.155772   \n",
              "4  1.003738  0.003846  0.094311  0.129539  0.917133  0.231110  0.154914   \n",
              "\n",
              "       B_13       R_5      D_58       S_9      B_14      D_59      D_60  \\\n",
              "0  0.118075  0.001882  0.158612  0.065728  0.018385  0.063646  0.199617   \n",
              "1  0.118737  0.001610  0.148459  0.093935  0.013035  0.065501  0.151387   \n",
              "2  0.114534  0.006328  0.139504  0.084757  0.056653  0.070607  0.305883   \n",
              "3  0.120740  0.004980  0.138100  0.048382  0.012498  0.065926  0.273553   \n",
              "4  0.095178  0.001653  0.126443  0.039259  0.027897  0.063697  0.233103   \n",
              "\n",
              "       D_61      B_15      S_11      D_62 D_63 D_64      D_65      B_16  B_17  \\\n",
              "0  0.308233  0.016361  0.401619  0.091071   CR    O  0.007126  0.007665   NaN   \n",
              "1  0.265026  0.017688  0.406326  0.086805   CR    O  0.002413  0.007148   NaN   \n",
              "2  0.212165  0.063955  0.406768  0.094001   CR    O  0.001878  0.003636   NaN   \n",
              "3  0.204300  0.022732  0.405175  0.094854   CR    O  0.005899  0.005896   NaN   \n",
              "4  0.175655  0.031171  0.487460  0.093915   CR    O  0.009479  0.001714   NaN   \n",
              "\n",
              "       B_18      B_19  D_66      B_20  D_68      S_12       R_6      S_13  \\\n",
              "0  0.652984  0.008520   NaN  0.004730   6.0  0.272008  0.008363  0.515222   \n",
              "1  0.647093  0.002238   NaN  0.003879   6.0  0.188970  0.004030  0.509048   \n",
              "2  0.645819  0.000408   NaN  0.004578   6.0  0.495308  0.006838  0.679257   \n",
              "3  0.654358  0.005897   NaN  0.005207   6.0  0.508670  0.008183  0.515282   \n",
              "4  0.650112  0.007773   NaN  0.005851   6.0  0.216507  0.008605  0.507712   \n",
              "\n",
              "       B_21      D_69      B_22      D_70      D_71      D_72      S_15  \\\n",
              "0  0.002644  0.009013  0.004808  0.008342  0.119403  0.004802  0.108271   \n",
              "1  0.004193  0.007842  0.001283  0.006524  0.140611  0.000094  0.101018   \n",
              "2  0.001337  0.006025  0.009393  0.002615  0.075868  0.007152  0.103239   \n",
              "3  0.008716  0.005271  0.004554  0.002052  0.150209  0.005364  0.206394   \n",
              "4  0.006821  0.000152  0.000104  0.001419  0.096441  0.007972  0.106020   \n",
              "\n",
              "       B_23  D_73       P_4      D_74      D_75  D_76      B_24       R_7  \\\n",
              "0  0.050882   NaN  0.007554  0.080422  0.069067   NaN  0.004327  0.007562   \n",
              "1  0.040469   NaN  0.004832  0.081413  0.074166   NaN  0.004203  0.005304   \n",
              "2  0.047454   NaN  0.006561  0.078891  0.076510   NaN  0.001782  0.001422   \n",
              "3  0.031705   NaN  0.009559  0.077490  0.071547   NaN  0.005595  0.006363   \n",
              "4  0.032733   NaN  0.008156  0.076561  0.074432   NaN  0.004933  0.004831   \n",
              "\n",
              "   D_77      B_25      B_26      D_78      D_79       R_8  R_9      S_16  \\\n",
              "0   NaN  0.007729  0.000272  0.001576  0.004239  0.001434  NaN  0.002271   \n",
              "1   NaN  0.001864  0.000979  0.009896  0.007597  0.000509  NaN  0.009810   \n",
              "2   NaN  0.005419  0.006149  0.009629  0.003094  0.008295  NaN  0.009362   \n",
              "3   NaN  0.000646  0.009193  0.008568  0.003895  0.005153  NaN  0.004876   \n",
              "4   NaN  0.001833  0.005738  0.003289  0.002608  0.007338  NaN  0.007447   \n",
              "\n",
              "       D_80      R_10      R_11      B_27      D_81      D_82      S_17  \\\n",
              "0  0.004061  0.007121  0.002456  0.002310  0.003532  0.506612  0.008033   \n",
              "1  0.000127  0.005966  0.000395  0.001327  0.007773  0.500855  0.000760   \n",
              "2  0.000954  0.005447  0.007345  0.007624  0.008811  0.504606  0.004056   \n",
              "3  0.005665  0.001888  0.004961  0.000034  0.004652  0.508998  0.006969   \n",
              "4  0.004465  0.006111  0.002246  0.002109  0.001141  0.506213  0.001770   \n",
              "\n",
              "       R_12      B_28      R_13      D_83      R_14      R_15      D_84  \\\n",
              "0  1.009825  0.084683  0.003820  0.007043  0.000438  0.006452  0.000830   \n",
              "1  1.009461  0.081843  0.000347  0.007789  0.004311  0.002332  0.009469   \n",
              "2  1.004291  0.081954  0.002709  0.004093  0.007139  0.008358  0.002325   \n",
              "3  1.004728  0.060634  0.009982  0.008817  0.008690  0.007364  0.005924   \n",
              "4  1.000904  0.062492  0.005860  0.001845  0.007816  0.002470  0.005516   \n",
              "\n",
              "       R_16  B_29  B_30      S_18      D_86  D_87      R_17      R_18  D_88  \\\n",
              "0  0.005055   NaN   0.0  0.005720  0.007084   NaN  0.000198  0.008907   NaN   \n",
              "1  0.003753   NaN   0.0  0.007584  0.006677   NaN  0.001142  0.005907   NaN   \n",
              "2  0.007381   NaN   0.0  0.005901  0.001185   NaN  0.008013  0.008882   NaN   \n",
              "3  0.008802   NaN   0.0  0.002520  0.003324   NaN  0.009455  0.008348   NaN   \n",
              "4  0.007166   NaN   0.0  0.000155  0.001504   NaN  0.002019  0.002678   NaN   \n",
              "\n",
              "   B_31      S_19      R_19      B_32      S_20      R_20      R_21      B_33  \\\n",
              "0     1  0.002537  0.005177  0.006626  0.009705  0.007782  0.002450  1.001101   \n",
              "1     1  0.008427  0.008979  0.001854  0.009924  0.005987  0.002247  1.006779   \n",
              "2     1  0.007327  0.002016  0.008686  0.008446  0.007291  0.007794  1.001014   \n",
              "3     1  0.007053  0.003909  0.002478  0.006614  0.009977  0.007686  1.002775   \n",
              "4     1  0.007728  0.003432  0.002199  0.005511  0.004105  0.009656  1.006536   \n",
              "\n",
              "       D_89      R_22      R_23      D_91      D_92      D_93      D_94  \\\n",
              "0  0.002665  0.007479  0.006893  1.503673  1.006133  0.003569  0.008871   \n",
              "1  0.002508  0.006827  0.002837  1.503577  1.005791  0.000571  0.000391   \n",
              "2  0.009634  0.009820  0.005080  1.503359  1.005801  0.007425  0.009234   \n",
              "3  0.007791  0.000458  0.007320  1.503701  1.007036  0.000664  0.003200   \n",
              "4  0.005158  0.003341  0.000264  1.509905  1.002915  0.003079  0.003845   \n",
              "\n",
              "       R_24      R_25      D_96      S_22      S_23      S_24      S_25  \\\n",
              "0  0.003950  0.003647  0.004950  0.894090  0.135561  0.911191  0.974539   \n",
              "1  0.008351  0.008850  0.003180  0.902135  0.136333  0.919876  0.975624   \n",
              "2  0.002471  0.009769  0.005433  0.939654  0.134938  0.958699  0.974067   \n",
              "3  0.008507  0.004858  0.000063  0.913205  0.140058  0.926341  0.975499   \n",
              "4  0.007190  0.002983  0.000535  0.921026  0.131620  0.933479  0.978027   \n",
              "\n",
              "       S_26     D_102     D_103     D_104     D_105  D_106     D_107  \\\n",
              "0  0.001243  0.766688  1.008691  1.004587  0.893734    NaN  0.670041   \n",
              "1  0.004561  0.786007  1.000084  1.004118  0.906841    NaN  0.668647   \n",
              "2  0.011736  0.806840  1.003014  1.009285  0.928719    NaN  0.670901   \n",
              "3  0.007571  0.808214  1.001517  1.004514  0.935383    NaN  0.672620   \n",
              "4  0.018200  0.822281  1.006125  1.005735  0.953363    NaN  0.673869   \n",
              "\n",
              "       B_36      B_37  R_26      R_27  B_38  D_108     D_109  D_110  D_111  \\\n",
              "0  0.009968  0.004572   NaN  1.008949   2.0    NaN  0.004326    NaN    NaN   \n",
              "1  0.003921  0.004654   NaN  1.003205   2.0    NaN  0.008707    NaN    NaN   \n",
              "2  0.001264  0.019176   NaN  1.000754   2.0    NaN  0.004092    NaN    NaN   \n",
              "3  0.002729  0.011720   NaN  1.005338   2.0    NaN  0.009703    NaN    NaN   \n",
              "4  0.009998  0.017598   NaN  1.003175   2.0    NaN  0.009120    NaN    NaN   \n",
              "\n",
              "   B_39     D_112      B_40      S_27     D_113  D_114     D_115  D_116  \\\n",
              "0   NaN  1.007336  0.210060  0.676922  0.007871    1.0  0.238250    0.0   \n",
              "1   NaN  1.007653  0.184093  0.822281  0.003444    1.0  0.247217    0.0   \n",
              "2   NaN  1.004312  0.154837  0.853498  0.003269    1.0  0.239867    0.0   \n",
              "3   NaN  1.002538  0.153939  0.844667  0.000053    1.0  0.240910    0.0   \n",
              "4   NaN  1.000130  0.120717  0.811199  0.008724    1.0  0.247939    0.0   \n",
              "\n",
              "   D_117     D_118     D_119  D_120     D_121     D_122     D_123     D_124  \\\n",
              "0    4.0  0.232120  0.236266    0.0  0.702280  0.434345  0.003057  0.686516   \n",
              "1    4.0  0.243532  0.241885    0.0  0.707017  0.430501  0.001306  0.686414   \n",
              "2    4.0  0.240768  0.239710    0.0  0.704843  0.434409  0.003954  0.690101   \n",
              "3    4.0  0.239400  0.240727    0.0  0.711546  0.436903  0.005135  0.687779   \n",
              "4    4.0  0.244199  0.242325    0.0  0.705343  0.437433  0.002849  0.688774   \n",
              "\n",
              "      D_125  D_126     D_127     D_128     D_129      B_41  B_42     D_130  \\\n",
              "0  0.008740    1.0  1.003319  1.007819  1.000080  0.006805   NaN  0.002052   \n",
              "1  0.000755    1.0  1.008394  1.004333  1.008344  0.004407   NaN  0.001034   \n",
              "2  0.009617    1.0  1.009307  1.007831  1.006878  0.003221   NaN  0.005681   \n",
              "3  0.004649    1.0  1.001671  1.003460  1.007573  0.007703   NaN  0.007108   \n",
              "4  0.000097    1.0  1.009886  1.005053  1.008132  0.009823   NaN  0.009680   \n",
              "\n",
              "      D_131  D_132     D_133      R_28  D_134  D_135  D_136  D_137  D_138  \\\n",
              "0  0.005972    NaN  0.004345  0.001535    NaN    NaN    NaN    NaN    NaN   \n",
              "1  0.004838    NaN  0.007495  0.004931    NaN    NaN    NaN    NaN    NaN   \n",
              "2  0.005497    NaN  0.009227  0.009123    NaN    NaN    NaN    NaN    NaN   \n",
              "3  0.008261    NaN  0.007206  0.002409    NaN    NaN    NaN    NaN    NaN   \n",
              "4  0.004848    NaN  0.006312  0.004462    NaN    NaN    NaN    NaN    NaN   \n",
              "\n",
              "      D_139     D_140     D_141  D_142     D_143     D_144     D_145  target  \n",
              "0  0.002427  0.003706  0.003818    NaN  0.000569  0.000610  0.002674       0  \n",
              "1  0.003954  0.003167  0.005032    NaN  0.009576  0.005492  0.009217       0  \n",
              "2  0.003269  0.007329  0.000427    NaN  0.003429  0.006986  0.002603       0  \n",
              "3  0.006117  0.004516  0.003200    NaN  0.008419  0.006527  0.009600       0  \n",
              "4  0.003671  0.004946  0.008889    NaN  0.001670  0.008126  0.009827       0  "
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#join labels with customer id, left join\n",
        "df = pd.merge(train, labels, on = 'customer_ID', how = 'left')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iioSjmzL6kEw",
        "outputId": "d43b5fbf-1a98-4145-b5e7-c26a377a0cac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5531451, 191)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZMGwEmB6kEw",
        "outputId": "c11aae12-034d-47d7-abcf-527c57bc63e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "458913"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#10 to 13 observations for every customer\n",
        "len(df['customer_ID'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqRk66Ds6kEx",
        "outputId": "ccd87dba-f0c0-4b0b-b32e-2af3c3250b34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13, 191)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#loc = name of column, iloc = index of column. Customer id to first row\n",
        "df.loc[df['customer_ID'] == df.iloc[1,0], :].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfIuWqWp6kEx",
        "outputId": "28888957-8599-43db-f70d-0d98fecc6119"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12.053376130116165"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#total number of observations / total number of customers\n",
        "5531451/458913"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BL0Np3x6kEx"
      },
      "source": [
        "### Randomly Selecting 1 month of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJVfUn4k6kEx",
        "outputId": "41a11b4c-5a4f-4e5f-f419-40774720527c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min : 2017-03-01 00:00:00 Max:  2018-03-31 00:00:00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(458913, 191)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df12 = df.sample(frac=1, random_state = 21)  #shuffling the data\n",
        "df1 = df12.groupby('customer_ID',as_index=False).first()\n",
        "print(\"Min :\", df1['S_2'].min(), \"Max: \", df1['S_2'].max())\n",
        "df1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5acdq7Au6kEx",
        "outputId": "085d63b5-8b2d-44e3-e598-e259749a7cdf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customer_ID</th>\n",
              "      <th>P_2</th>\n",
              "      <th>D_39</th>\n",
              "      <th>B_1</th>\n",
              "      <th>B_2</th>\n",
              "      <th>R_1</th>\n",
              "      <th>S_3</th>\n",
              "      <th>D_41</th>\n",
              "      <th>B_3</th>\n",
              "      <th>D_42</th>\n",
              "      <th>D_43</th>\n",
              "      <th>D_44</th>\n",
              "      <th>B_4</th>\n",
              "      <th>D_45</th>\n",
              "      <th>B_5</th>\n",
              "      <th>R_2</th>\n",
              "      <th>D_46</th>\n",
              "      <th>D_47</th>\n",
              "      <th>D_48</th>\n",
              "      <th>D_49</th>\n",
              "      <th>B_6</th>\n",
              "      <th>B_7</th>\n",
              "      <th>B_8</th>\n",
              "      <th>D_50</th>\n",
              "      <th>D_51</th>\n",
              "      <th>B_9</th>\n",
              "      <th>R_3</th>\n",
              "      <th>D_52</th>\n",
              "      <th>P_3</th>\n",
              "      <th>B_10</th>\n",
              "      <th>D_53</th>\n",
              "      <th>S_5</th>\n",
              "      <th>B_11</th>\n",
              "      <th>S_6</th>\n",
              "      <th>D_54</th>\n",
              "      <th>R_4</th>\n",
              "      <th>S_7</th>\n",
              "      <th>B_12</th>\n",
              "      <th>S_8</th>\n",
              "      <th>D_55</th>\n",
              "      <th>D_56</th>\n",
              "      <th>B_13</th>\n",
              "      <th>R_5</th>\n",
              "      <th>D_58</th>\n",
              "      <th>S_9</th>\n",
              "      <th>B_14</th>\n",
              "      <th>D_59</th>\n",
              "      <th>D_60</th>\n",
              "      <th>D_61</th>\n",
              "      <th>B_15</th>\n",
              "      <th>S_11</th>\n",
              "      <th>D_62</th>\n",
              "      <th>D_63</th>\n",
              "      <th>D_64</th>\n",
              "      <th>D_65</th>\n",
              "      <th>B_16</th>\n",
              "      <th>B_17</th>\n",
              "      <th>B_18</th>\n",
              "      <th>B_19</th>\n",
              "      <th>D_66</th>\n",
              "      <th>B_20</th>\n",
              "      <th>D_68</th>\n",
              "      <th>S_12</th>\n",
              "      <th>R_6</th>\n",
              "      <th>S_13</th>\n",
              "      <th>B_21</th>\n",
              "      <th>D_69</th>\n",
              "      <th>B_22</th>\n",
              "      <th>D_70</th>\n",
              "      <th>D_71</th>\n",
              "      <th>D_72</th>\n",
              "      <th>S_15</th>\n",
              "      <th>B_23</th>\n",
              "      <th>D_73</th>\n",
              "      <th>P_4</th>\n",
              "      <th>D_74</th>\n",
              "      <th>D_75</th>\n",
              "      <th>D_76</th>\n",
              "      <th>B_24</th>\n",
              "      <th>R_7</th>\n",
              "      <th>D_77</th>\n",
              "      <th>B_25</th>\n",
              "      <th>B_26</th>\n",
              "      <th>D_78</th>\n",
              "      <th>D_79</th>\n",
              "      <th>R_8</th>\n",
              "      <th>R_9</th>\n",
              "      <th>S_16</th>\n",
              "      <th>D_80</th>\n",
              "      <th>R_10</th>\n",
              "      <th>R_11</th>\n",
              "      <th>B_27</th>\n",
              "      <th>D_81</th>\n",
              "      <th>D_82</th>\n",
              "      <th>S_17</th>\n",
              "      <th>R_12</th>\n",
              "      <th>B_28</th>\n",
              "      <th>R_13</th>\n",
              "      <th>D_83</th>\n",
              "      <th>R_14</th>\n",
              "      <th>R_15</th>\n",
              "      <th>D_84</th>\n",
              "      <th>R_16</th>\n",
              "      <th>B_29</th>\n",
              "      <th>B_30</th>\n",
              "      <th>S_18</th>\n",
              "      <th>D_86</th>\n",
              "      <th>D_87</th>\n",
              "      <th>R_17</th>\n",
              "      <th>R_18</th>\n",
              "      <th>D_88</th>\n",
              "      <th>B_31</th>\n",
              "      <th>S_19</th>\n",
              "      <th>R_19</th>\n",
              "      <th>B_32</th>\n",
              "      <th>S_20</th>\n",
              "      <th>R_20</th>\n",
              "      <th>R_21</th>\n",
              "      <th>B_33</th>\n",
              "      <th>D_89</th>\n",
              "      <th>R_22</th>\n",
              "      <th>R_23</th>\n",
              "      <th>D_91</th>\n",
              "      <th>D_92</th>\n",
              "      <th>D_93</th>\n",
              "      <th>D_94</th>\n",
              "      <th>R_24</th>\n",
              "      <th>R_25</th>\n",
              "      <th>D_96</th>\n",
              "      <th>S_22</th>\n",
              "      <th>S_23</th>\n",
              "      <th>S_24</th>\n",
              "      <th>S_25</th>\n",
              "      <th>S_26</th>\n",
              "      <th>D_102</th>\n",
              "      <th>D_103</th>\n",
              "      <th>D_104</th>\n",
              "      <th>D_105</th>\n",
              "      <th>D_106</th>\n",
              "      <th>D_107</th>\n",
              "      <th>B_36</th>\n",
              "      <th>B_37</th>\n",
              "      <th>R_26</th>\n",
              "      <th>R_27</th>\n",
              "      <th>B_38</th>\n",
              "      <th>D_108</th>\n",
              "      <th>D_109</th>\n",
              "      <th>D_110</th>\n",
              "      <th>D_111</th>\n",
              "      <th>B_39</th>\n",
              "      <th>D_112</th>\n",
              "      <th>B_40</th>\n",
              "      <th>S_27</th>\n",
              "      <th>D_113</th>\n",
              "      <th>D_114</th>\n",
              "      <th>D_115</th>\n",
              "      <th>D_116</th>\n",
              "      <th>D_117</th>\n",
              "      <th>D_118</th>\n",
              "      <th>D_119</th>\n",
              "      <th>D_120</th>\n",
              "      <th>D_121</th>\n",
              "      <th>D_122</th>\n",
              "      <th>D_123</th>\n",
              "      <th>D_124</th>\n",
              "      <th>D_125</th>\n",
              "      <th>D_126</th>\n",
              "      <th>D_127</th>\n",
              "      <th>D_128</th>\n",
              "      <th>D_129</th>\n",
              "      <th>B_41</th>\n",
              "      <th>B_42</th>\n",
              "      <th>D_130</th>\n",
              "      <th>D_131</th>\n",
              "      <th>D_132</th>\n",
              "      <th>D_133</th>\n",
              "      <th>R_28</th>\n",
              "      <th>D_134</th>\n",
              "      <th>D_135</th>\n",
              "      <th>D_136</th>\n",
              "      <th>D_137</th>\n",
              "      <th>D_138</th>\n",
              "      <th>D_139</th>\n",
              "      <th>D_140</th>\n",
              "      <th>D_141</th>\n",
              "      <th>D_142</th>\n",
              "      <th>D_143</th>\n",
              "      <th>D_144</th>\n",
              "      <th>D_145</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S_2</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-03-31</th>\n",
              "      <td>30730</td>\n",
              "      <td>30600</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>26787</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>4606</td>\n",
              "      <td>24965</td>\n",
              "      <td>29565</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>26658</td>\n",
              "      <td>30730</td>\n",
              "      <td>28845</td>\n",
              "      <td>3931</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30715</td>\n",
              "      <td>14705</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30676</td>\n",
              "      <td>30599</td>\n",
              "      <td>30730</td>\n",
              "      <td>10143</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>26787</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30674</td>\n",
              "      <td>17489</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>20853</td>\n",
              "      <td>30730</td>\n",
              "      <td>30680</td>\n",
              "      <td>30730</td>\n",
              "      <td>29206</td>\n",
              "      <td>30687</td>\n",
              "      <td>30730</td>\n",
              "      <td>28822</td>\n",
              "      <td>30730</td>\n",
              "      <td>30724</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>17104</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>4156</td>\n",
              "      <td>30730</td>\n",
              "      <td>30724</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30724</td>\n",
              "      <td>30730</td>\n",
              "      <td>30639</td>\n",
              "      <td>30730</td>\n",
              "      <td>30676</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>325</td>\n",
              "      <td>30730</td>\n",
              "      <td>30641</td>\n",
              "      <td>30730</td>\n",
              "      <td>4108</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>24320</td>\n",
              "      <td>30687</td>\n",
              "      <td>30730</td>\n",
              "      <td>29565</td>\n",
              "      <td>30631</td>\n",
              "      <td>30730</td>\n",
              "      <td>2442</td>\n",
              "      <td>30730</td>\n",
              "      <td>30641</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30694</td>\n",
              "      <td>9038</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30724</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30676</td>\n",
              "      <td>30730</td>\n",
              "      <td>2655</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>59</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>170</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30676</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30695</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30729</td>\n",
              "      <td>30730</td>\n",
              "      <td>30729</td>\n",
              "      <td>30726</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30631</td>\n",
              "      <td>30631</td>\n",
              "      <td>15821</td>\n",
              "      <td>3925</td>\n",
              "      <td>30631</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>3268</td>\n",
              "      <td>30329</td>\n",
              "      <td>30730</td>\n",
              "      <td>1383</td>\n",
              "      <td>30730</td>\n",
              "      <td>344</td>\n",
              "      <td>344</td>\n",
              "      <td>345</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>26216</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>30631</td>\n",
              "      <td>30631</td>\n",
              "      <td>30730</td>\n",
              "      <td>482</td>\n",
              "      <td>30631</td>\n",
              "      <td>30631</td>\n",
              "      <td>3931</td>\n",
              "      <td>30730</td>\n",
              "      <td>30730</td>\n",
              "      <td>2270</td>\n",
              "      <td>2270</td>\n",
              "      <td>2270</td>\n",
              "      <td>2270</td>\n",
              "      <td>2270</td>\n",
              "      <td>30631</td>\n",
              "      <td>30730</td>\n",
              "      <td>30631</td>\n",
              "      <td>5477</td>\n",
              "      <td>30631</td>\n",
              "      <td>30730</td>\n",
              "      <td>30631</td>\n",
              "      <td>30730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-04-30</th>\n",
              "      <td>31233</td>\n",
              "      <td>31097</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>27205</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>4907</td>\n",
              "      <td>25305</td>\n",
              "      <td>30024</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>26900</td>\n",
              "      <td>31233</td>\n",
              "      <td>29248</td>\n",
              "      <td>3970</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31216</td>\n",
              "      <td>14866</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31175</td>\n",
              "      <td>31106</td>\n",
              "      <td>31233</td>\n",
              "      <td>10274</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>27205</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31167</td>\n",
              "      <td>17580</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>21162</td>\n",
              "      <td>31233</td>\n",
              "      <td>31176</td>\n",
              "      <td>31233</td>\n",
              "      <td>29613</td>\n",
              "      <td>31186</td>\n",
              "      <td>31233</td>\n",
              "      <td>29154</td>\n",
              "      <td>31233</td>\n",
              "      <td>31226</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>17203</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>4159</td>\n",
              "      <td>31233</td>\n",
              "      <td>31226</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31226</td>\n",
              "      <td>31233</td>\n",
              "      <td>31141</td>\n",
              "      <td>31233</td>\n",
              "      <td>31175</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>362</td>\n",
              "      <td>31233</td>\n",
              "      <td>31142</td>\n",
              "      <td>31233</td>\n",
              "      <td>4087</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>24690</td>\n",
              "      <td>31186</td>\n",
              "      <td>31233</td>\n",
              "      <td>30024</td>\n",
              "      <td>31126</td>\n",
              "      <td>31233</td>\n",
              "      <td>2494</td>\n",
              "      <td>31233</td>\n",
              "      <td>31142</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31199</td>\n",
              "      <td>9012</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31226</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31175</td>\n",
              "      <td>31233</td>\n",
              "      <td>2667</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>62</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>154</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31175</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31191</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31232</td>\n",
              "      <td>31233</td>\n",
              "      <td>31232</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31126</td>\n",
              "      <td>31126</td>\n",
              "      <td>16191</td>\n",
              "      <td>3964</td>\n",
              "      <td>31126</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>3281</td>\n",
              "      <td>30830</td>\n",
              "      <td>31233</td>\n",
              "      <td>1457</td>\n",
              "      <td>31233</td>\n",
              "      <td>299</td>\n",
              "      <td>299</td>\n",
              "      <td>300</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>26683</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>31126</td>\n",
              "      <td>31126</td>\n",
              "      <td>31233</td>\n",
              "      <td>479</td>\n",
              "      <td>31126</td>\n",
              "      <td>31126</td>\n",
              "      <td>3970</td>\n",
              "      <td>31233</td>\n",
              "      <td>31233</td>\n",
              "      <td>2253</td>\n",
              "      <td>2253</td>\n",
              "      <td>2253</td>\n",
              "      <td>2253</td>\n",
              "      <td>2253</td>\n",
              "      <td>31126</td>\n",
              "      <td>31233</td>\n",
              "      <td>31126</td>\n",
              "      <td>5593</td>\n",
              "      <td>31126</td>\n",
              "      <td>31233</td>\n",
              "      <td>31126</td>\n",
              "      <td>31233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-05-31</th>\n",
              "      <td>31048</td>\n",
              "      <td>30901</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>27106</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>5182</td>\n",
              "      <td>25095</td>\n",
              "      <td>29842</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>26824</td>\n",
              "      <td>31048</td>\n",
              "      <td>29086</td>\n",
              "      <td>4030</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31029</td>\n",
              "      <td>14811</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>30987</td>\n",
              "      <td>30909</td>\n",
              "      <td>31048</td>\n",
              "      <td>10327</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>27106</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>30976</td>\n",
              "      <td>17402</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>21161</td>\n",
              "      <td>31048</td>\n",
              "      <td>30982</td>\n",
              "      <td>31048</td>\n",
              "      <td>29472</td>\n",
              "      <td>30988</td>\n",
              "      <td>31048</td>\n",
              "      <td>29031</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>17198</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>4171</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>30951</td>\n",
              "      <td>31048</td>\n",
              "      <td>30987</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>374</td>\n",
              "      <td>31048</td>\n",
              "      <td>30952</td>\n",
              "      <td>31048</td>\n",
              "      <td>4071</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>24521</td>\n",
              "      <td>30988</td>\n",
              "      <td>31048</td>\n",
              "      <td>29842</td>\n",
              "      <td>30934</td>\n",
              "      <td>31048</td>\n",
              "      <td>2563</td>\n",
              "      <td>31048</td>\n",
              "      <td>30952</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31009</td>\n",
              "      <td>8944</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>30987</td>\n",
              "      <td>31048</td>\n",
              "      <td>2327</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>67</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>144</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>30987</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31000</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31046</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>30934</td>\n",
              "      <td>30934</td>\n",
              "      <td>16014</td>\n",
              "      <td>4027</td>\n",
              "      <td>30934</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>3381</td>\n",
              "      <td>30637</td>\n",
              "      <td>31048</td>\n",
              "      <td>1431</td>\n",
              "      <td>31048</td>\n",
              "      <td>308</td>\n",
              "      <td>308</td>\n",
              "      <td>312</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>26598</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>30934</td>\n",
              "      <td>30934</td>\n",
              "      <td>31048</td>\n",
              "      <td>493</td>\n",
              "      <td>30934</td>\n",
              "      <td>30934</td>\n",
              "      <td>4030</td>\n",
              "      <td>31048</td>\n",
              "      <td>31048</td>\n",
              "      <td>2248</td>\n",
              "      <td>2248</td>\n",
              "      <td>2248</td>\n",
              "      <td>2248</td>\n",
              "      <td>2248</td>\n",
              "      <td>30934</td>\n",
              "      <td>31048</td>\n",
              "      <td>30934</td>\n",
              "      <td>5593</td>\n",
              "      <td>30934</td>\n",
              "      <td>31048</td>\n",
              "      <td>30934</td>\n",
              "      <td>31048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-06-30</th>\n",
              "      <td>31815</td>\n",
              "      <td>31683</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>27846</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>5885</td>\n",
              "      <td>25718</td>\n",
              "      <td>30623</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>27520</td>\n",
              "      <td>31815</td>\n",
              "      <td>29881</td>\n",
              "      <td>4187</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31793</td>\n",
              "      <td>14952</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31755</td>\n",
              "      <td>31683</td>\n",
              "      <td>31815</td>\n",
              "      <td>10632</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>27846</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31752</td>\n",
              "      <td>17718</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>21793</td>\n",
              "      <td>31815</td>\n",
              "      <td>31748</td>\n",
              "      <td>31815</td>\n",
              "      <td>30301</td>\n",
              "      <td>31755</td>\n",
              "      <td>31815</td>\n",
              "      <td>29836</td>\n",
              "      <td>31815</td>\n",
              "      <td>31809</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>17763</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>4234</td>\n",
              "      <td>31815</td>\n",
              "      <td>31807</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31812</td>\n",
              "      <td>31815</td>\n",
              "      <td>31732</td>\n",
              "      <td>31815</td>\n",
              "      <td>31755</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>446</td>\n",
              "      <td>31815</td>\n",
              "      <td>31732</td>\n",
              "      <td>31815</td>\n",
              "      <td>4069</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>25212</td>\n",
              "      <td>31755</td>\n",
              "      <td>31815</td>\n",
              "      <td>30623</td>\n",
              "      <td>31714</td>\n",
              "      <td>31815</td>\n",
              "      <td>2801</td>\n",
              "      <td>31815</td>\n",
              "      <td>31732</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31773</td>\n",
              "      <td>9042</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31812</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31755</td>\n",
              "      <td>31815</td>\n",
              "      <td>2557</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>68</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>156</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31755</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31767</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31813</td>\n",
              "      <td>31815</td>\n",
              "      <td>31813</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31713</td>\n",
              "      <td>31713</td>\n",
              "      <td>16558</td>\n",
              "      <td>4181</td>\n",
              "      <td>31713</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>3560</td>\n",
              "      <td>31439</td>\n",
              "      <td>31815</td>\n",
              "      <td>1483</td>\n",
              "      <td>31815</td>\n",
              "      <td>307</td>\n",
              "      <td>307</td>\n",
              "      <td>307</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>27327</td>\n",
              "      <td>31812</td>\n",
              "      <td>31812</td>\n",
              "      <td>31812</td>\n",
              "      <td>31812</td>\n",
              "      <td>31812</td>\n",
              "      <td>31812</td>\n",
              "      <td>31812</td>\n",
              "      <td>31812</td>\n",
              "      <td>31812</td>\n",
              "      <td>31812</td>\n",
              "      <td>31812</td>\n",
              "      <td>31812</td>\n",
              "      <td>31812</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>31713</td>\n",
              "      <td>31713</td>\n",
              "      <td>31815</td>\n",
              "      <td>505</td>\n",
              "      <td>31713</td>\n",
              "      <td>31713</td>\n",
              "      <td>4187</td>\n",
              "      <td>31815</td>\n",
              "      <td>31815</td>\n",
              "      <td>2423</td>\n",
              "      <td>2423</td>\n",
              "      <td>2423</td>\n",
              "      <td>2423</td>\n",
              "      <td>2423</td>\n",
              "      <td>31713</td>\n",
              "      <td>31815</td>\n",
              "      <td>31713</td>\n",
              "      <td>5938</td>\n",
              "      <td>31713</td>\n",
              "      <td>31815</td>\n",
              "      <td>31713</td>\n",
              "      <td>31815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-07-31</th>\n",
              "      <td>32620</td>\n",
              "      <td>32491</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>28477</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>6541</td>\n",
              "      <td>26228</td>\n",
              "      <td>31410</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>28199</td>\n",
              "      <td>32620</td>\n",
              "      <td>30677</td>\n",
              "      <td>4321</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32594</td>\n",
              "      <td>15321</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32548</td>\n",
              "      <td>32486</td>\n",
              "      <td>32620</td>\n",
              "      <td>10505</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>28477</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32547</td>\n",
              "      <td>17996</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>22449</td>\n",
              "      <td>32620</td>\n",
              "      <td>32548</td>\n",
              "      <td>32620</td>\n",
              "      <td>31121</td>\n",
              "      <td>32549</td>\n",
              "      <td>32620</td>\n",
              "      <td>30600</td>\n",
              "      <td>32620</td>\n",
              "      <td>32609</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>17970</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>4366</td>\n",
              "      <td>32620</td>\n",
              "      <td>32609</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32614</td>\n",
              "      <td>32620</td>\n",
              "      <td>32520</td>\n",
              "      <td>32620</td>\n",
              "      <td>32548</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>489</td>\n",
              "      <td>32620</td>\n",
              "      <td>32522</td>\n",
              "      <td>32620</td>\n",
              "      <td>4049</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>26044</td>\n",
              "      <td>32549</td>\n",
              "      <td>32620</td>\n",
              "      <td>31410</td>\n",
              "      <td>32501</td>\n",
              "      <td>32620</td>\n",
              "      <td>2769</td>\n",
              "      <td>32620</td>\n",
              "      <td>32522</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32576</td>\n",
              "      <td>9192</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32614</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32548</td>\n",
              "      <td>32620</td>\n",
              "      <td>2608</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>60</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>203</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32548</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32566</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32617</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32501</td>\n",
              "      <td>32501</td>\n",
              "      <td>16889</td>\n",
              "      <td>4312</td>\n",
              "      <td>32501</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>3720</td>\n",
              "      <td>32176</td>\n",
              "      <td>32620</td>\n",
              "      <td>1629</td>\n",
              "      <td>32620</td>\n",
              "      <td>314</td>\n",
              "      <td>314</td>\n",
              "      <td>314</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>27942</td>\n",
              "      <td>32614</td>\n",
              "      <td>32614</td>\n",
              "      <td>32614</td>\n",
              "      <td>32614</td>\n",
              "      <td>32614</td>\n",
              "      <td>32614</td>\n",
              "      <td>32614</td>\n",
              "      <td>32614</td>\n",
              "      <td>32614</td>\n",
              "      <td>32614</td>\n",
              "      <td>32614</td>\n",
              "      <td>32614</td>\n",
              "      <td>32614</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>32501</td>\n",
              "      <td>32501</td>\n",
              "      <td>32620</td>\n",
              "      <td>431</td>\n",
              "      <td>32501</td>\n",
              "      <td>32501</td>\n",
              "      <td>4319</td>\n",
              "      <td>32620</td>\n",
              "      <td>32620</td>\n",
              "      <td>2453</td>\n",
              "      <td>2453</td>\n",
              "      <td>2453</td>\n",
              "      <td>2453</td>\n",
              "      <td>2453</td>\n",
              "      <td>32501</td>\n",
              "      <td>32620</td>\n",
              "      <td>32501</td>\n",
              "      <td>6049</td>\n",
              "      <td>32501</td>\n",
              "      <td>32620</td>\n",
              "      <td>32501</td>\n",
              "      <td>32620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-08-31</th>\n",
              "      <td>33253</td>\n",
              "      <td>33131</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>29175</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>7147</td>\n",
              "      <td>26644</td>\n",
              "      <td>32039</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>28736</td>\n",
              "      <td>33253</td>\n",
              "      <td>31272</td>\n",
              "      <td>4480</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33239</td>\n",
              "      <td>15530</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33190</td>\n",
              "      <td>33125</td>\n",
              "      <td>33253</td>\n",
              "      <td>10861</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>29175</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33191</td>\n",
              "      <td>18210</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>23063</td>\n",
              "      <td>33253</td>\n",
              "      <td>33196</td>\n",
              "      <td>33253</td>\n",
              "      <td>31696</td>\n",
              "      <td>33201</td>\n",
              "      <td>33253</td>\n",
              "      <td>31214</td>\n",
              "      <td>33253</td>\n",
              "      <td>33235</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>18362</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>4382</td>\n",
              "      <td>33253</td>\n",
              "      <td>33237</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33247</td>\n",
              "      <td>33253</td>\n",
              "      <td>33162</td>\n",
              "      <td>33253</td>\n",
              "      <td>33190</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>523</td>\n",
              "      <td>33253</td>\n",
              "      <td>33165</td>\n",
              "      <td>33253</td>\n",
              "      <td>4130</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>26539</td>\n",
              "      <td>33201</td>\n",
              "      <td>33253</td>\n",
              "      <td>32039</td>\n",
              "      <td>33133</td>\n",
              "      <td>33253</td>\n",
              "      <td>2901</td>\n",
              "      <td>33253</td>\n",
              "      <td>33165</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33213</td>\n",
              "      <td>9393</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33247</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33190</td>\n",
              "      <td>33253</td>\n",
              "      <td>2696</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>70</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>193</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33190</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33214</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33251</td>\n",
              "      <td>33253</td>\n",
              "      <td>33251</td>\n",
              "      <td>33251</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33133</td>\n",
              "      <td>33133</td>\n",
              "      <td>17288</td>\n",
              "      <td>4474</td>\n",
              "      <td>33133</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>3954</td>\n",
              "      <td>32825</td>\n",
              "      <td>33253</td>\n",
              "      <td>1599</td>\n",
              "      <td>33253</td>\n",
              "      <td>320</td>\n",
              "      <td>320</td>\n",
              "      <td>323</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>28638</td>\n",
              "      <td>33246</td>\n",
              "      <td>33246</td>\n",
              "      <td>33246</td>\n",
              "      <td>33246</td>\n",
              "      <td>33246</td>\n",
              "      <td>33246</td>\n",
              "      <td>33246</td>\n",
              "      <td>33246</td>\n",
              "      <td>33246</td>\n",
              "      <td>33246</td>\n",
              "      <td>33246</td>\n",
              "      <td>33246</td>\n",
              "      <td>33246</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>33133</td>\n",
              "      <td>33133</td>\n",
              "      <td>33253</td>\n",
              "      <td>473</td>\n",
              "      <td>33133</td>\n",
              "      <td>33133</td>\n",
              "      <td>4480</td>\n",
              "      <td>33253</td>\n",
              "      <td>33253</td>\n",
              "      <td>2620</td>\n",
              "      <td>2620</td>\n",
              "      <td>2620</td>\n",
              "      <td>2620</td>\n",
              "      <td>2620</td>\n",
              "      <td>33133</td>\n",
              "      <td>33253</td>\n",
              "      <td>33133</td>\n",
              "      <td>6244</td>\n",
              "      <td>33133</td>\n",
              "      <td>33253</td>\n",
              "      <td>33133</td>\n",
              "      <td>33253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-09-30</th>\n",
              "      <td>33427</td>\n",
              "      <td>33314</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>29398</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>7648</td>\n",
              "      <td>26894</td>\n",
              "      <td>32204</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>28951</td>\n",
              "      <td>33427</td>\n",
              "      <td>31441</td>\n",
              "      <td>4546</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33416</td>\n",
              "      <td>15487</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33384</td>\n",
              "      <td>33303</td>\n",
              "      <td>33427</td>\n",
              "      <td>10955</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>29398</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33381</td>\n",
              "      <td>18067</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>23206</td>\n",
              "      <td>33427</td>\n",
              "      <td>33389</td>\n",
              "      <td>33427</td>\n",
              "      <td>31941</td>\n",
              "      <td>33391</td>\n",
              "      <td>33427</td>\n",
              "      <td>31408</td>\n",
              "      <td>33427</td>\n",
              "      <td>33388</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>18540</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>4446</td>\n",
              "      <td>33427</td>\n",
              "      <td>33390</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33409</td>\n",
              "      <td>33427</td>\n",
              "      <td>33355</td>\n",
              "      <td>33427</td>\n",
              "      <td>33384</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>571</td>\n",
              "      <td>33427</td>\n",
              "      <td>33365</td>\n",
              "      <td>33427</td>\n",
              "      <td>4251</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>26975</td>\n",
              "      <td>33391</td>\n",
              "      <td>33427</td>\n",
              "      <td>32204</td>\n",
              "      <td>33338</td>\n",
              "      <td>33427</td>\n",
              "      <td>2909</td>\n",
              "      <td>33427</td>\n",
              "      <td>33365</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33396</td>\n",
              "      <td>9125</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33409</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33384</td>\n",
              "      <td>33427</td>\n",
              "      <td>2683</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>65</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>173</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33384</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33399</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33426</td>\n",
              "      <td>33427</td>\n",
              "      <td>33426</td>\n",
              "      <td>33425</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33338</td>\n",
              "      <td>33338</td>\n",
              "      <td>17432</td>\n",
              "      <td>4542</td>\n",
              "      <td>33338</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>4025</td>\n",
              "      <td>32987</td>\n",
              "      <td>33427</td>\n",
              "      <td>1548</td>\n",
              "      <td>33427</td>\n",
              "      <td>323</td>\n",
              "      <td>323</td>\n",
              "      <td>325</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>28864</td>\n",
              "      <td>33411</td>\n",
              "      <td>33411</td>\n",
              "      <td>33411</td>\n",
              "      <td>33411</td>\n",
              "      <td>33411</td>\n",
              "      <td>33411</td>\n",
              "      <td>33411</td>\n",
              "      <td>33411</td>\n",
              "      <td>33411</td>\n",
              "      <td>33411</td>\n",
              "      <td>33411</td>\n",
              "      <td>33411</td>\n",
              "      <td>33411</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>33338</td>\n",
              "      <td>33338</td>\n",
              "      <td>33427</td>\n",
              "      <td>508</td>\n",
              "      <td>33338</td>\n",
              "      <td>33338</td>\n",
              "      <td>4546</td>\n",
              "      <td>33427</td>\n",
              "      <td>33427</td>\n",
              "      <td>2635</td>\n",
              "      <td>2635</td>\n",
              "      <td>2635</td>\n",
              "      <td>2635</td>\n",
              "      <td>2635</td>\n",
              "      <td>33338</td>\n",
              "      <td>33427</td>\n",
              "      <td>33338</td>\n",
              "      <td>6237</td>\n",
              "      <td>33338</td>\n",
              "      <td>33427</td>\n",
              "      <td>33338</td>\n",
              "      <td>33427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-10-31</th>\n",
              "      <td>34439</td>\n",
              "      <td>34294</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>30296</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>8438</td>\n",
              "      <td>27355</td>\n",
              "      <td>33103</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>29708</td>\n",
              "      <td>34439</td>\n",
              "      <td>32338</td>\n",
              "      <td>4724</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34417</td>\n",
              "      <td>15778</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34375</td>\n",
              "      <td>34254</td>\n",
              "      <td>34439</td>\n",
              "      <td>11028</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>30296</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34373</td>\n",
              "      <td>18595</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>23988</td>\n",
              "      <td>34439</td>\n",
              "      <td>34381</td>\n",
              "      <td>34439</td>\n",
              "      <td>32877</td>\n",
              "      <td>34384</td>\n",
              "      <td>34439</td>\n",
              "      <td>32247</td>\n",
              "      <td>34439</td>\n",
              "      <td>34395</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>18707</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>4494</td>\n",
              "      <td>34439</td>\n",
              "      <td>34396</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34425</td>\n",
              "      <td>34439</td>\n",
              "      <td>34337</td>\n",
              "      <td>34439</td>\n",
              "      <td>34375</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>637</td>\n",
              "      <td>34439</td>\n",
              "      <td>34344</td>\n",
              "      <td>34439</td>\n",
              "      <td>4355</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>27811</td>\n",
              "      <td>34384</td>\n",
              "      <td>34439</td>\n",
              "      <td>33103</td>\n",
              "      <td>34322</td>\n",
              "      <td>34439</td>\n",
              "      <td>3029</td>\n",
              "      <td>34439</td>\n",
              "      <td>34344</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34395</td>\n",
              "      <td>9432</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34425</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34375</td>\n",
              "      <td>34439</td>\n",
              "      <td>2678</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>69</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>210</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34375</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34399</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34437</td>\n",
              "      <td>34439</td>\n",
              "      <td>34437</td>\n",
              "      <td>34438</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34322</td>\n",
              "      <td>34322</td>\n",
              "      <td>17864</td>\n",
              "      <td>4718</td>\n",
              "      <td>34322</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>4150</td>\n",
              "      <td>33941</td>\n",
              "      <td>34439</td>\n",
              "      <td>1712</td>\n",
              "      <td>34439</td>\n",
              "      <td>293</td>\n",
              "      <td>293</td>\n",
              "      <td>294</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>29787</td>\n",
              "      <td>34425</td>\n",
              "      <td>34425</td>\n",
              "      <td>34425</td>\n",
              "      <td>34425</td>\n",
              "      <td>34425</td>\n",
              "      <td>34425</td>\n",
              "      <td>34425</td>\n",
              "      <td>34425</td>\n",
              "      <td>34425</td>\n",
              "      <td>34425</td>\n",
              "      <td>34425</td>\n",
              "      <td>34425</td>\n",
              "      <td>34425</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>34322</td>\n",
              "      <td>34322</td>\n",
              "      <td>34439</td>\n",
              "      <td>486</td>\n",
              "      <td>34322</td>\n",
              "      <td>34322</td>\n",
              "      <td>4724</td>\n",
              "      <td>34439</td>\n",
              "      <td>34439</td>\n",
              "      <td>2746</td>\n",
              "      <td>2746</td>\n",
              "      <td>2746</td>\n",
              "      <td>2746</td>\n",
              "      <td>2746</td>\n",
              "      <td>34322</td>\n",
              "      <td>34439</td>\n",
              "      <td>34322</td>\n",
              "      <td>6416</td>\n",
              "      <td>34322</td>\n",
              "      <td>34439</td>\n",
              "      <td>34322</td>\n",
              "      <td>34439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-11-30</th>\n",
              "      <td>35664</td>\n",
              "      <td>35550</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>31552</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>9465</td>\n",
              "      <td>28037</td>\n",
              "      <td>34470</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>30477</td>\n",
              "      <td>35664</td>\n",
              "      <td>33620</td>\n",
              "      <td>4835</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35662</td>\n",
              "      <td>16203</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35643</td>\n",
              "      <td>35224</td>\n",
              "      <td>35664</td>\n",
              "      <td>11380</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>31552</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35645</td>\n",
              "      <td>18809</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>24996</td>\n",
              "      <td>35664</td>\n",
              "      <td>35660</td>\n",
              "      <td>35664</td>\n",
              "      <td>34193</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>33509</td>\n",
              "      <td>35664</td>\n",
              "      <td>35585</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>19466</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>4714</td>\n",
              "      <td>35664</td>\n",
              "      <td>35591</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35643</td>\n",
              "      <td>35664</td>\n",
              "      <td>35604</td>\n",
              "      <td>35664</td>\n",
              "      <td>35643</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>753</td>\n",
              "      <td>35664</td>\n",
              "      <td>35620</td>\n",
              "      <td>35664</td>\n",
              "      <td>4234</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>29005</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>34470</td>\n",
              "      <td>35588</td>\n",
              "      <td>35664</td>\n",
              "      <td>3163</td>\n",
              "      <td>35664</td>\n",
              "      <td>35620</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35647</td>\n",
              "      <td>9593</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35643</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35643</td>\n",
              "      <td>35664</td>\n",
              "      <td>2812</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>59</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>192</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35643</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35527</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35662</td>\n",
              "      <td>35664</td>\n",
              "      <td>35662</td>\n",
              "      <td>35660</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35588</td>\n",
              "      <td>35588</td>\n",
              "      <td>18460</td>\n",
              "      <td>4826</td>\n",
              "      <td>35588</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>4413</td>\n",
              "      <td>35205</td>\n",
              "      <td>35664</td>\n",
              "      <td>1702</td>\n",
              "      <td>35664</td>\n",
              "      <td>334</td>\n",
              "      <td>334</td>\n",
              "      <td>337</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>30943</td>\n",
              "      <td>35643</td>\n",
              "      <td>35643</td>\n",
              "      <td>35643</td>\n",
              "      <td>35643</td>\n",
              "      <td>35643</td>\n",
              "      <td>35643</td>\n",
              "      <td>35643</td>\n",
              "      <td>35643</td>\n",
              "      <td>35643</td>\n",
              "      <td>35643</td>\n",
              "      <td>35643</td>\n",
              "      <td>35643</td>\n",
              "      <td>35643</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>35588</td>\n",
              "      <td>35588</td>\n",
              "      <td>35664</td>\n",
              "      <td>494</td>\n",
              "      <td>35588</td>\n",
              "      <td>35588</td>\n",
              "      <td>4835</td>\n",
              "      <td>35664</td>\n",
              "      <td>35664</td>\n",
              "      <td>2871</td>\n",
              "      <td>2871</td>\n",
              "      <td>2871</td>\n",
              "      <td>2871</td>\n",
              "      <td>2871</td>\n",
              "      <td>35588</td>\n",
              "      <td>35664</td>\n",
              "      <td>35588</td>\n",
              "      <td>6738</td>\n",
              "      <td>35588</td>\n",
              "      <td>35664</td>\n",
              "      <td>35588</td>\n",
              "      <td>35664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-12-31</th>\n",
              "      <td>36977</td>\n",
              "      <td>36885</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>32729</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>10632</td>\n",
              "      <td>28646</td>\n",
              "      <td>35637</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>30778</td>\n",
              "      <td>36977</td>\n",
              "      <td>34708</td>\n",
              "      <td>5078</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36972</td>\n",
              "      <td>16655</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36967</td>\n",
              "      <td>35710</td>\n",
              "      <td>36977</td>\n",
              "      <td>11456</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>32729</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36963</td>\n",
              "      <td>19491</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>25836</td>\n",
              "      <td>36977</td>\n",
              "      <td>36971</td>\n",
              "      <td>36977</td>\n",
              "      <td>35400</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>34642</td>\n",
              "      <td>36977</td>\n",
              "      <td>36906</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>19753</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>4845</td>\n",
              "      <td>36977</td>\n",
              "      <td>36903</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36947</td>\n",
              "      <td>36977</td>\n",
              "      <td>36921</td>\n",
              "      <td>36977</td>\n",
              "      <td>36967</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>825</td>\n",
              "      <td>36977</td>\n",
              "      <td>36947</td>\n",
              "      <td>36977</td>\n",
              "      <td>4216</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>30060</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>35637</td>\n",
              "      <td>36919</td>\n",
              "      <td>36977</td>\n",
              "      <td>3327</td>\n",
              "      <td>36977</td>\n",
              "      <td>36947</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36968</td>\n",
              "      <td>9690</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36947</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36967</td>\n",
              "      <td>36977</td>\n",
              "      <td>2785</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>83</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>203</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36967</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36786</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36974</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36919</td>\n",
              "      <td>36919</td>\n",
              "      <td>19327</td>\n",
              "      <td>5072</td>\n",
              "      <td>36919</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>4316</td>\n",
              "      <td>35375</td>\n",
              "      <td>36977</td>\n",
              "      <td>1676</td>\n",
              "      <td>36977</td>\n",
              "      <td>291</td>\n",
              "      <td>291</td>\n",
              "      <td>291</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>32118</td>\n",
              "      <td>36948</td>\n",
              "      <td>36948</td>\n",
              "      <td>36948</td>\n",
              "      <td>36948</td>\n",
              "      <td>36948</td>\n",
              "      <td>36948</td>\n",
              "      <td>36948</td>\n",
              "      <td>36948</td>\n",
              "      <td>36948</td>\n",
              "      <td>36948</td>\n",
              "      <td>36948</td>\n",
              "      <td>36948</td>\n",
              "      <td>36948</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>36919</td>\n",
              "      <td>36919</td>\n",
              "      <td>36977</td>\n",
              "      <td>555</td>\n",
              "      <td>36919</td>\n",
              "      <td>36919</td>\n",
              "      <td>5078</td>\n",
              "      <td>36977</td>\n",
              "      <td>36977</td>\n",
              "      <td>2847</td>\n",
              "      <td>2847</td>\n",
              "      <td>2847</td>\n",
              "      <td>2847</td>\n",
              "      <td>2847</td>\n",
              "      <td>36919</td>\n",
              "      <td>36977</td>\n",
              "      <td>36919</td>\n",
              "      <td>7058</td>\n",
              "      <td>36919</td>\n",
              "      <td>36977</td>\n",
              "      <td>36919</td>\n",
              "      <td>36977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-01-31</th>\n",
              "      <td>38957</td>\n",
              "      <td>38745</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>34485</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>12200</td>\n",
              "      <td>29596</td>\n",
              "      <td>37454</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>31198</td>\n",
              "      <td>38957</td>\n",
              "      <td>36525</td>\n",
              "      <td>5480</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38828</td>\n",
              "      <td>17018</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38878</td>\n",
              "      <td>36137</td>\n",
              "      <td>38957</td>\n",
              "      <td>11790</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>34485</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38712</td>\n",
              "      <td>19882</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>27302</td>\n",
              "      <td>38957</td>\n",
              "      <td>38880</td>\n",
              "      <td>38957</td>\n",
              "      <td>37301</td>\n",
              "      <td>38885</td>\n",
              "      <td>38957</td>\n",
              "      <td>36283</td>\n",
              "      <td>38957</td>\n",
              "      <td>38751</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>20313</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>5018</td>\n",
              "      <td>38957</td>\n",
              "      <td>38752</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38822</td>\n",
              "      <td>38957</td>\n",
              "      <td>38722</td>\n",
              "      <td>38957</td>\n",
              "      <td>38878</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>935</td>\n",
              "      <td>38957</td>\n",
              "      <td>38818</td>\n",
              "      <td>38957</td>\n",
              "      <td>4355</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>31730</td>\n",
              "      <td>38885</td>\n",
              "      <td>38957</td>\n",
              "      <td>37454</td>\n",
              "      <td>38804</td>\n",
              "      <td>38957</td>\n",
              "      <td>3436</td>\n",
              "      <td>38957</td>\n",
              "      <td>38818</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38904</td>\n",
              "      <td>9942</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38822</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38878</td>\n",
              "      <td>38957</td>\n",
              "      <td>3191</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>51</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>195</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38878</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>37626</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38953</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38804</td>\n",
              "      <td>38804</td>\n",
              "      <td>20155</td>\n",
              "      <td>5477</td>\n",
              "      <td>38804</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>4423</td>\n",
              "      <td>35495</td>\n",
              "      <td>38957</td>\n",
              "      <td>1800</td>\n",
              "      <td>38957</td>\n",
              "      <td>330</td>\n",
              "      <td>330</td>\n",
              "      <td>331</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>33861</td>\n",
              "      <td>38825</td>\n",
              "      <td>38825</td>\n",
              "      <td>38825</td>\n",
              "      <td>38825</td>\n",
              "      <td>38825</td>\n",
              "      <td>38825</td>\n",
              "      <td>38825</td>\n",
              "      <td>38825</td>\n",
              "      <td>38825</td>\n",
              "      <td>38825</td>\n",
              "      <td>38825</td>\n",
              "      <td>38825</td>\n",
              "      <td>38825</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>38804</td>\n",
              "      <td>38804</td>\n",
              "      <td>38957</td>\n",
              "      <td>492</td>\n",
              "      <td>38804</td>\n",
              "      <td>38804</td>\n",
              "      <td>5480</td>\n",
              "      <td>38957</td>\n",
              "      <td>38957</td>\n",
              "      <td>2990</td>\n",
              "      <td>2990</td>\n",
              "      <td>2990</td>\n",
              "      <td>2990</td>\n",
              "      <td>2990</td>\n",
              "      <td>38804</td>\n",
              "      <td>38957</td>\n",
              "      <td>38804</td>\n",
              "      <td>7464</td>\n",
              "      <td>38804</td>\n",
              "      <td>38957</td>\n",
              "      <td>38804</td>\n",
              "      <td>38957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-02-28</th>\n",
              "      <td>41614</td>\n",
              "      <td>41336</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>37024</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>13981</td>\n",
              "      <td>30152</td>\n",
              "      <td>39995</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>31405</td>\n",
              "      <td>41614</td>\n",
              "      <td>38912</td>\n",
              "      <td>5811</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41020</td>\n",
              "      <td>18230</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41502</td>\n",
              "      <td>36220</td>\n",
              "      <td>41614</td>\n",
              "      <td>12059</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>37024</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41106</td>\n",
              "      <td>20848</td>\n",
              "      <td>41389</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>28516</td>\n",
              "      <td>41614</td>\n",
              "      <td>41032</td>\n",
              "      <td>41614</td>\n",
              "      <td>39874</td>\n",
              "      <td>41563</td>\n",
              "      <td>41614</td>\n",
              "      <td>38643</td>\n",
              "      <td>41614</td>\n",
              "      <td>40462</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>21002</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>5221</td>\n",
              "      <td>41614</td>\n",
              "      <td>40467</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>40567</td>\n",
              "      <td>41614</td>\n",
              "      <td>40908</td>\n",
              "      <td>41614</td>\n",
              "      <td>41503</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>899</td>\n",
              "      <td>41614</td>\n",
              "      <td>41432</td>\n",
              "      <td>41614</td>\n",
              "      <td>4709</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>33940</td>\n",
              "      <td>41563</td>\n",
              "      <td>41614</td>\n",
              "      <td>39995</td>\n",
              "      <td>41393</td>\n",
              "      <td>41614</td>\n",
              "      <td>3579</td>\n",
              "      <td>41614</td>\n",
              "      <td>41432</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41523</td>\n",
              "      <td>10293</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>40567</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41502</td>\n",
              "      <td>41614</td>\n",
              "      <td>3580</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>58</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>221</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41502</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>38398</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41611</td>\n",
              "      <td>41614</td>\n",
              "      <td>41611</td>\n",
              "      <td>41610</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41388</td>\n",
              "      <td>41388</td>\n",
              "      <td>21211</td>\n",
              "      <td>5808</td>\n",
              "      <td>41388</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>4429</td>\n",
              "      <td>35252</td>\n",
              "      <td>41614</td>\n",
              "      <td>1736</td>\n",
              "      <td>41614</td>\n",
              "      <td>316</td>\n",
              "      <td>316</td>\n",
              "      <td>318</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>36258</td>\n",
              "      <td>40570</td>\n",
              "      <td>40570</td>\n",
              "      <td>40570</td>\n",
              "      <td>40570</td>\n",
              "      <td>40570</td>\n",
              "      <td>40570</td>\n",
              "      <td>40570</td>\n",
              "      <td>40570</td>\n",
              "      <td>40570</td>\n",
              "      <td>40570</td>\n",
              "      <td>40570</td>\n",
              "      <td>40570</td>\n",
              "      <td>40570</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>41388</td>\n",
              "      <td>41388</td>\n",
              "      <td>41614</td>\n",
              "      <td>567</td>\n",
              "      <td>41388</td>\n",
              "      <td>41388</td>\n",
              "      <td>5812</td>\n",
              "      <td>41614</td>\n",
              "      <td>41614</td>\n",
              "      <td>2987</td>\n",
              "      <td>2987</td>\n",
              "      <td>2987</td>\n",
              "      <td>2987</td>\n",
              "      <td>2987</td>\n",
              "      <td>41388</td>\n",
              "      <td>41614</td>\n",
              "      <td>41388</td>\n",
              "      <td>7951</td>\n",
              "      <td>41388</td>\n",
              "      <td>41614</td>\n",
              "      <td>41388</td>\n",
              "      <td>41614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-03-31</th>\n",
              "      <td>47136</td>\n",
              "      <td>46452</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47105</td>\n",
              "      <td>47136</td>\n",
              "      <td>40237</td>\n",
              "      <td>47105</td>\n",
              "      <td>47105</td>\n",
              "      <td>16892</td>\n",
              "      <td>31203</td>\n",
              "      <td>44982</td>\n",
              "      <td>47136</td>\n",
              "      <td>47105</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>31591</td>\n",
              "      <td>47136</td>\n",
              "      <td>43544</td>\n",
              "      <td>6222</td>\n",
              "      <td>47096</td>\n",
              "      <td>47136</td>\n",
              "      <td>45033</td>\n",
              "      <td>20336</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>46690</td>\n",
              "      <td>36631</td>\n",
              "      <td>47136</td>\n",
              "      <td>12646</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47105</td>\n",
              "      <td>47136</td>\n",
              "      <td>40237</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>45948</td>\n",
              "      <td>22512</td>\n",
              "      <td>45798</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>29354</td>\n",
              "      <td>47136</td>\n",
              "      <td>44474</td>\n",
              "      <td>47136</td>\n",
              "      <td>44835</td>\n",
              "      <td>47079</td>\n",
              "      <td>47136</td>\n",
              "      <td>43174</td>\n",
              "      <td>47136</td>\n",
              "      <td>43496</td>\n",
              "      <td>47136</td>\n",
              "      <td>47105</td>\n",
              "      <td>22102</td>\n",
              "      <td>47136</td>\n",
              "      <td>47105</td>\n",
              "      <td>5570</td>\n",
              "      <td>47105</td>\n",
              "      <td>43512</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>43642</td>\n",
              "      <td>47105</td>\n",
              "      <td>44803</td>\n",
              "      <td>47136</td>\n",
              "      <td>46704</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>869</td>\n",
              "      <td>47136</td>\n",
              "      <td>46638</td>\n",
              "      <td>47136</td>\n",
              "      <td>5287</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>37378</td>\n",
              "      <td>47079</td>\n",
              "      <td>47105</td>\n",
              "      <td>44982</td>\n",
              "      <td>45991</td>\n",
              "      <td>47136</td>\n",
              "      <td>4123</td>\n",
              "      <td>47136</td>\n",
              "      <td>46638</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47105</td>\n",
              "      <td>46714</td>\n",
              "      <td>10789</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>43642</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>46690</td>\n",
              "      <td>47136</td>\n",
              "      <td>3574</td>\n",
              "      <td>47105</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>58</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>205</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47105</td>\n",
              "      <td>46690</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>39560</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>47032</td>\n",
              "      <td>47135</td>\n",
              "      <td>47033</td>\n",
              "      <td>47120</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>45984</td>\n",
              "      <td>45984</td>\n",
              "      <td>23644</td>\n",
              "      <td>6211</td>\n",
              "      <td>45984</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>4346</td>\n",
              "      <td>35434</td>\n",
              "      <td>47105</td>\n",
              "      <td>1774</td>\n",
              "      <td>47105</td>\n",
              "      <td>308</td>\n",
              "      <td>308</td>\n",
              "      <td>309</td>\n",
              "      <td>47105</td>\n",
              "      <td>47136</td>\n",
              "      <td>37941</td>\n",
              "      <td>43669</td>\n",
              "      <td>43669</td>\n",
              "      <td>43669</td>\n",
              "      <td>43669</td>\n",
              "      <td>43669</td>\n",
              "      <td>43669</td>\n",
              "      <td>43669</td>\n",
              "      <td>43669</td>\n",
              "      <td>43669</td>\n",
              "      <td>43669</td>\n",
              "      <td>43669</td>\n",
              "      <td>43669</td>\n",
              "      <td>43669</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>45984</td>\n",
              "      <td>45984</td>\n",
              "      <td>47136</td>\n",
              "      <td>544</td>\n",
              "      <td>45984</td>\n",
              "      <td>45984</td>\n",
              "      <td>6221</td>\n",
              "      <td>47136</td>\n",
              "      <td>47136</td>\n",
              "      <td>3213</td>\n",
              "      <td>3213</td>\n",
              "      <td>3213</td>\n",
              "      <td>3213</td>\n",
              "      <td>3213</td>\n",
              "      <td>45984</td>\n",
              "      <td>47136</td>\n",
              "      <td>45984</td>\n",
              "      <td>8822</td>\n",
              "      <td>45984</td>\n",
              "      <td>47136</td>\n",
              "      <td>45984</td>\n",
              "      <td>47136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            customer_ID    P_2   D_39    B_1    B_2    R_1    S_3   D_41  \\\n",
              "S_2                                                                        \n",
              "2017-03-31        30730  30600  30730  30730  30730  30730  26787  30730   \n",
              "2017-04-30        31233  31097  31233  31233  31233  31233  27205  31233   \n",
              "2017-05-31        31048  30901  31048  31048  31048  31048  27106  31048   \n",
              "2017-06-30        31815  31683  31815  31815  31815  31815  27846  31815   \n",
              "2017-07-31        32620  32491  32620  32620  32620  32620  28477  32620   \n",
              "2017-08-31        33253  33131  33253  33253  33253  33253  29175  33253   \n",
              "2017-09-30        33427  33314  33427  33427  33427  33427  29398  33427   \n",
              "2017-10-31        34439  34294  34439  34439  34439  34439  30296  34439   \n",
              "2017-11-30        35664  35550  35664  35664  35664  35664  31552  35664   \n",
              "2017-12-31        36977  36885  36977  36977  36977  36977  32729  36977   \n",
              "2018-01-31        38957  38745  38957  38957  38957  38957  34485  38957   \n",
              "2018-02-28        41614  41336  41614  41614  41614  41614  37024  41614   \n",
              "2018-03-31        47136  46452  47136  47136  47105  47136  40237  47105   \n",
              "\n",
              "              B_3   D_42   D_43   D_44    B_4   D_45    B_5    R_2   D_46  \\\n",
              "S_2                                                                         \n",
              "2017-03-31  30730   4606  24965  29565  30730  30730  30730  30730  26658   \n",
              "2017-04-30  31233   4907  25305  30024  31233  31233  31233  31233  26900   \n",
              "2017-05-31  31048   5182  25095  29842  31048  31048  31048  31048  26824   \n",
              "2017-06-30  31815   5885  25718  30623  31815  31815  31815  31815  27520   \n",
              "2017-07-31  32620   6541  26228  31410  32620  32620  32620  32620  28199   \n",
              "2017-08-31  33253   7147  26644  32039  33253  33253  33253  33253  28736   \n",
              "2017-09-30  33427   7648  26894  32204  33427  33427  33427  33427  28951   \n",
              "2017-10-31  34439   8438  27355  33103  34439  34439  34439  34439  29708   \n",
              "2017-11-30  35664   9465  28037  34470  35664  35664  35664  35664  30477   \n",
              "2017-12-31  36977  10632  28646  35637  36977  36977  36977  36977  30778   \n",
              "2018-01-31  38957  12200  29596  37454  38957  38957  38957  38957  31198   \n",
              "2018-02-28  41614  13981  30152  39995  41614  41614  41614  41614  31405   \n",
              "2018-03-31  47105  16892  31203  44982  47136  47105  47136  47136  31591   \n",
              "\n",
              "             D_47   D_48  D_49    B_6    B_7    B_8   D_50   D_51    B_9  \\\n",
              "S_2                                                                        \n",
              "2017-03-31  30730  28845  3931  30730  30730  30715  14705  30730  30730   \n",
              "2017-04-30  31233  29248  3970  31233  31233  31216  14866  31233  31233   \n",
              "2017-05-31  31048  29086  4030  31048  31048  31029  14811  31048  31048   \n",
              "2017-06-30  31815  29881  4187  31815  31815  31793  14952  31815  31815   \n",
              "2017-07-31  32620  30677  4321  32620  32620  32594  15321  32620  32620   \n",
              "2017-08-31  33253  31272  4480  33253  33253  33239  15530  33253  33253   \n",
              "2017-09-30  33427  31441  4546  33427  33427  33416  15487  33427  33427   \n",
              "2017-10-31  34439  32338  4724  34439  34439  34417  15778  34439  34439   \n",
              "2017-11-30  35664  33620  4835  35664  35664  35662  16203  35664  35664   \n",
              "2017-12-31  36977  34708  5078  36977  36977  36972  16655  36977  36977   \n",
              "2018-01-31  38957  36525  5480  38957  38957  38828  17018  38957  38957   \n",
              "2018-02-28  41614  38912  5811  41614  41614  41020  18230  41614  41614   \n",
              "2018-03-31  47136  43544  6222  47096  47136  45033  20336  47136  47136   \n",
              "\n",
              "              R_3   D_52    P_3   B_10   D_53    S_5   B_11    S_6   D_54  \\\n",
              "S_2                                                                         \n",
              "2017-03-31  30730  30676  30599  30730  10143  30730  30730  30730  30730   \n",
              "2017-04-30  31233  31175  31106  31233  10274  31233  31233  31233  31233   \n",
              "2017-05-31  31048  30987  30909  31048  10327  31048  31048  31048  31048   \n",
              "2017-06-30  31815  31755  31683  31815  10632  31815  31815  31815  31815   \n",
              "2017-07-31  32620  32548  32486  32620  10505  32620  32620  32620  32620   \n",
              "2017-08-31  33253  33190  33125  33253  10861  33253  33253  33253  33253   \n",
              "2017-09-30  33427  33384  33303  33427  10955  33427  33427  33427  33427   \n",
              "2017-10-31  34439  34375  34254  34439  11028  34439  34439  34439  34439   \n",
              "2017-11-30  35664  35643  35224  35664  11380  35664  35664  35664  35664   \n",
              "2017-12-31  36977  36967  35710  36977  11456  36977  36977  36977  36977   \n",
              "2018-01-31  38957  38878  36137  38957  11790  38957  38957  38957  38957   \n",
              "2018-02-28  41614  41502  36220  41614  12059  41614  41614  41614  41614   \n",
              "2018-03-31  47136  46690  36631  47136  12646  47136  47136  47136  47105   \n",
              "\n",
              "              R_4    S_7   B_12    S_8   D_55   D_56   B_13    R_5   D_58  \\\n",
              "S_2                                                                         \n",
              "2017-03-31  30730  26787  30730  30730  30674  17489  30730  30730  30730   \n",
              "2017-04-30  31233  27205  31233  31233  31167  17580  31233  31233  31233   \n",
              "2017-05-31  31048  27106  31048  31048  30976  17402  31048  31048  31048   \n",
              "2017-06-30  31815  27846  31815  31815  31752  17718  31815  31815  31815   \n",
              "2017-07-31  32620  28477  32620  32620  32547  17996  32620  32620  32620   \n",
              "2017-08-31  33253  29175  33253  33253  33191  18210  33253  33253  33253   \n",
              "2017-09-30  33427  29398  33427  33427  33381  18067  33427  33427  33427   \n",
              "2017-10-31  34439  30296  34439  34439  34373  18595  34439  34439  34439   \n",
              "2017-11-30  35664  31552  35664  35664  35645  18809  35664  35664  35664   \n",
              "2017-12-31  36977  32729  36977  36977  36963  19491  36977  36977  36977   \n",
              "2018-01-31  38957  34485  38957  38957  38712  19882  38957  38957  38957   \n",
              "2018-02-28  41614  37024  41614  41614  41106  20848  41389  41614  41614   \n",
              "2018-03-31  47136  40237  47136  47136  45948  22512  45798  47136  47136   \n",
              "\n",
              "              S_9   B_14   D_59   D_60   D_61   B_15   S_11   D_62   D_63  \\\n",
              "S_2                                                                         \n",
              "2017-03-31  20853  30730  30680  30730  29206  30687  30730  28822  30730   \n",
              "2017-04-30  21162  31233  31176  31233  29613  31186  31233  29154  31233   \n",
              "2017-05-31  21161  31048  30982  31048  29472  30988  31048  29031  31048   \n",
              "2017-06-30  21793  31815  31748  31815  30301  31755  31815  29836  31815   \n",
              "2017-07-31  22449  32620  32548  32620  31121  32549  32620  30600  32620   \n",
              "2017-08-31  23063  33253  33196  33253  31696  33201  33253  31214  33253   \n",
              "2017-09-30  23206  33427  33389  33427  31941  33391  33427  31408  33427   \n",
              "2017-10-31  23988  34439  34381  34439  32877  34384  34439  32247  34439   \n",
              "2017-11-30  24996  35664  35660  35664  34193  35664  35664  33509  35664   \n",
              "2017-12-31  25836  36977  36971  36977  35400  36977  36977  34642  36977   \n",
              "2018-01-31  27302  38957  38880  38957  37301  38885  38957  36283  38957   \n",
              "2018-02-28  28516  41614  41032  41614  39874  41563  41614  38643  41614   \n",
              "2018-03-31  29354  47136  44474  47136  44835  47079  47136  43174  47136   \n",
              "\n",
              "             D_64   D_65   B_16   B_17   B_18   B_19  D_66   B_20   D_68  \\\n",
              "S_2                                                                        \n",
              "2017-03-31  30724  30730  30730  17104  30730  30730  4156  30730  30724   \n",
              "2017-04-30  31226  31233  31233  17203  31233  31233  4159  31233  31226   \n",
              "2017-05-31  31048  31048  31048  17198  31048  31048  4171  31048  31048   \n",
              "2017-06-30  31809  31815  31815  17763  31815  31815  4234  31815  31807   \n",
              "2017-07-31  32609  32620  32620  17970  32620  32620  4366  32620  32609   \n",
              "2017-08-31  33235  33253  33253  18362  33253  33253  4382  33253  33237   \n",
              "2017-09-30  33388  33427  33427  18540  33427  33427  4446  33427  33390   \n",
              "2017-10-31  34395  34439  34439  18707  34439  34439  4494  34439  34396   \n",
              "2017-11-30  35585  35664  35664  19466  35664  35664  4714  35664  35591   \n",
              "2017-12-31  36906  36977  36977  19753  36977  36977  4845  36977  36903   \n",
              "2018-01-31  38751  38957  38957  20313  38957  38957  5018  38957  38752   \n",
              "2018-02-28  40462  41614  41614  21002  41614  41614  5221  41614  40467   \n",
              "2018-03-31  43496  47136  47105  22102  47136  47105  5570  47105  43512   \n",
              "\n",
              "             S_12    R_6   S_13   B_21   D_69   B_22   D_70   D_71   D_72  \\\n",
              "S_2                                                                         \n",
              "2017-03-31  30730  30730  30730  30730  30724  30730  30639  30730  30676   \n",
              "2017-04-30  31233  31233  31233  31233  31226  31233  31141  31233  31175   \n",
              "2017-05-31  31048  31048  31048  31048  31048  31048  30951  31048  30987   \n",
              "2017-06-30  31815  31815  31815  31815  31812  31815  31732  31815  31755   \n",
              "2017-07-31  32620  32620  32620  32620  32614  32620  32520  32620  32548   \n",
              "2017-08-31  33253  33253  33253  33253  33247  33253  33162  33253  33190   \n",
              "2017-09-30  33427  33427  33427  33427  33409  33427  33355  33427  33384   \n",
              "2017-10-31  34439  34439  34439  34439  34425  34439  34337  34439  34375   \n",
              "2017-11-30  35664  35664  35664  35664  35643  35664  35604  35664  35643   \n",
              "2017-12-31  36977  36977  36977  36977  36947  36977  36921  36977  36967   \n",
              "2018-01-31  38957  38957  38957  38957  38822  38957  38722  38957  38878   \n",
              "2018-02-28  41614  41614  41614  41614  40567  41614  40908  41614  41503   \n",
              "2018-03-31  47136  47136  47136  47136  43642  47105  44803  47136  46704   \n",
              "\n",
              "             S_15   B_23  D_73    P_4   D_74   D_75  D_76   B_24    R_7  \\\n",
              "S_2                                                                       \n",
              "2017-03-31  30730  30730   325  30730  30641  30730  4108  30730  30730   \n",
              "2017-04-30  31233  31233   362  31233  31142  31233  4087  31233  31233   \n",
              "2017-05-31  31048  31048   374  31048  30952  31048  4071  31048  31048   \n",
              "2017-06-30  31815  31815   446  31815  31732  31815  4069  31815  31815   \n",
              "2017-07-31  32620  32620   489  32620  32522  32620  4049  32620  32620   \n",
              "2017-08-31  33253  33253   523  33253  33165  33253  4130  33253  33253   \n",
              "2017-09-30  33427  33427   571  33427  33365  33427  4251  33427  33427   \n",
              "2017-10-31  34439  34439   637  34439  34344  34439  4355  34439  34439   \n",
              "2017-11-30  35664  35664   753  35664  35620  35664  4234  35664  35664   \n",
              "2017-12-31  36977  36977   825  36977  36947  36977  4216  36977  36977   \n",
              "2018-01-31  38957  38957   935  38957  38818  38957  4355  38957  38957   \n",
              "2018-02-28  41614  41614   899  41614  41432  41614  4709  41614  41614   \n",
              "2018-03-31  47136  47136   869  47136  46638  47136  5287  47136  47136   \n",
              "\n",
              "             D_77   B_25   B_26   D_78   D_79    R_8   R_9   S_16   D_80  \\\n",
              "S_2                                                                        \n",
              "2017-03-31  24320  30687  30730  29565  30631  30730  2442  30730  30641   \n",
              "2017-04-30  24690  31186  31233  30024  31126  31233  2494  31233  31142   \n",
              "2017-05-31  24521  30988  31048  29842  30934  31048  2563  31048  30952   \n",
              "2017-06-30  25212  31755  31815  30623  31714  31815  2801  31815  31732   \n",
              "2017-07-31  26044  32549  32620  31410  32501  32620  2769  32620  32522   \n",
              "2017-08-31  26539  33201  33253  32039  33133  33253  2901  33253  33165   \n",
              "2017-09-30  26975  33391  33427  32204  33338  33427  2909  33427  33365   \n",
              "2017-10-31  27811  34384  34439  33103  34322  34439  3029  34439  34344   \n",
              "2017-11-30  29005  35664  35664  34470  35588  35664  3163  35664  35620   \n",
              "2017-12-31  30060  36977  36977  35637  36919  36977  3327  36977  36947   \n",
              "2018-01-31  31730  38885  38957  37454  38804  38957  3436  38957  38818   \n",
              "2018-02-28  33940  41563  41614  39995  41393  41614  3579  41614  41432   \n",
              "2018-03-31  37378  47079  47105  44982  45991  47136  4123  47136  46638   \n",
              "\n",
              "             R_10   R_11   B_27   D_81   D_82   S_17   R_12   B_28   R_13  \\\n",
              "S_2                                                                         \n",
              "2017-03-31  30730  30730  30730  30694   9038  30730  30730  30730  30730   \n",
              "2017-04-30  31233  31233  31233  31199   9012  31233  31233  31233  31233   \n",
              "2017-05-31  31048  31048  31048  31009   8944  31048  31048  31048  31048   \n",
              "2017-06-30  31815  31815  31815  31773   9042  31815  31815  31815  31815   \n",
              "2017-07-31  32620  32620  32620  32576   9192  32620  32620  32620  32620   \n",
              "2017-08-31  33253  33253  33253  33213   9393  33253  33253  33253  33253   \n",
              "2017-09-30  33427  33427  33427  33396   9125  33427  33427  33427  33427   \n",
              "2017-10-31  34439  34439  34439  34395   9432  34439  34439  34439  34439   \n",
              "2017-11-30  35664  35664  35664  35647   9593  35664  35664  35664  35664   \n",
              "2017-12-31  36977  36977  36977  36968   9690  36977  36977  36977  36977   \n",
              "2018-01-31  38957  38957  38957  38904   9942  38957  38957  38957  38957   \n",
              "2018-02-28  41614  41614  41614  41523  10293  41614  41614  41614  41614   \n",
              "2018-03-31  47136  47136  47105  46714  10789  47136  47136  47136  47136   \n",
              "\n",
              "             D_83   R_14   R_15   D_84   R_16  B_29   B_30   S_18   D_86  \\\n",
              "S_2                                                                        \n",
              "2017-03-31  30724  30730  30730  30676  30730  2655  30730  30730  30730   \n",
              "2017-04-30  31226  31233  31233  31175  31233  2667  31233  31233  31233   \n",
              "2017-05-31  31048  31048  31048  30987  31048  2327  31048  31048  31048   \n",
              "2017-06-30  31812  31815  31815  31755  31815  2557  31815  31815  31815   \n",
              "2017-07-31  32614  32620  32620  32548  32620  2608  32620  32620  32620   \n",
              "2017-08-31  33247  33253  33253  33190  33253  2696  33253  33253  33253   \n",
              "2017-09-30  33409  33427  33427  33384  33427  2683  33427  33427  33427   \n",
              "2017-10-31  34425  34439  34439  34375  34439  2678  34439  34439  34439   \n",
              "2017-11-30  35643  35664  35664  35643  35664  2812  35664  35664  35664   \n",
              "2017-12-31  36947  36977  36977  36967  36977  2785  36977  36977  36977   \n",
              "2018-01-31  38822  38957  38957  38878  38957  3191  38957  38957  38957   \n",
              "2018-02-28  40567  41614  41614  41502  41614  3580  41614  41614  41614   \n",
              "2018-03-31  43642  47136  47136  46690  47136  3574  47105  47136  47136   \n",
              "\n",
              "            D_87   R_17   R_18  D_88   B_31   S_19   R_19   B_32   S_20  \\\n",
              "S_2                                                                       \n",
              "2017-03-31    59  30730  30730   170  30730  30730  30730  30730  30730   \n",
              "2017-04-30    62  31233  31233   154  31233  31233  31233  31233  31233   \n",
              "2017-05-31    67  31048  31048   144  31048  31048  31048  31048  31048   \n",
              "2017-06-30    68  31815  31815   156  31815  31815  31815  31815  31815   \n",
              "2017-07-31    60  32620  32620   203  32620  32620  32620  32620  32620   \n",
              "2017-08-31    70  33253  33253   193  33253  33253  33253  33253  33253   \n",
              "2017-09-30    65  33427  33427   173  33427  33427  33427  33427  33427   \n",
              "2017-10-31    69  34439  34439   210  34439  34439  34439  34439  34439   \n",
              "2017-11-30    59  35664  35664   192  35664  35664  35664  35664  35664   \n",
              "2017-12-31    83  36977  36977   203  36977  36977  36977  36977  36977   \n",
              "2018-01-31    51  38957  38957   195  38957  38957  38957  38957  38957   \n",
              "2018-02-28    58  41614  41614   221  41614  41614  41614  41614  41614   \n",
              "2018-03-31    58  47136  47136   205  47136  47136  47136  47136  47136   \n",
              "\n",
              "             R_20   R_21   B_33   D_89   R_22   R_23   D_91   D_92   D_93  \\\n",
              "S_2                                                                         \n",
              "2017-03-31  30730  30730  30730  30676  30730  30730  30695  30730  30730   \n",
              "2017-04-30  31233  31233  31233  31175  31233  31233  31191  31233  31233   \n",
              "2017-05-31  31048  31048  31048  30987  31048  31048  31000  31048  31048   \n",
              "2017-06-30  31815  31815  31815  31755  31815  31815  31767  31815  31815   \n",
              "2017-07-31  32620  32620  32620  32548  32620  32620  32566  32620  32620   \n",
              "2017-08-31  33253  33253  33253  33190  33253  33253  33214  33253  33253   \n",
              "2017-09-30  33427  33427  33427  33384  33427  33427  33399  33427  33427   \n",
              "2017-10-31  34439  34439  34439  34375  34439  34439  34399  34439  34439   \n",
              "2017-11-30  35664  35664  35664  35643  35664  35664  35527  35664  35664   \n",
              "2017-12-31  36977  36977  36977  36967  36977  36977  36786  36977  36977   \n",
              "2018-01-31  38957  38957  38957  38878  38957  38957  37626  38957  38957   \n",
              "2018-02-28  41614  41614  41614  41502  41614  41614  38398  41614  41614   \n",
              "2018-03-31  47136  47136  47105  46690  47136  47136  39560  47136  47136   \n",
              "\n",
              "             D_94   R_24   R_25   D_96   S_22   S_23   S_24   S_25   S_26  \\\n",
              "S_2                                                                         \n",
              "2017-03-31  30730  30730  30730  30730  30729  30730  30729  30726  30730   \n",
              "2017-04-30  31233  31233  31233  31233  31232  31233  31232  31233  31233   \n",
              "2017-05-31  31048  31048  31048  31048  31048  31048  31048  31046  31048   \n",
              "2017-06-30  31815  31815  31815  31815  31813  31815  31813  31815  31815   \n",
              "2017-07-31  32620  32620  32620  32620  32620  32620  32620  32617  32620   \n",
              "2017-08-31  33253  33253  33253  33253  33251  33253  33251  33251  33253   \n",
              "2017-09-30  33427  33427  33427  33427  33426  33427  33426  33425  33427   \n",
              "2017-10-31  34439  34439  34439  34439  34437  34439  34437  34438  34439   \n",
              "2017-11-30  35664  35664  35664  35664  35662  35664  35662  35660  35664   \n",
              "2017-12-31  36977  36977  36977  36977  36977  36977  36977  36974  36977   \n",
              "2018-01-31  38957  38957  38957  38957  38957  38957  38957  38953  38957   \n",
              "2018-02-28  41614  41614  41614  41614  41611  41614  41611  41610  41614   \n",
              "2018-03-31  47136  47136  47136  47136  47032  47135  47033  47120  47136   \n",
              "\n",
              "            D_102  D_103  D_104  D_105  D_106  D_107   B_36   B_37  R_26  \\\n",
              "S_2                                                                        \n",
              "2017-03-31  30730  30631  30631  15821   3925  30631  30730  30730  3268   \n",
              "2017-04-30  31233  31126  31126  16191   3964  31126  31233  31233  3281   \n",
              "2017-05-31  31048  30934  30934  16014   4027  30934  31048  31048  3381   \n",
              "2017-06-30  31815  31713  31713  16558   4181  31713  31815  31815  3560   \n",
              "2017-07-31  32620  32501  32501  16889   4312  32501  32620  32620  3720   \n",
              "2017-08-31  33253  33133  33133  17288   4474  33133  33253  33253  3954   \n",
              "2017-09-30  33427  33338  33338  17432   4542  33338  33427  33427  4025   \n",
              "2017-10-31  34439  34322  34322  17864   4718  34322  34439  34439  4150   \n",
              "2017-11-30  35664  35588  35588  18460   4826  35588  35664  35664  4413   \n",
              "2017-12-31  36977  36919  36919  19327   5072  36919  36977  36977  4316   \n",
              "2018-01-31  38957  38804  38804  20155   5477  38804  38957  38957  4423   \n",
              "2018-02-28  41614  41388  41388  21211   5808  41388  41614  41614  4429   \n",
              "2018-03-31  47136  45984  45984  23644   6211  45984  47136  47136  4346   \n",
              "\n",
              "             R_27   B_38  D_108  D_109  D_110  D_111  B_39  D_112   B_40  \\\n",
              "S_2                                                                        \n",
              "2017-03-31  30329  30730   1383  30730    344    344   345  30730  30730   \n",
              "2017-04-30  30830  31233   1457  31233    299    299   300  31233  31233   \n",
              "2017-05-31  30637  31048   1431  31048    308    308   312  31048  31048   \n",
              "2017-06-30  31439  31815   1483  31815    307    307   307  31815  31815   \n",
              "2017-07-31  32176  32620   1629  32620    314    314   314  32620  32620   \n",
              "2017-08-31  32825  33253   1599  33253    320    320   323  33253  33253   \n",
              "2017-09-30  32987  33427   1548  33427    323    323   325  33427  33427   \n",
              "2017-10-31  33941  34439   1712  34439    293    293   294  34439  34439   \n",
              "2017-11-30  35205  35664   1702  35664    334    334   337  35664  35664   \n",
              "2017-12-31  35375  36977   1676  36977    291    291   291  36977  36977   \n",
              "2018-01-31  35495  38957   1800  38957    330    330   331  38957  38957   \n",
              "2018-02-28  35252  41614   1736  41614    316    316   318  41614  41614   \n",
              "2018-03-31  35434  47105   1774  47105    308    308   309  47105  47136   \n",
              "\n",
              "             S_27  D_113  D_114  D_115  D_116  D_117  D_118  D_119  D_120  \\\n",
              "S_2                                                                         \n",
              "2017-03-31  26216  30730  30730  30730  30730  30730  30730  30730  30730   \n",
              "2017-04-30  26683  31233  31233  31233  31233  31233  31233  31233  31233   \n",
              "2017-05-31  26598  31048  31048  31048  31048  31048  31048  31048  31048   \n",
              "2017-06-30  27327  31812  31812  31812  31812  31812  31812  31812  31812   \n",
              "2017-07-31  27942  32614  32614  32614  32614  32614  32614  32614  32614   \n",
              "2017-08-31  28638  33246  33246  33246  33246  33246  33246  33246  33246   \n",
              "2017-09-30  28864  33411  33411  33411  33411  33411  33411  33411  33411   \n",
              "2017-10-31  29787  34425  34425  34425  34425  34425  34425  34425  34425   \n",
              "2017-11-30  30943  35643  35643  35643  35643  35643  35643  35643  35643   \n",
              "2017-12-31  32118  36948  36948  36948  36948  36948  36948  36948  36948   \n",
              "2018-01-31  33861  38825  38825  38825  38825  38825  38825  38825  38825   \n",
              "2018-02-28  36258  40570  40570  40570  40570  40570  40570  40570  40570   \n",
              "2018-03-31  37941  43669  43669  43669  43669  43669  43669  43669  43669   \n",
              "\n",
              "            D_121  D_122  D_123  D_124  D_125  D_126  D_127  D_128  D_129  \\\n",
              "S_2                                                                         \n",
              "2017-03-31  30730  30730  30730  30730  30730  30730  30730  30631  30631   \n",
              "2017-04-30  31233  31233  31233  31233  31233  31233  31233  31126  31126   \n",
              "2017-05-31  31048  31048  31048  31048  31048  31048  31048  30934  30934   \n",
              "2017-06-30  31812  31812  31812  31812  31812  31815  31815  31713  31713   \n",
              "2017-07-31  32614  32614  32614  32614  32614  32620  32620  32501  32501   \n",
              "2017-08-31  33246  33246  33246  33246  33246  33253  33253  33133  33133   \n",
              "2017-09-30  33411  33411  33411  33411  33411  33427  33427  33338  33338   \n",
              "2017-10-31  34425  34425  34425  34425  34425  34439  34439  34322  34322   \n",
              "2017-11-30  35643  35643  35643  35643  35643  35664  35664  35588  35588   \n",
              "2017-12-31  36948  36948  36948  36948  36948  36977  36977  36919  36919   \n",
              "2018-01-31  38825  38825  38825  38825  38825  38957  38957  38804  38804   \n",
              "2018-02-28  40570  40570  40570  40570  40570  41614  41614  41388  41388   \n",
              "2018-03-31  43669  43669  43669  43669  43669  47136  47136  45984  45984   \n",
              "\n",
              "             B_41  B_42  D_130  D_131  D_132  D_133   R_28  D_134  D_135  \\\n",
              "S_2                                                                        \n",
              "2017-03-31  30730   482  30631  30631   3931  30730  30730   2270   2270   \n",
              "2017-04-30  31233   479  31126  31126   3970  31233  31233   2253   2253   \n",
              "2017-05-31  31048   493  30934  30934   4030  31048  31048   2248   2248   \n",
              "2017-06-30  31815   505  31713  31713   4187  31815  31815   2423   2423   \n",
              "2017-07-31  32620   431  32501  32501   4319  32620  32620   2453   2453   \n",
              "2017-08-31  33253   473  33133  33133   4480  33253  33253   2620   2620   \n",
              "2017-09-30  33427   508  33338  33338   4546  33427  33427   2635   2635   \n",
              "2017-10-31  34439   486  34322  34322   4724  34439  34439   2746   2746   \n",
              "2017-11-30  35664   494  35588  35588   4835  35664  35664   2871   2871   \n",
              "2017-12-31  36977   555  36919  36919   5078  36977  36977   2847   2847   \n",
              "2018-01-31  38957   492  38804  38804   5480  38957  38957   2990   2990   \n",
              "2018-02-28  41614   567  41388  41388   5812  41614  41614   2987   2987   \n",
              "2018-03-31  47136   544  45984  45984   6221  47136  47136   3213   3213   \n",
              "\n",
              "            D_136  D_137  D_138  D_139  D_140  D_141  D_142  D_143  D_144  \\\n",
              "S_2                                                                         \n",
              "2017-03-31   2270   2270   2270  30631  30730  30631   5477  30631  30730   \n",
              "2017-04-30   2253   2253   2253  31126  31233  31126   5593  31126  31233   \n",
              "2017-05-31   2248   2248   2248  30934  31048  30934   5593  30934  31048   \n",
              "2017-06-30   2423   2423   2423  31713  31815  31713   5938  31713  31815   \n",
              "2017-07-31   2453   2453   2453  32501  32620  32501   6049  32501  32620   \n",
              "2017-08-31   2620   2620   2620  33133  33253  33133   6244  33133  33253   \n",
              "2017-09-30   2635   2635   2635  33338  33427  33338   6237  33338  33427   \n",
              "2017-10-31   2746   2746   2746  34322  34439  34322   6416  34322  34439   \n",
              "2017-11-30   2871   2871   2871  35588  35664  35588   6738  35588  35664   \n",
              "2017-12-31   2847   2847   2847  36919  36977  36919   7058  36919  36977   \n",
              "2018-01-31   2990   2990   2990  38804  38957  38804   7464  38804  38957   \n",
              "2018-02-28   2987   2987   2987  41388  41614  41388   7951  41388  41614   \n",
              "2018-03-31   3213   3213   3213  45984  47136  45984   8822  45984  47136   \n",
              "\n",
              "            D_145  target  \n",
              "S_2                        \n",
              "2017-03-31  30631   30730  \n",
              "2017-04-30  31126   31233  \n",
              "2017-05-31  30934   31048  \n",
              "2017-06-30  31713   31815  \n",
              "2017-07-31  32501   32620  \n",
              "2017-08-31  33133   33253  \n",
              "2017-09-30  33338   33427  \n",
              "2017-10-31  34322   34439  \n",
              "2017-11-30  35588   35664  \n",
              "2017-12-31  36919   36977  \n",
              "2018-01-31  38804   38957  \n",
              "2018-02-28  41388   41614  \n",
              "2018-03-31  45984   47136  "
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#grouping it by months and checking the count. Set index S_2 in datetime format\n",
        "df1.set_index('S_2').groupby(pd.Grouper(freq = 'M')).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16WVCYZD6kEy",
        "outputId": "3dba8b68-de0f-46be-94dc-6c979b087332"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "customer_ID         0\n",
              "S_2                 0\n",
              "P_2              2434\n",
              "D_39                0\n",
              "B_1                 0\n",
              "                ...  \n",
              "D_142          373333\n",
              "D_143            2532\n",
              "D_144               0\n",
              "D_145            2532\n",
              "target              0\n",
              "Length: 191, dtype: int64"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#check null in each column. Basically checking in the Target column\n",
        "df1.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "iK6Ze98x6kEy",
        "outputId": "e6271874-41ea-4df3-80bc-ea74b5304863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "customer_ID object\n",
            "S_2 datetime64[ns]\n",
            "D_63 object\n",
            "D_64 object\n",
            "B_31 int64\n",
            "target int64\n"
          ]
        }
      ],
      "source": [
        "#Categorial data to convert to dummies.\n",
        "for i in df1.columns:\n",
        "    if df1[i].dtypes != 'float64':\n",
        "        print(i, df1[i].dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXWUin5-6kEy",
        "outputId": "a49963b4-63b2-4ded-a56f-575cb8c33824"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['CR', 'CO', 'CL', 'XZ', 'XM', 'XL'], dtype=object)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1['D_63'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws8gK4Ed6kEy",
        "outputId": "39cf3b55-3666-4f9c-b956-08bee0394fac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['O', 'R', 'U', '-1', None], dtype=object)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1['D_64'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wBtuUkH6kEy"
      },
      "source": [
        "#### Converting Categorical Variables to dummy variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zjZRETd6kEy",
        "outputId": "e0f745db-0c37-40e5-9a1f-c9ea5bf8c741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min:  2017-03-01 00:00:00 Max:  2018-03-31 00:00:00\n"
          ]
        }
      ],
      "source": [
        "#Categorical values = objects therefore they'll be converted into dummies.\n",
        "print(\"Min: \", df1['S_2'].min(), \"Max: \", df1['S_2'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KGiBEg76kEz"
      },
      "outputs": [],
      "source": [
        "#dummies. Then drop the original columns, 2 columns. D_63 and D_64 are dropped.\n",
        "#191 - 2\n",
        "df_new1 = pd.get_dummies(df1, columns = ['D_63', 'D_64'], drop_first = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FJhbSXU6kEz",
        "outputId": "5567ec64-da32-476f-ff4b-f1ef87786bdc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(458913, 197)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "eZLvkTr06kEz"
      },
      "outputs": [],
      "source": [
        "df_new1.to_csv('final_df.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-r3x7Sq6kEz",
        "outputId": "ca96e49e-b041-43b1-9290-b2d303edb224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on function read_csv in module pandas.io.parsers:\n",
            "\n",
            "read_csv(filepath_or_buffer: Union[ForwardRef('PathLike[str]'), str, IO[~T], io.RawIOBase, io.BufferedIOBase, io.TextIOBase, _io.TextIOWrapper, mmap.mmap], sep=<object object at 0x7f990d789410>, delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal: str = '.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, dialect=None, error_bad_lines=True, warn_bad_lines=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None, storage_options: Union[Dict[str, Any], NoneType] = None)\n",
            "    Read a comma-separated values (csv) file into DataFrame.\n",
            "    \n",
            "    Also supports optionally iterating or breaking of the file\n",
            "    into chunks.\n",
            "    \n",
            "    Additional help can be found in the online docs for\n",
            "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    filepath_or_buffer : str, path object or file-like object\n",
            "        Any valid string path is acceptable. The string could be a URL. Valid\n",
            "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
            "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
            "    \n",
            "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
            "    \n",
            "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
            "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
            "    sep : str, default ','\n",
            "        Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
            "        the separator, but the Python parsing engine can, meaning the latter will\n",
            "        be used and automatically detect the separator by Python's builtin sniffer\n",
            "        tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
            "        different from ``'\\s+'`` will be interpreted as regular expressions and\n",
            "        will also force the use of the Python parsing engine. Note that regex\n",
            "        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
            "    delimiter : str, default ``None``\n",
            "        Alias for sep.\n",
            "    header : int, list of int, default 'infer'\n",
            "        Row number(s) to use as the column names, and the start of the\n",
            "        data.  Default behavior is to infer the column names: if no names\n",
            "        are passed the behavior is identical to ``header=0`` and column\n",
            "        names are inferred from the first line of the file, if column\n",
            "        names are passed explicitly then the behavior is identical to\n",
            "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
            "        replace existing names. The header can be a list of integers that\n",
            "        specify row locations for a multi-index on the columns\n",
            "        e.g. [0,1,3]. Intervening rows that are not specified will be\n",
            "        skipped (e.g. 2 in this example is skipped). Note that this\n",
            "        parameter ignores commented lines and empty lines if\n",
            "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
            "        data rather than the first line of the file.\n",
            "    names : array-like, optional\n",
            "        List of column names to use. If the file contains a header row,\n",
            "        then you should explicitly pass ``header=0`` to override the column names.\n",
            "        Duplicates in this list are not allowed.\n",
            "    index_col : int, str, sequence of int / str, or False, default ``None``\n",
            "      Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
            "      string name or column index. If a sequence of int / str is given, a\n",
            "      MultiIndex is used.\n",
            "    \n",
            "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
            "      column as the index, e.g. when you have a malformed file with delimiters at\n",
            "      the end of each line.\n",
            "    usecols : list-like or callable, optional\n",
            "        Return a subset of the columns. If list-like, all elements must either\n",
            "        be positional (i.e. integer indices into the document columns) or strings\n",
            "        that correspond to column names provided either by the user in `names` or\n",
            "        inferred from the document header row(s). For example, a valid list-like\n",
            "        `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
            "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
            "        To instantiate a DataFrame from ``data`` with element order preserved use\n",
            "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
            "        in ``['foo', 'bar']`` order or\n",
            "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
            "        for ``['bar', 'foo']`` order.\n",
            "    \n",
            "        If callable, the callable function will be evaluated against the column\n",
            "        names, returning names where the callable function evaluates to True. An\n",
            "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
            "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
            "        parsing time and lower memory usage.\n",
            "    squeeze : bool, default False\n",
            "        If the parsed data only contains one column then return a Series.\n",
            "    prefix : str, optional\n",
            "        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
            "    mangle_dupe_cols : bool, default True\n",
            "        Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n",
            "        'X'...'X'. Passing in False will cause data to be overwritten if there\n",
            "        are duplicate names in the columns.\n",
            "    dtype : Type name or dict of column -> type, optional\n",
            "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
            "        'c': 'Int64'}\n",
            "        Use `str` or `object` together with suitable `na_values` settings\n",
            "        to preserve and not interpret dtype.\n",
            "        If converters are specified, they will be applied INSTEAD\n",
            "        of dtype conversion.\n",
            "    engine : {'c', 'python'}, optional\n",
            "        Parser engine to use. The C engine is faster while the python engine is\n",
            "        currently more feature-complete.\n",
            "    converters : dict, optional\n",
            "        Dict of functions for converting values in certain columns. Keys can either\n",
            "        be integers or column labels.\n",
            "    true_values : list, optional\n",
            "        Values to consider as True.\n",
            "    false_values : list, optional\n",
            "        Values to consider as False.\n",
            "    skipinitialspace : bool, default False\n",
            "        Skip spaces after delimiter.\n",
            "    skiprows : list-like, int or callable, optional\n",
            "        Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
            "        at the start of the file.\n",
            "    \n",
            "        If callable, the callable function will be evaluated against the row\n",
            "        indices, returning True if the row should be skipped and False otherwise.\n",
            "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
            "    skipfooter : int, default 0\n",
            "        Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
            "    nrows : int, optional\n",
            "        Number of rows of file to read. Useful for reading pieces of large files.\n",
            "    na_values : scalar, str, list-like, or dict, optional\n",
            "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
            "        per-column NA values.  By default the following values are interpreted as\n",
            "        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
            "        '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',\n",
            "        'nan', 'null'.\n",
            "    keep_default_na : bool, default True\n",
            "        Whether or not to include the default NaN values when parsing the data.\n",
            "        Depending on whether `na_values` is passed in, the behavior is as follows:\n",
            "    \n",
            "        * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
            "          is appended to the default NaN values used for parsing.\n",
            "        * If `keep_default_na` is True, and `na_values` are not specified, only\n",
            "          the default NaN values are used for parsing.\n",
            "        * If `keep_default_na` is False, and `na_values` are specified, only\n",
            "          the NaN values specified `na_values` are used for parsing.\n",
            "        * If `keep_default_na` is False, and `na_values` are not specified, no\n",
            "          strings will be parsed as NaN.\n",
            "    \n",
            "        Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
            "        `na_values` parameters will be ignored.\n",
            "    na_filter : bool, default True\n",
            "        Detect missing value markers (empty strings and the value of na_values). In\n",
            "        data without any NAs, passing na_filter=False can improve the performance\n",
            "        of reading a large file.\n",
            "    verbose : bool, default False\n",
            "        Indicate number of NA values placed in non-numeric columns.\n",
            "    skip_blank_lines : bool, default True\n",
            "        If True, skip over blank lines rather than interpreting as NaN values.\n",
            "    parse_dates : bool or list of int or names or list of lists or dict, default False\n",
            "        The behavior is as follows:\n",
            "    \n",
            "        * boolean. If True -> try parsing the index.\n",
            "        * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
            "          each as a separate date column.\n",
            "        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
            "          a single date column.\n",
            "        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
            "          result 'foo'\n",
            "    \n",
            "        If a column or index cannot be represented as an array of datetimes,\n",
            "        say because of an unparsable value or a mixture of timezones, the column\n",
            "        or index will be returned unaltered as an object data type. For\n",
            "        non-standard datetime parsing, use ``pd.to_datetime`` after\n",
            "        ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n",
            "        specify ``date_parser`` to be a partially-applied\n",
            "        :func:`pandas.to_datetime` with ``utc=True``. See\n",
            "        :ref:`io.csv.mixed_timezones` for more.\n",
            "    \n",
            "        Note: A fast-path exists for iso8601-formatted dates.\n",
            "    infer_datetime_format : bool, default False\n",
            "        If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
            "        format of the datetime strings in the columns, and if it can be inferred,\n",
            "        switch to a faster method of parsing them. In some cases this can increase\n",
            "        the parsing speed by 5-10x.\n",
            "    keep_date_col : bool, default False\n",
            "        If True and `parse_dates` specifies combining multiple columns then\n",
            "        keep the original columns.\n",
            "    date_parser : function, optional\n",
            "        Function to use for converting a sequence of string columns to an array of\n",
            "        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
            "        conversion. Pandas will try to call `date_parser` in three different ways,\n",
            "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
            "        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
            "        string values from the columns defined by `parse_dates` into a single array\n",
            "        and pass that; and 3) call `date_parser` once for each row using one or\n",
            "        more strings (corresponding to the columns defined by `parse_dates`) as\n",
            "        arguments.\n",
            "    dayfirst : bool, default False\n",
            "        DD/MM format dates, international and European format.\n",
            "    cache_dates : bool, default True\n",
            "        If True, use a cache of unique, converted dates to apply the datetime\n",
            "        conversion. May produce significant speed-up when parsing duplicate\n",
            "        date strings, especially ones with timezone offsets.\n",
            "    \n",
            "        .. versionadded:: 0.25.0\n",
            "    iterator : bool, default False\n",
            "        Return TextFileReader object for iteration or getting chunks with\n",
            "        ``get_chunk()``.\n",
            "    \n",
            "        .. versionchanged:: 1.2\n",
            "    \n",
            "           ``TextFileReader`` is a context manager.\n",
            "    chunksize : int, optional\n",
            "        Return TextFileReader object for iteration.\n",
            "        See the `IO Tools docs\n",
            "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
            "        for more information on ``iterator`` and ``chunksize``.\n",
            "    \n",
            "        .. versionchanged:: 1.2\n",
            "    \n",
            "           ``TextFileReader`` is a context manager.\n",
            "    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
            "        For on-the-fly decompression of on-disk data. If 'infer' and\n",
            "        `filepath_or_buffer` is path-like, then detect compression from the\n",
            "        following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n",
            "        decompression). If using 'zip', the ZIP file must contain only one data\n",
            "        file to be read in. Set to None for no decompression.\n",
            "    thousands : str, optional\n",
            "        Thousands separator.\n",
            "    decimal : str, default '.'\n",
            "        Character to recognize as decimal point (e.g. use ',' for European data).\n",
            "    lineterminator : str (length 1), optional\n",
            "        Character to break file into lines. Only valid with C parser.\n",
            "    quotechar : str (length 1), optional\n",
            "        The character used to denote the start and end of a quoted item. Quoted\n",
            "        items can include the delimiter and it will be ignored.\n",
            "    quoting : int or csv.QUOTE_* instance, default 0\n",
            "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
            "        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
            "    doublequote : bool, default ``True``\n",
            "       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
            "       whether or not to interpret two consecutive quotechar elements INSIDE a\n",
            "       field as a single ``quotechar`` element.\n",
            "    escapechar : str (length 1), optional\n",
            "        One-character string used to escape other characters.\n",
            "    comment : str, optional\n",
            "        Indicates remainder of line should not be parsed. If found at the beginning\n",
            "        of a line, the line will be ignored altogether. This parameter must be a\n",
            "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
            "        fully commented lines are ignored by the parameter `header` but not by\n",
            "        `skiprows`. For example, if ``comment='#'``, parsing\n",
            "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
            "        treated as the header.\n",
            "    encoding : str, optional\n",
            "        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
            "        standard encodings\n",
            "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
            "        .. versionchanged:: 1.2\n",
            "    \n",
            "           When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n",
            "           ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n",
            "           This behavior was previously only the case for ``engine=\"python\"``.\n",
            "    dialect : str or csv.Dialect, optional\n",
            "        If provided, this parameter will override values (default or not) for the\n",
            "        following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
            "        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
            "        override values, a ParserWarning will be issued. See csv.Dialect\n",
            "        documentation for more details.\n",
            "    error_bad_lines : bool, default True\n",
            "        Lines with too many fields (e.g. a csv line with too many commas) will by\n",
            "        default cause an exception to be raised, and no DataFrame will be returned.\n",
            "        If False, then these \"bad lines\" will dropped from the DataFrame that is\n",
            "        returned.\n",
            "    warn_bad_lines : bool, default True\n",
            "        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
            "        \"bad line\" will be output.\n",
            "    delim_whitespace : bool, default False\n",
            "        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
            "        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
            "        is set to True, nothing should be passed in for the ``delimiter``\n",
            "        parameter.\n",
            "    low_memory : bool, default True\n",
            "        Internally process the file in chunks, resulting in lower memory use\n",
            "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
            "        types either set False, or specify the type with the `dtype` parameter.\n",
            "        Note that the entire file is read into a single DataFrame regardless,\n",
            "        use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
            "        (Only valid with C parser).\n",
            "    memory_map : bool, default False\n",
            "        If a filepath is provided for `filepath_or_buffer`, map the file object\n",
            "        directly onto memory and access the data directly from there. Using this\n",
            "        option can improve performance because there is no longer any I/O overhead.\n",
            "    float_precision : str, optional\n",
            "        Specifies which converter the C engine should use for floating-point\n",
            "        values. The options are ``None`` or 'high' for the ordinary converter,\n",
            "        'legacy' for the original lower precision pandas converter, and\n",
            "        'round_trip' for the round-trip converter.\n",
            "    \n",
            "        .. versionchanged:: 1.2\n",
            "    \n",
            "    storage_options : dict, optional\n",
            "        Extra options that make sense for a particular storage connection, e.g.\n",
            "        host, port, username, password, etc., if using a URL that will\n",
            "        be parsed by ``fsspec``, e.g., starting \"s3://\", \"gcs://\". An error\n",
            "        will be raised if providing this argument with a non-fsspec URL.\n",
            "        See the fsspec and backend storage implementation docs for the set of\n",
            "        allowed keys and values.\n",
            "    \n",
            "        .. versionadded:: 1.2\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    DataFrame or TextParser\n",
            "        A comma-separated values (csv) file is returned as two-dimensional\n",
            "        data structure with labeled axes.\n",
            "    \n",
            "    See Also\n",
            "    --------\n",
            "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
            "    read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
            "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(pd.read_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "U3VvHUxP6kEz",
        "outputId": "bc2659b1-436d-41b1-b88a-7574cde16880"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(458913, 197)"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new = pd.read_csv('final_df.csv', index_col = False)\n",
        "df_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "angW9cY66kEz",
        "outputId": "bef3927a-6078-433b-9a07-8fef91b60014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "customer_ID\n",
            "S_2\n",
            "P_2\n",
            "D_39\n",
            "B_1\n",
            "B_2\n",
            "R_1\n",
            "S_3\n",
            "D_41\n",
            "B_3\n",
            "D_42\n",
            "D_43\n",
            "D_44\n",
            "B_4\n",
            "D_45\n",
            "B_5\n",
            "R_2\n",
            "D_46\n",
            "D_47\n",
            "D_48\n",
            "D_49\n",
            "B_6\n",
            "B_7\n",
            "B_8\n",
            "D_50\n",
            "D_51\n",
            "B_9\n",
            "R_3\n",
            "D_52\n",
            "P_3\n",
            "B_10\n",
            "D_53\n",
            "S_5\n",
            "B_11\n",
            "S_6\n",
            "D_54\n",
            "R_4\n",
            "S_7\n",
            "B_12\n",
            "S_8\n",
            "D_55\n",
            "D_56\n",
            "B_13\n",
            "R_5\n",
            "D_58\n",
            "S_9\n",
            "B_14\n",
            "D_59\n",
            "D_60\n",
            "D_61\n",
            "B_15\n",
            "S_11\n",
            "D_62\n",
            "D_65\n",
            "B_16\n",
            "B_17\n",
            "B_18\n",
            "B_19\n",
            "D_66\n",
            "B_20\n",
            "D_68\n",
            "S_12\n",
            "R_6\n",
            "S_13\n",
            "B_21\n",
            "D_69\n",
            "B_22\n",
            "D_70\n",
            "D_71\n",
            "D_72\n",
            "S_15\n",
            "B_23\n",
            "D_73\n",
            "P_4\n",
            "D_74\n",
            "D_75\n",
            "D_76\n",
            "B_24\n",
            "R_7\n",
            "D_77\n",
            "B_25\n",
            "B_26\n",
            "D_78\n",
            "D_79\n",
            "R_8\n",
            "R_9\n",
            "S_16\n",
            "D_80\n",
            "R_10\n",
            "R_11\n",
            "B_27\n",
            "D_81\n",
            "D_82\n",
            "S_17\n",
            "R_12\n",
            "B_28\n",
            "R_13\n",
            "D_83\n",
            "R_14\n",
            "R_15\n",
            "D_84\n",
            "R_16\n",
            "B_29\n",
            "B_30\n",
            "S_18\n",
            "D_86\n",
            "D_87\n",
            "R_17\n",
            "R_18\n",
            "D_88\n",
            "B_31\n",
            "S_19\n",
            "R_19\n",
            "B_32\n",
            "S_20\n",
            "R_20\n",
            "R_21\n",
            "B_33\n",
            "D_89\n",
            "R_22\n",
            "R_23\n",
            "D_91\n",
            "D_92\n",
            "D_93\n",
            "D_94\n",
            "R_24\n",
            "R_25\n",
            "D_96\n",
            "S_22\n",
            "S_23\n",
            "S_24\n",
            "S_25\n",
            "S_26\n",
            "D_102\n",
            "D_103\n",
            "D_104\n",
            "D_105\n",
            "D_106\n",
            "D_107\n",
            "B_36\n",
            "B_37\n",
            "R_26\n",
            "R_27\n",
            "B_38\n",
            "D_108\n",
            "D_109\n",
            "D_110\n",
            "D_111\n",
            "B_39\n",
            "D_112\n",
            "B_40\n",
            "S_27\n",
            "D_113\n",
            "D_114\n",
            "D_115\n",
            "D_116\n",
            "D_117\n",
            "D_118\n",
            "D_119\n",
            "D_120\n",
            "D_121\n",
            "D_122\n",
            "D_123\n",
            "D_124\n",
            "D_125\n",
            "D_126\n",
            "D_127\n",
            "D_128\n",
            "D_129\n",
            "B_41\n",
            "B_42\n",
            "D_130\n",
            "D_131\n",
            "D_132\n",
            "D_133\n",
            "R_28\n",
            "D_134\n",
            "D_135\n",
            "D_136\n",
            "D_137\n",
            "D_138\n",
            "D_139\n",
            "D_140\n",
            "D_141\n",
            "D_142\n",
            "D_143\n",
            "D_144\n",
            "D_145\n",
            "target\n",
            "D_63_CO\n",
            "D_63_CR\n",
            "D_63_XL\n",
            "D_63_XM\n",
            "D_63_XZ\n",
            "D_64_O\n",
            "D_64_R\n",
            "D_64_U\n"
          ]
        }
      ],
      "source": [
        "for i in df_new.columns:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTWozQWf6kE0"
      },
      "source": [
        "### Split Dataset in Train, Test1 & Test2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "tYv-bNY56kE0"
      },
      "outputs": [],
      "source": [
        "train_df = df_new.loc[(df_new['S_2'] >= '2017-05') & (df_new['S_2'] <= '2018-01'), :]\n",
        "test1_df = df_new.loc[(df_new['S_2'] <= '2017-04') , :]\n",
        "test2_df = df_new.loc[ (df_new['S_2'] >= '2018-02'), :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzHocTG36kE0",
        "outputId": "ebe523a6-c04f-4137-c327-3bd5f96b786b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(269243, 197)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROpOKG4U6kE0"
      },
      "outputs": [],
      "source": [
        "xtrain = train_df.drop(columns = ['target', 'customer_ID', 'S_2'])\n",
        "ytrain = train_df['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRgql1Bt6kE0",
        "outputId": "dcbeb2fa-4169-4bba-b35f-dc5ad828288e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(269243, 194)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xtrain.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "i9m68zew6kE0",
        "outputId": "0e737842-3ebf-4c7c-a482-7ec41bf7aa76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "218.29219317436218\n"
          ]
        }
      ],
      "source": [
        "t1 = t.time()\n",
        "m1 = xgb.XGBClassifier(random_state = 21)\n",
        "m1.fit(xtrain, ytrain)\n",
        "feat_imp = pd.DataFrame({'columns': xtrain.columns, 'feat_imp': m1.feature_importances_})\n",
        "feat_imp.loc[feat_imp['feat_imp'] > 0.005,:].sort_values(['feat_imp'], ascending = False)\n",
        "feat_imp.to_csv('feat_imp.csv')\n",
        "t2 = t.time()\n",
        "print(t2-t1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9rgNkT96kE1",
        "outputId": "4fcb958b-9f60-467c-82b9-612d44fb8316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on XGBClassifier in module xgboost.sklearn object:\n",
            "\n",
            "class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
            " |  XGBClassifier(*, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', use_label_encoder: Union[bool, NoneType] = None, **kwargs: Any) -> None\n",
            " |  \n",
            " |  Implementation of the scikit-learn API for XGBoost classification.\n",
            " |  \n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  \n",
            " |      n_estimators : int\n",
            " |          Number of boosting rounds.\n",
            " |  \n",
            " |      max_depth :  Optional[int]\n",
            " |          Maximum tree depth for base learners.\n",
            " |      max_leaves :\n",
            " |          Maximum number of leaves; 0 indicates no limit.\n",
            " |      max_bin :\n",
            " |          If using histogram-based algorithm, maximum number of bins per feature\n",
            " |      grow_policy :\n",
            " |          Tree growing policy. 0: favor splitting at nodes closest to the node, i.e. grow\n",
            " |          depth-wise. 1: favor splitting at nodes with highest loss change.\n",
            " |      learning_rate : Optional[float]\n",
            " |          Boosting learning rate (xgb's \"eta\")\n",
            " |      verbosity : Optional[int]\n",
            " |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
            " |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
            " |          Specify the learning task and the corresponding learning objective or\n",
            " |          a custom objective function to be used (see note below).\n",
            " |      booster: Optional[str]\n",
            " |          Specify which booster to use: gbtree, gblinear or dart.\n",
            " |      tree_method: Optional[str]\n",
            " |          Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
            " |          default, XGBoost will choose the most conservative option available.  It's\n",
            " |          recommended to study this option from the parameters document :doc:`tree method\n",
            " |          </treemethod>`\n",
            " |      n_jobs : Optional[int]\n",
            " |          Number of parallel threads used to run xgboost.  When used with other\n",
            " |          Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
            " |          parallelize and balance the threads.  Creating thread contention will\n",
            " |          significantly slow down both algorithms.\n",
            " |      gamma : Optional[float]\n",
            " |          (min_split_loss) Minimum loss reduction required to make a further partition on a\n",
            " |          leaf node of the tree.\n",
            " |      min_child_weight : Optional[float]\n",
            " |          Minimum sum of instance weight(hessian) needed in a child.\n",
            " |      max_delta_step : Optional[float]\n",
            " |          Maximum delta step we allow each tree's weight estimation to be.\n",
            " |      subsample : Optional[float]\n",
            " |          Subsample ratio of the training instance.\n",
            " |      sampling_method :\n",
            " |          Sampling method. Used only by `gpu_hist` tree method.\n",
            " |            - `uniform`: select random training instances uniformly.\n",
            " |            - `gradient_based` select random training instances with higher probability when\n",
            " |              the gradient and hessian are larger. (cf. CatBoost)\n",
            " |      colsample_bytree : Optional[float]\n",
            " |          Subsample ratio of columns when constructing each tree.\n",
            " |      colsample_bylevel : Optional[float]\n",
            " |          Subsample ratio of columns for each level.\n",
            " |      colsample_bynode : Optional[float]\n",
            " |          Subsample ratio of columns for each split.\n",
            " |      reg_alpha : Optional[float]\n",
            " |          L1 regularization term on weights (xgb's alpha).\n",
            " |      reg_lambda : Optional[float]\n",
            " |          L2 regularization term on weights (xgb's lambda).\n",
            " |      scale_pos_weight : Optional[float]\n",
            " |          Balancing of positive and negative weights.\n",
            " |      base_score : Optional[float]\n",
            " |          The initial prediction score of all instances, global bias.\n",
            " |      random_state : Optional[Union[numpy.random.RandomState, int]]\n",
            " |          Random number seed.\n",
            " |  \n",
            " |          .. note::\n",
            " |  \n",
            " |             Using gblinear booster with shotgun updater is nondeterministic as\n",
            " |             it uses Hogwild algorithm.\n",
            " |  \n",
            " |      missing : float, default np.nan\n",
            " |          Value in the data which needs to be present as a missing value.\n",
            " |      num_parallel_tree: Optional[int]\n",
            " |          Used for boosting random forest.\n",
            " |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
            " |          Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
            " |          for more information.\n",
            " |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
            " |          Constraints for interaction representing permitted interactions.  The\n",
            " |          constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
            " |          3, 4]]``, where each inner list is a group of indices of features that are\n",
            " |          allowed to interact with each other.  See :doc:`tutorial\n",
            " |          </tutorials/feature_interaction_constraint>` for more information\n",
            " |      importance_type: Optional[str]\n",
            " |          The feature importance type for the feature_importances\\_ property:\n",
            " |  \n",
            " |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
            " |            \"total_cover\".\n",
            " |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
            " |            without bias.\n",
            " |  \n",
            " |      gpu_id : Optional[int]\n",
            " |          Device ordinal.\n",
            " |      validate_parameters : Optional[bool]\n",
            " |          Give warnings for unknown parameter.\n",
            " |      predictor : Optional[str]\n",
            " |          Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
            " |          gpu_predictor].\n",
            " |      enable_categorical : bool\n",
            " |  \n",
            " |          .. versionadded:: 1.5.0\n",
            " |  \n",
            " |          .. note:: This parameter is experimental\n",
            " |  \n",
            " |          Experimental support for categorical data.  When enabled, cudf/pandas.DataFrame\n",
            " |          should be used to specify categorical data type.  Also, JSON/UBJSON\n",
            " |          serialization format is required.\n",
            " |  \n",
            " |      feature_types : FeatureTypes\n",
            " |  \n",
            " |          .. versionadded:: 1.7.0\n",
            " |  \n",
            " |          Used for specifying feature types without constructing a dataframe. See\n",
            " |          :py:class:`DMatrix` for details.\n",
            " |  \n",
            " |      max_cat_to_onehot : Optional[int]\n",
            " |  \n",
            " |          .. versionadded:: 1.6.0\n",
            " |  \n",
            " |          .. note:: This parameter is experimental\n",
            " |  \n",
            " |          A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
            " |          for categorical data.  When number of categories is lesser than the threshold\n",
            " |          then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
            " |          into children nodes. Also, `enable_categorical` needs to be set to have\n",
            " |          categorical feature support. See :doc:`Categorical Data\n",
            " |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            " |  \n",
            " |      max_cat_threshold : Optional[int]\n",
            " |  \n",
            " |          .. versionadded:: 1.7.0\n",
            " |  \n",
            " |          .. note:: This parameter is experimental\n",
            " |  \n",
            " |          Maximum number of categories considered for each split. Used only by\n",
            " |          partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
            " |          needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
            " |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            " |  \n",
            " |      eval_metric : Optional[Union[str, List[str], Callable]]\n",
            " |  \n",
            " |          .. versionadded:: 1.6.0\n",
            " |  \n",
            " |          Metric used for monitoring the training result and early stopping.  It can be a\n",
            " |          string or list of strings as names of predefined metric in XGBoost (See\n",
            " |          doc/parameter.rst), one of the metrics in :py:mod:`sklearn.metrics`, or any other\n",
            " |          user defined metric that looks like `sklearn.metrics`.\n",
            " |  \n",
            " |          If custom objective is also provided, then custom metric should implement the\n",
            " |          corresponding reverse link function.\n",
            " |  \n",
            " |          Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
            " |          object is provided, it's assumed to be a cost function and by default XGBoost will\n",
            " |          minimize the result during early stopping.\n",
            " |  \n",
            " |          For advanced usage on Early stopping like directly choosing to maximize instead of\n",
            " |          minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
            " |  \n",
            " |          See :doc:`Custom Objective and Evaluation Metric </tutorials/custom_metric_obj>`\n",
            " |          for more.\n",
            " |  \n",
            " |          .. note::\n",
            " |  \n",
            " |               This parameter replaces `eval_metric` in :py:meth:`fit` method.  The old one\n",
            " |               receives un-transformed prediction regardless of whether custom objective is\n",
            " |               being used.\n",
            " |  \n",
            " |          .. code-block:: python\n",
            " |  \n",
            " |              from sklearn.datasets import load_diabetes\n",
            " |              from sklearn.metrics import mean_absolute_error\n",
            " |              X, y = load_diabetes(return_X_y=True)\n",
            " |              reg = xgb.XGBRegressor(\n",
            " |                  tree_method=\"hist\",\n",
            " |                  eval_metric=mean_absolute_error,\n",
            " |              )\n",
            " |              reg.fit(X, y, eval_set=[(X, y)])\n",
            " |  \n",
            " |      early_stopping_rounds : Optional[int]\n",
            " |  \n",
            " |          .. versionadded:: 1.6.0\n",
            " |  \n",
            " |          Activates early stopping. Validation metric needs to improve at least once in\n",
            " |          every **early_stopping_rounds** round(s) to continue training.  Requires at least\n",
            " |          one item in **eval_set** in :py:meth:`fit`.\n",
            " |  \n",
            " |          The method returns the model from the last iteration (not the best one).  If\n",
            " |          there's more than one item in **eval_set**, the last entry will be used for early\n",
            " |          stopping.  If there's more than one metric in **eval_metric**, the last metric\n",
            " |          will be used for early stopping.\n",
            " |  \n",
            " |          If early stopping occurs, the model will have three additional fields:\n",
            " |          :py:attr:`best_score`, :py:attr:`best_iteration` and\n",
            " |          :py:attr:`best_ntree_limit`.\n",
            " |  \n",
            " |          .. note::\n",
            " |  \n",
            " |              This parameter replaces `early_stopping_rounds` in :py:meth:`fit` method.\n",
            " |  \n",
            " |      callbacks : Optional[List[TrainingCallback]]\n",
            " |          List of callback functions that are applied at end of each iteration.\n",
            " |          It is possible to use predefined callbacks by using\n",
            " |          :ref:`Callback API <callback_api>`.\n",
            " |  \n",
            " |          .. note::\n",
            " |  \n",
            " |             States in callback are not preserved during training, which means callback\n",
            " |             objects can not be reused for multiple training sessions without\n",
            " |             reinitialization or deepcopy.\n",
            " |  \n",
            " |          .. code-block:: python\n",
            " |  \n",
            " |              for params in parameters_grid:\n",
            " |                  # be sure to (re)initialize the callbacks before each run\n",
            " |                  callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
            " |                  xgboost.train(params, Xy, callbacks=callbacks)\n",
            " |  \n",
            " |      kwargs : dict, optional\n",
            " |          Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
            " |          can be found :doc:`here </parameter>`.\n",
            " |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
            " |          dict simultaneously will result in a TypeError.\n",
            " |  \n",
            " |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
            " |  \n",
            " |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
            " |              that parameters passed via this argument will interact properly\n",
            " |              with scikit-learn.\n",
            " |  \n",
            " |          .. note::  Custom objective function\n",
            " |  \n",
            " |              A custom objective function can be provided for the ``objective``\n",
            " |              parameter. In this case, it should have the signature\n",
            " |              ``objective(y_true, y_pred) -> grad, hess``:\n",
            " |  \n",
            " |              y_true: array_like of shape [n_samples]\n",
            " |                  The target values\n",
            " |              y_pred: array_like of shape [n_samples]\n",
            " |                  The predicted values\n",
            " |  \n",
            " |              grad: array_like of shape [n_samples]\n",
            " |                  The value of the gradient for each sample point.\n",
            " |              hess: array_like of shape [n_samples]\n",
            " |                  The value of the second derivative for each sample point\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      XGBClassifier\n",
            " |      XGBModel\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', use_label_encoder: Union[bool, NoneType] = None, **kwargs: Any) -> None\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X: Any, y: Any, *, sample_weight: Union[Any, NoneType] = None, base_margin: Union[Any, NoneType] = None, eval_set: Union[Sequence[Tuple[Any, Any]], NoneType] = None, eval_metric: Union[str, Sequence[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType] = None, early_stopping_rounds: Union[int, NoneType] = None, verbose: Union[bool, int, NoneType] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Union[Sequence[Any], NoneType] = None, base_margin_eval_set: Union[Sequence[Any], NoneType] = None, feature_weights: Union[Any, NoneType] = None, callbacks: Union[Sequence[xgboost.callback.TrainingCallback], NoneType] = None) -> 'XGBClassifier'\n",
            " |      Fit gradient boosting classifier.\n",
            " |      \n",
            " |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
            " |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
            " |      pass ``xgb_model`` argument.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X :\n",
            " |          Feature matrix\n",
            " |      y :\n",
            " |          Labels\n",
            " |      sample_weight :\n",
            " |          instance weights\n",
            " |      base_margin :\n",
            " |          global bias for each instance.\n",
            " |      eval_set :\n",
            " |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
            " |          metrics will be computed.\n",
            " |          Validation metrics will help us track the performance of the model.\n",
            " |      \n",
            " |      eval_metric : str, list of str, or callable, optional\n",
            " |          .. deprecated:: 1.6.0\n",
            " |              Use `eval_metric` in :py:meth:`__init__` or :py:meth:`set_params` instead.\n",
            " |      \n",
            " |      early_stopping_rounds : int\n",
            " |          .. deprecated:: 1.6.0\n",
            " |              Use `early_stopping_rounds` in :py:meth:`__init__` or\n",
            " |              :py:meth:`set_params` instead.\n",
            " |      verbose :\n",
            " |          If `verbose` is True and an evaluation set is used, the evaluation metric\n",
            " |          measured on the validation set is printed to stdout at each boosting stage.\n",
            " |          If `verbose` is an integer, the evaluation metric is printed at each `verbose`\n",
            " |          boosting stage. The last boosting stage / the boosting stage found by using\n",
            " |          `early_stopping_rounds` is also printed.\n",
            " |      xgb_model :\n",
            " |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
            " |          loaded before training (allows training continuation).\n",
            " |      sample_weight_eval_set :\n",
            " |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
            " |          object storing instance weights for the i-th validation set.\n",
            " |      base_margin_eval_set :\n",
            " |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
            " |          object storing base margin for the i-th validation set.\n",
            " |      feature_weights :\n",
            " |          Weight for each feature, defines the probability of each feature being\n",
            " |          selected when colsample is being used.  All values must be greater than 0,\n",
            " |          otherwise a `ValueError` is thrown.\n",
            " |      \n",
            " |      callbacks :\n",
            " |          .. deprecated:: 1.6.0\n",
            " |              Use `callbacks` in :py:meth:`__init__` or :py:meth:`set_params` instead.\n",
            " |  \n",
            " |  predict(self, X: Any, output_margin: bool = False, ntree_limit: Union[int, NoneType] = None, validate_features: bool = True, base_margin: Union[Any, NoneType] = None, iteration_range: Union[Tuple[int, int], NoneType] = None) -> numpy.ndarray\n",
            " |      Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\n",
            " |      is used automatically.  For tree models, when data is on GPU, like cupy array or\n",
            " |      cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\n",
            " |      automatically, otherwise it will run on CPU.\n",
            " |      \n",
            " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X :\n",
            " |          Data to predict with.\n",
            " |      output_margin :\n",
            " |          Whether to output the raw untransformed margin value.\n",
            " |      ntree_limit :\n",
            " |          Deprecated, use `iteration_range` instead.\n",
            " |      validate_features :\n",
            " |          When this is True, validate that the Booster's and data's feature_names are\n",
            " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
            " |      base_margin :\n",
            " |          Margin added to prediction.\n",
            " |      iteration_range :\n",
            " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
            " |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
            " |          20)``, then only the forests built during [10, 20) (half open set) rounds\n",
            " |          are used in this prediction.\n",
            " |      \n",
            " |          .. versionadded:: 1.4.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      prediction\n",
            " |  \n",
            " |  predict_proba(self, X: Any, ntree_limit: Union[int, NoneType] = None, validate_features: bool = True, base_margin: Union[Any, NoneType] = None, iteration_range: Union[Tuple[int, int], NoneType] = None) -> numpy.ndarray\n",
            " |      Predict the probability of each `X` example being of a given class.\n",
            " |      \n",
            " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array_like\n",
            " |          Feature matrix.\n",
            " |      ntree_limit : int\n",
            " |          Deprecated, use `iteration_range` instead.\n",
            " |      validate_features : bool\n",
            " |          When this is True, validate that the Booster's and data's feature_names are\n",
            " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
            " |      base_margin : array_like\n",
            " |          Margin added to prediction.\n",
            " |      iteration_range :\n",
            " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
            " |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
            " |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
            " |          used in this prediction.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      prediction :\n",
            " |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n",
            " |          probability of each data example being of a given class.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from XGBModel:\n",
            " |  \n",
            " |  __sklearn_is_fitted__(self) -> bool\n",
            " |  \n",
            " |  apply(self, X: Any, ntree_limit: int = 0, iteration_range: Union[Tuple[int, int], NoneType] = None) -> numpy.ndarray\n",
            " |      Return the predicted leaf every tree for each sample. If the model is trained with\n",
            " |      early stopping, then `best_iteration` is used automatically.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array_like, shape=[n_samples, n_features]\n",
            " |          Input features matrix.\n",
            " |      \n",
            " |      iteration_range :\n",
            " |          See :py:meth:`predict`.\n",
            " |      \n",
            " |      ntree_limit :\n",
            " |          Deprecated, use ``iteration_range`` instead.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
            " |          For each datapoint x in X and for each tree, return the index of the\n",
            " |          leaf x ends up in. Leaves are numbered within\n",
            " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
            " |  \n",
            " |  evals_result(self) -> Dict[str, Dict[str, List[float]]]\n",
            " |      Return the evaluation results.\n",
            " |      \n",
            " |      If **eval_set** is passed to the :py:meth:`fit` function, you can call\n",
            " |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.  When\n",
            " |      **eval_metric** is also passed to the :py:meth:`fit` function, the\n",
            " |      **evals_result** will contain the **eval_metrics** passed to the :py:meth:`fit`\n",
            " |      function.\n",
            " |      \n",
            " |      The returned evaluation result is a dictionary:\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
            " |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      evals_result\n",
            " |  \n",
            " |  get_booster(self) -> xgboost.core.Booster\n",
            " |      Get the underlying xgboost Booster of this model.\n",
            " |      \n",
            " |      This will raise an exception when fit was not called\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      booster : a xgboost booster of underlying model\n",
            " |  \n",
            " |  get_num_boosting_rounds(self) -> int\n",
            " |      Gets the number of xgboost boosting rounds.\n",
            " |  \n",
            " |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
            " |      Get parameters.\n",
            " |  \n",
            " |  get_xgb_params(self) -> Dict[str, Any]\n",
            " |      Get xgboost specific parameters.\n",
            " |  \n",
            " |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n",
            " |      Load the model from a file or bytearray. Path to file can be local\n",
            " |      or as an URI.\n",
            " |      \n",
            " |      The model is loaded from XGBoost format which is universal among the various\n",
            " |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
            " |      feature_names) will not be loaded when using binary format.  To save those\n",
            " |      attributes, use JSON/UBJ instead.  See :doc:`Model IO </tutorials/saving_model>`\n",
            " |      for more info.\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |        model.load_model(\"model.json\")\n",
            " |        # or\n",
            " |        model.load_model(\"model.ubj\")\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname :\n",
            " |          Input file name or memory buffer(see also save_raw)\n",
            " |  \n",
            " |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
            " |      Save the model to a file.\n",
            " |      \n",
            " |      The model is saved in an XGBoost internal format which is universal among the\n",
            " |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            " |      (such as feature_names) will not be saved when using binary format.  To save\n",
            " |      those attributes, use JSON/UBJ instead. See :doc:`Model IO\n",
            " |      </tutorials/saving_model>` for more info.\n",
            " |      \n",
            " |      .. code-block:: python\n",
            " |      \n",
            " |        model.save_model(\"model.json\")\n",
            " |        # or\n",
            " |        model.save_model(\"model.ubj\")\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : string or os.PathLike\n",
            " |          Output file name\n",
            " |  \n",
            " |  set_params(self, **params: Any) -> 'XGBModel'\n",
            " |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
            " |      allow unknown kwargs. This allows using the full range of xgboost\n",
            " |      parameters that are not defined as member variables in sklearn grid\n",
            " |      search.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from XGBModel:\n",
            " |  \n",
            " |  best_iteration\n",
            " |      The best iteration obtained by early stopping.  This attribute is 0-based,\n",
            " |      for instance if the best iteration is the first round, then best_iteration is 0.\n",
            " |  \n",
            " |  best_ntree_limit\n",
            " |  \n",
            " |  best_score\n",
            " |      The best score obtained by early stopping.\n",
            " |  \n",
            " |  coef_\n",
            " |      Coefficients property\n",
            " |      \n",
            " |      .. note:: Coefficients are defined only for linear learners\n",
            " |      \n",
            " |          Coefficients are only defined when the linear model is chosen as\n",
            " |          base learner (`booster=gblinear`). It is not defined for other base\n",
            " |          learner types, such as tree learners (`booster=gbtree`).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
            " |  \n",
            " |  feature_importances_\n",
            " |      Feature importances property, return depends on `importance_type`\n",
            " |      parameter. When model trained with multi-class/multi-label/multi-target dataset,\n",
            " |      the feature importance is \"averaged\" over all targets. The \"average\" is defined\n",
            " |      based on the importance type. For instance, if the importance type is\n",
            " |      \"total_gain\", then the score is sum of loss change for each split from all\n",
            " |      trees.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
            " |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
            " |  \n",
            " |  feature_names_in_\n",
            " |      Names of features seen during :py:meth:`fit`.  Defined only when `X` has feature\n",
            " |      names that are all strings.\n",
            " |  \n",
            " |  intercept_\n",
            " |      Intercept (bias) property\n",
            " |      \n",
            " |      .. note:: Intercept is defined only for linear learners\n",
            " |      \n",
            " |          Intercept (bias) is only defined when the linear model is chosen as base\n",
            " |          learner (`booster=gblinear`). It is not defined for other base learner types,\n",
            " |          such as tree learners (`booster=gbtree`).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
            " |  \n",
            " |  n_features_in_\n",
            " |      Number of features seen during :py:meth:`fit`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for `X`.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(xgb.XGBClassifier())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "r4odv9bx6kE1",
        "outputId": "1fcbb827-a799-44d1-b220-d80281f58700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "146.99498295783997\n"
          ]
        }
      ],
      "source": [
        "t1 = t.time() #max_dept = max number of nodes, subsample = % of rows considered from the total data in the main tree at once, colsample_bytree = % of columns considered from the total data in the main tree at once\n",
        "m2 = xgb.XGBClassifier(n_estimators = 300, learning_rate = 0.5, \n",
        "                       max_depth = 4, subsample = .5, colsample_bytree = 0.5, scale_pos_weight = 5, random_state =21 )\n",
        "m2.fit(xtrain, ytrain)\n",
        "feat_imp1 = pd.DataFrame({'columns': xtrain.columns, 'feat_imp': m2.feature_importances_})\n",
        "feat_imp1.loc[feat_imp1['feat_imp'] > 0.005,:].sort_values(['feat_imp'], ascending = False)\n",
        "feat_imp1.to_csv('feat_imp1.csv')\n",
        "t2 = t.time()\n",
        "print(t2-t1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFTCEWr56kE1",
        "outputId": "de149434-ca9c-403b-86d0-64210e91de9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(415602, 190)\n",
            "31.137080669403076\n"
          ]
        }
      ],
      "source": [
        "#the total time taken to run the code\n",
        "start = t.time()\n",
        "df_rnd1 = train.loc[(train['S_2'].dt.year == 2017) & (train['S_2'].dt.month == 7), :]\n",
        "print(df_rnd1.shape)\n",
        "end = t.time()\n",
        "print(end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLTtkh_Z6kE1",
        "outputId": "8329ab50-c4cc-40ac-f968-6242366a41a7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>columns</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>P_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>R_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>S_3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>D_41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>B_3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>D_42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>D_43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>D_44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>B_4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>D_45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>15</td>\n",
              "      <td>D_46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>17</td>\n",
              "      <td>D_48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>18</td>\n",
              "      <td>D_49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>21</td>\n",
              "      <td>B_8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>22</td>\n",
              "      <td>D_50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>23</td>\n",
              "      <td>D_51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>28</td>\n",
              "      <td>B_10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>29</td>\n",
              "      <td>D_53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>35</td>\n",
              "      <td>S_7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>53</td>\n",
              "      <td>B_17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>56</td>\n",
              "      <td>D_66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>72</td>\n",
              "      <td>D_74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>73</td>\n",
              "      <td>D_75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>74</td>\n",
              "      <td>D_76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>77</td>\n",
              "      <td>D_77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>107</td>\n",
              "      <td>D_88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>127</td>\n",
              "      <td>S_23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>139</td>\n",
              "      <td>R_26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>141</td>\n",
              "      <td>B_38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>144</td>\n",
              "      <td>D_110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>145</td>\n",
              "      <td>D_111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>146</td>\n",
              "      <td>B_39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>171</td>\n",
              "      <td>D_132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>191</td>\n",
              "      <td>D_64_O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index columns\n",
              "0       0     P_2\n",
              "1       2     B_1\n",
              "2       3     B_2\n",
              "3       4     R_1\n",
              "4       5     S_3\n",
              "5       6    D_41\n",
              "6       7     B_3\n",
              "7       8    D_42\n",
              "8       9    D_43\n",
              "9      10    D_44\n",
              "10     11     B_4\n",
              "11     12    D_45\n",
              "12     15    D_46\n",
              "13     17    D_48\n",
              "14     18    D_49\n",
              "15     21     B_8\n",
              "16     22    D_50\n",
              "17     23    D_51\n",
              "18     28    B_10\n",
              "19     29    D_53\n",
              "20     35     S_7\n",
              "21     53    B_17\n",
              "22     56    D_66\n",
              "23     72    D_74\n",
              "24     73    D_75\n",
              "25     74    D_76\n",
              "26     77    D_77\n",
              "27    107    D_88\n",
              "28    127    S_23\n",
              "29    139    R_26\n",
              "30    141    B_38\n",
              "31    144   D_110\n",
              "32    145   D_111\n",
              "33    146    B_39\n",
              "34    171   D_132\n",
              "35    191  D_64_O"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#segregated the features based on their feature importance > 0.005 for model imp1\n",
        "feat_imp1.loc[feat_imp1['feat_imp'] > 0.005, 'columns'].reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlpvGnIv6kE1",
        "outputId": "cfb04dd3-9eee-4d81-982c-02bafd4964f0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>columns</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>P_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B_2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>R_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>S_3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>D_41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>B_3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>D_42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>D_43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>D_44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>B_4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>D_45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>15</td>\n",
              "      <td>D_46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>17</td>\n",
              "      <td>D_48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>18</td>\n",
              "      <td>D_49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>21</td>\n",
              "      <td>B_8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>22</td>\n",
              "      <td>D_50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>23</td>\n",
              "      <td>D_51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>28</td>\n",
              "      <td>B_10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>29</td>\n",
              "      <td>D_53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>35</td>\n",
              "      <td>S_7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>53</td>\n",
              "      <td>B_17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>56</td>\n",
              "      <td>D_66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>72</td>\n",
              "      <td>D_74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>73</td>\n",
              "      <td>D_75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>74</td>\n",
              "      <td>D_76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>77</td>\n",
              "      <td>D_77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>107</td>\n",
              "      <td>D_88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>127</td>\n",
              "      <td>S_23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>139</td>\n",
              "      <td>R_26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>141</td>\n",
              "      <td>B_38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>144</td>\n",
              "      <td>D_110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>145</td>\n",
              "      <td>D_111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>146</td>\n",
              "      <td>B_39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>171</td>\n",
              "      <td>D_132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>191</td>\n",
              "      <td>D_64_O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index columns\n",
              "0       0     P_2\n",
              "1       2     B_1\n",
              "2       3     B_2\n",
              "3       4     R_1\n",
              "4       5     S_3\n",
              "5       6    D_41\n",
              "6       7     B_3\n",
              "7       8    D_42\n",
              "8       9    D_43\n",
              "9      10    D_44\n",
              "10     11     B_4\n",
              "11     12    D_45\n",
              "12     15    D_46\n",
              "13     17    D_48\n",
              "14     18    D_49\n",
              "15     21     B_8\n",
              "16     22    D_50\n",
              "17     23    D_51\n",
              "18     28    B_10\n",
              "19     29    D_53\n",
              "20     35     S_7\n",
              "21     53    B_17\n",
              "22     56    D_66\n",
              "23     72    D_74\n",
              "24     73    D_75\n",
              "25     74    D_76\n",
              "26     77    D_77\n",
              "27    107    D_88\n",
              "28    127    S_23\n",
              "29    139    R_26\n",
              "30    141    B_38\n",
              "31    144   D_110\n",
              "32    145   D_111\n",
              "33    146    B_39\n",
              "34    171   D_132\n",
              "35    191  D_64_O"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#segregated the features based on their feature importance > 0.005 for model imp1\n",
        "feat_imp.loc[feat_imp1['feat_imp'] > 0.005, 'columns'].reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r84npl326kE1",
        "outputId": "e44504cd-309e-4fa1-d283-fd21c101fa2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['D_50', 'R_27', 'S_3', 'B_17', 'B_39', 'D_48', 'D_77', 'D_75', 'D_53', 'D_132', 'B_1', 'B_3', 'D_44', 'B_2', 'D_42', 'B_9', 'D_49', 'D_111', 'D_45', 'D_88', 'B_5', 'D_66', 'B_38', 'S_23', 'D_43', 'D_63_CO', 'D_41', 'D_51', 'P_2', 'B_7', 'R_26', 'D_74', 'D_62', 'B_4', 'D_56', 'B_10', 'B_8', 'D_46', 'D_76', 'D_110', 'P_3', 'D_64_O', 'S_7', 'R_1']\n"
          ]
        }
      ],
      "source": [
        "#union the above two chunk's output based on the filter by ignoring values that occur twice. Eg : A B C A D E - here we consider 5 values\n",
        "col = list(set(feat_imp1.loc[feat_imp1['feat_imp'] > 0.005, 'columns'].to_list()).union(set(feat_imp.loc[feat_imp['feat_imp'] > 0.005, 'columns'].to_list())))\n",
        "print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaKUBT-h6kE2",
        "outputId": "903d07ad-9ec4-4e6d-c8c0-d8c5b160b077"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctkQmVbL6kE2",
        "outputId": "84ecbb1a-67b1-4d87-c2cf-a32768d37893"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(458913, 44)"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# : = all rows\n",
        "df = df_new.loc[:, col]\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03GWmt2w6kE2",
        "outputId": "ee51d663-0573-46fb-ed4c-9cd4f1e66c27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(269243, 197)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPpicyBD6kE2"
      },
      "outputs": [],
      "source": [
        "#Refer section \"Split Dataset in Train, Test1 & Test2\".\n",
        "#train_df contains 197 columns within the range of dates assigned to it. Now, after we assign the [col] to it, we are only choosing the columns that are in the col variable i.e. there are 44\n",
        "xtrain = train_df[col]\n",
        "xtest1 = test1_df[col]\n",
        "xtest2 = test2_df[col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BqeZKCm6kE2"
      },
      "outputs": [],
      "source": [
        "#refer the same train_df variable output\n",
        "ytrain = train_df['target']\n",
        "ytest1 = test1_df['target']\n",
        "ytest2 = test2_df['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmTUa3za6kE2"
      },
      "outputs": [],
      "source": [
        "#saving it to csv\n",
        "xtrain.to_csv('xtrain.csv', index = False)\n",
        "xtest1.to_csv('xtest1.csv', index = False)\n",
        "xtest2.to_csv('xtest2.csv', index = False)\n",
        "ytest1.to_csv('ytest1.csv', index = False)\n",
        "ytest2.to_csv('ytest2.csv', index = False)\n",
        "ytrain.to_csv('ytrain.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb5vAzji6kE3",
        "outputId": "4296b36f-3b98-4562-f4de-949c0f74e19b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['customer_ID', 'S_2', 'P_2', 'D_39', 'B_1', 'B_2', 'R_1', 'S_3', 'D_41',\n",
              "       'B_3',\n",
              "       ...\n",
              "       'D_145', 'target', 'D_63_CO', 'D_63_CR', 'D_63_XL', 'D_63_XM',\n",
              "       'D_63_XZ', 'D_64_O', 'D_64_R', 'D_64_U'],\n",
              "      dtype='object', length=197)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test2_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bRhcZlXQ6kE3",
        "outputId": "8b1acec8-cfb0-472a-92e8-6f58e1e9fbd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(269243, 44)"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xtrain.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-afYAh76kE3",
        "outputId": "4d550461-be3e-4077-a4c2-f98e1b4a170d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(269243,)"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ytrain.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cpeZt3x6kE3",
        "outputId": "7f350b16-c4a8-4ec1-f01e-1d6e2f0c2d5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "5724.6286079883575\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>No. of Trees</th>\n",
              "      <th>LR</th>\n",
              "      <th>Subsample</th>\n",
              "      <th>%features</th>\n",
              "      <th>Default Weight</th>\n",
              "      <th>AUC Train</th>\n",
              "      <th>AUC Test1</th>\n",
              "      <th>AUC Test2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.930578</td>\n",
              "      <td>0.918653</td>\n",
              "      <td>0.93205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.928086</td>\n",
              "      <td>0.91662</td>\n",
              "      <td>0.928584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>10</td>\n",
              "      <td>0.926039</td>\n",
              "      <td>0.914617</td>\n",
              "      <td>0.925916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.925662</td>\n",
              "      <td>0.912498</td>\n",
              "      <td>0.929065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.92363</td>\n",
              "      <td>0.911914</td>\n",
              "      <td>0.9262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>500</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.961602</td>\n",
              "      <td>0.930949</td>\n",
              "      <td>0.941255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>500</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.5</td>\n",
              "      <td>10</td>\n",
              "      <td>0.960659</td>\n",
              "      <td>0.929978</td>\n",
              "      <td>0.940514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>500</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.96475</td>\n",
              "      <td>0.93116</td>\n",
              "      <td>0.942132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>500</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.963798</td>\n",
              "      <td>0.930387</td>\n",
              "      <td>0.941157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>500</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>0.962328</td>\n",
              "      <td>0.929791</td>\n",
              "      <td>0.939917</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>72 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   No. of Trees    LR Subsample %features Default Weight AUC Train AUC Test1  \\\n",
              "0            50  0.01       0.5       0.5              1  0.930578  0.918653   \n",
              "1            50  0.01       0.5       0.5              5  0.928086   0.91662   \n",
              "2            50  0.01       0.5       0.5             10  0.926039  0.914617   \n",
              "3            50  0.01       0.5         1              1  0.925662  0.912498   \n",
              "4            50  0.01       0.5         1              5   0.92363  0.911914   \n",
              "..          ...   ...       ...       ...            ...       ...       ...   \n",
              "67          500   0.1       0.8       0.5              5  0.961602  0.930949   \n",
              "68          500   0.1       0.8       0.5             10  0.960659  0.929978   \n",
              "69          500   0.1       0.8         1              1   0.96475   0.93116   \n",
              "70          500   0.1       0.8         1              5  0.963798  0.930387   \n",
              "71          500   0.1       0.8         1             10  0.962328  0.929791   \n",
              "\n",
              "   AUC Test2  \n",
              "0    0.93205  \n",
              "1   0.928584  \n",
              "2   0.925916  \n",
              "3   0.929065  \n",
              "4     0.9262  \n",
              "..       ...  \n",
              "67  0.941255  \n",
              "68  0.940514  \n",
              "69  0.942132  \n",
              "70  0.941157  \n",
              "71  0.939917  \n",
              "\n",
              "[72 rows x 8 columns]"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#grid search = P&C of each value in the document (3*2*2*2*3)\n",
        "t1 = t.time()\n",
        "grid_search = pd.DataFrame(columns = ['No. of Trees', 'LR', 'Subsample', '%features', 'Default Weight', 'AUC Train', 'AUC Test1', 'AUC Test2'])\n",
        "num_trees = [50, 100, 300]\n",
        "lr = [0.01, .1]\n",
        "subsample = [.5, .8]\n",
        "feat = [.5, 1]\n",
        "def_w = [1, 5, 10]\n",
        "\n",
        "row = 0\n",
        "for i in num_trees:\n",
        "    for j in lr:\n",
        "        for k in subsample:\n",
        "            for f in feat:\n",
        "                for d in def_w:\n",
        "                    xgb_inst = xgb.XGBClassifier(n_estimators = i, learning_rate = j, subsample = k, colsample_bytree = f, scale_pos_weight = d, random_state = 21)\n",
        "                    model = xgb_inst.fit(xtrain, ytrain)\n",
        "                    print(row)\n",
        "                    grid_search.loc[row, 'No. of Trees'] = i \n",
        "                    grid_search.loc[row, 'LR'] = j \n",
        "                    grid_search.loc[row, 'Subsample'] = k \n",
        "                    grid_search.loc[row, '%features'] = f \n",
        "                    grid_search.loc[row, 'Default Weight'] = d \n",
        "                    grid_search.loc[row,\"AUC Train\"] = roc_auc_score(ytrain, model.predict_proba(xtrain)[:,1])\n",
        "                    grid_search.loc[row,\"AUC Test1\"] = roc_auc_score(ytest1, model.predict_proba(xtest1)[:,1])\n",
        "                    grid_search.loc[row,\"AUC Test2\"] = roc_auc_score(ytest2, model.predict_proba(xtest2)[:,1])\n",
        "                    row += 1\n",
        "                    \n",
        "t2 = t.time()  \n",
        "print(t2-t1)\n",
        "grid_search\n",
        "                    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bias - error in training\n",
        "#variance - consistency "
      ],
      "metadata": {
        "id": "z4IkoZurc1BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNho8SVs6kE3"
      },
      "outputs": [],
      "source": [
        "grid_search.to_csv(\"grid_search.csv\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERdSDYYi6kE3",
        "outputId": "164d4d50-1e58-4d85-f366-db37ae8ec67a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_search.loc[0, '%features']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhX9y6Hp6kE4",
        "outputId": "bc97470f-c29c-4c22-9a90-7aded9efe80a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9305782740137187"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_search.iloc[0, -3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEzkRfeb6kE4",
        "outputId": "c583893f-e84f-4689-be1a-cbf9f6513b6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(72, 8)"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "grid_search.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09oQnlC96kE4"
      },
      "outputs": [],
      "source": [
        "xgb_inst = xgb.XGBClassifier(n_estimators = 500, learning_rate = 0.1, subsample = 0.8, colsample_bytree = 1, scale_pos_weight = 1, random_state = 21)\n",
        "final_xgb = xgb_inst.fit(xtrain, ytrain)\n",
        "final_xgb.save_model(\"final_xgb.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12xALGlO6kE4"
      },
      "source": [
        "### Neural Network "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtest1 = pd.read_csv('xtest1.csv', index_col = False)\n",
        "xtest2 = pd.read_csv('xtest2.csv', index_col = False)\n",
        "ytest1 = pd.read_csv('ytest1.csv', index_col = False)\n",
        "ytest2 = pd.read_csv('ytest2.csv', index_col = False)\n",
        "ytrain = pd.read_csv('ytrain.csv', index_col = False)\n",
        "xtrain = pd.read_csv('xtrain.csv', index_col = False)"
      ],
      "metadata": {
        "id": "DVA-rh8U6-SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgoZGTsT6kE4"
      },
      "source": [
        "### Outlier Treatment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nP8_mPi76kE5",
        "outputId": "27175192-d869-4afe-bc55-43c0fc32e69c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            count      mean       std           min        1%       50%  \\\n",
              "D_50     124737.0  0.173642  0.596443 -5.450931e+00  0.001119  0.106474   \n",
              "R_27     264585.0  0.889581  0.317309 -1.645560e-02  0.004221  1.004331   \n",
              "S_3      236579.0  0.229211  0.192048 -5.420518e-01  0.009204  0.166853   \n",
              "B_17     147759.0  0.681198  0.415114  1.197814e-06  0.000451  0.939358   \n",
              "B_39       2503.0  0.196954  0.288252 -1.815520e-02 -0.003727  0.090875   \n",
              "D_48     253023.0  0.366331  0.323967 -9.608557e-03  0.000314  0.262911   \n",
              "D_77     216167.0  0.218326  0.223926  1.474925e-06  0.002235  0.159015   \n",
              "D_75     269243.0  0.170062  0.222543  1.921960e-07  0.000262  0.074414   \n",
              "D_53      87144.0  0.073556  0.189549  1.134726e-07  0.000266  0.014077   \n",
              "D_132     36199.0  0.183624  0.265833 -1.454371e-02  0.001326  0.120108   \n",
              "B_1      269243.0  0.123169  0.209989 -1.024709e+00  0.000493  0.032070   \n",
              "B_3      269243.0  0.126750  0.229081  1.693698e-07  0.000250  0.009641   \n",
              "D_44     259328.0  0.115371  0.216164  1.027098e-07  0.000154  0.007649   \n",
              "B_2      269243.0  0.624860  0.398530  4.737755e-07  0.003198  0.814353   \n",
              "D_42      60938.0  0.180515  0.223688 -3.155777e-04  0.002543  0.117614   \n",
              "B_9      269243.0  0.189615  0.282701  3.926994e-08  0.000234  0.027939   \n",
              "D_49      36201.0  0.181701  0.381921  6.079387e-06  0.001951  0.118945   \n",
              "D_111      2490.0  0.869496  0.274212  3.187095e-05  0.001394  1.003666   \n",
              "D_45     269243.0  0.245879  0.243041  7.336260e-07  0.003214  0.171394   \n",
              "D_88       1474.0  0.181093  0.232830  3.955652e-04  0.002828  0.093810   \n",
              "B_5      269243.0  0.081871  0.393577  8.843823e-08  0.000404  0.014929   \n",
              "D_66      35652.0  0.890890  0.311782  0.000000e+00  0.000000  1.000000   \n",
              "B_38     269243.0  2.674239  1.574335  1.000000e+00  1.000000  2.000000   \n",
              "S_23     269243.0  0.176719  0.895719 -1.305714e+02  0.011962  0.136196   \n",
              "D_43     214617.0  0.152664  0.211613  1.154550e-07  0.002251  0.087089   \n",
              "D_63_CO  269243.0  0.748591  0.433824  0.000000e+00  0.000000  1.000000   \n",
              "D_41     269243.0  0.052705  0.181690  6.039876e-08  0.000112  0.005679   \n",
              "D_51     269243.0  0.141732  0.239573  5.615784e-08  0.000147  0.007216   \n",
              "P_2      268249.0  0.653899  0.240682 -3.842365e-01  0.028479  0.686045   \n",
              "B_7      269243.0  0.185880  0.230025 -2.702491e-01  0.002560  0.077522   \n",
              "R_26      31519.0  0.085856  0.243151  1.994525e-07  0.000214  0.037300   \n",
              "D_74     268647.0  0.152802  0.218103  9.117276e-08  0.000227  0.075749   \n",
              "D_62     252487.0  0.187195  0.229514 -2.373225e-03  0.003517  0.092012   \n",
              "B_4      269243.0  0.168284  0.217046  2.796505e-07  0.000722  0.080901   \n",
              "D_56     146288.0  0.196957  0.218137 -1.698373e-02  0.002402  0.142620   \n",
              "B_10     269243.0  0.239842  8.494480 -2.955690e-03  0.003761  0.109502   \n",
              "B_8      269122.0  0.464343  0.498265  1.179623e-07  0.000185  0.009245   \n",
              "D_46     231193.0  0.476199  0.167152 -6.931337e+00  0.028248  0.460192   \n",
              "D_76      33375.0  0.138690  0.279716  2.859264e-06  0.000936  0.057868   \n",
              "D_110      2490.0  0.739950  0.307269 -2.361460e-02  0.002106  0.875735   \n",
              "P_3      266694.0  0.599790  0.172868 -1.165741e+00  0.003759  0.617731   \n",
              "D_64_O   269243.0  0.536530  0.498665  0.000000e+00  0.000000  1.000000   \n",
              "S_7      236579.0  0.225074  0.204621 -3.972371e-01  0.008108  0.145958   \n",
              "R_1      269243.0  0.075871  0.220068  2.066058e-08  0.000115  0.005779   \n",
              "\n",
              "              99%          max  \n",
              "D_50     1.025614   103.129809  \n",
              "R_27     1.009884     1.010000  \n",
              "S_3      1.007797     4.882418  \n",
              "B_17     1.009725     1.010000  \n",
              "B_39     1.008819     1.057267  \n",
              "D_48     0.997020     8.964546  \n",
              "D_77     0.998699    10.009623  \n",
              "D_75     1.002986     4.270527  \n",
              "D_53     0.923819     6.171899  \n",
              "D_132    1.057431     5.934922  \n",
              "B_1      0.998345     1.324055  \n",
              "B_3      0.991403     1.465107  \n",
              "D_44     1.004204     5.634724  \n",
              "B_2      1.009681     1.010000  \n",
              "D_42     0.978335     4.190794  \n",
              "B_9      1.007247    13.974121  \n",
              "D_49     0.971915    45.840118  \n",
              "D_111    1.009892     1.009995  \n",
              "D_45     1.004107     1.594811  \n",
              "D_88     1.094812     2.669250  \n",
              "B_5      0.998441    68.707425  \n",
              "D_66     1.000000     1.000000  \n",
              "B_38     7.000000     7.000000  \n",
              "S_23     0.976839   423.436222  \n",
              "D_43     0.999963     8.560369  \n",
              "D_63_CO  1.000000     1.000000  \n",
              "D_41     0.879120     7.139698  \n",
              "D_51     1.006601     2.342054  \n",
              "P_2      1.005737     1.009999  \n",
              "B_7      1.011665     1.252750  \n",
              "R_26     1.038282     9.184288  \n",
              "D_74     1.002641     4.506363  \n",
              "D_62     0.987957    10.857220  \n",
              "B_4      0.980753     3.591951  \n",
              "D_56     0.987219    10.956514  \n",
              "B_10     1.028737  4097.440729  \n",
              "B_8      1.009562     1.012301  \n",
              "D_46     0.986337    16.319901  \n",
              "D_76     0.966506    15.131427  \n",
              "D_110    1.009672     1.009997  \n",
              "P_3      1.008099     2.100572  \n",
              "D_64_O   1.000000     1.000000  \n",
              "S_7      1.004786     3.516453  \n",
              "R_1      1.008782     2.756292  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00456b8f-afd0-4c68-839c-c8bb4f9a89bb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>1%</th>\n",
              "      <th>50%</th>\n",
              "      <th>99%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>D_50</th>\n",
              "      <td>124737.0</td>\n",
              "      <td>0.173642</td>\n",
              "      <td>0.596443</td>\n",
              "      <td>-5.450931e+00</td>\n",
              "      <td>0.001119</td>\n",
              "      <td>0.106474</td>\n",
              "      <td>1.025614</td>\n",
              "      <td>103.129809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>R_27</th>\n",
              "      <td>264585.0</td>\n",
              "      <td>0.889581</td>\n",
              "      <td>0.317309</td>\n",
              "      <td>-1.645560e-02</td>\n",
              "      <td>0.004221</td>\n",
              "      <td>1.004331</td>\n",
              "      <td>1.009884</td>\n",
              "      <td>1.010000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S_3</th>\n",
              "      <td>236579.0</td>\n",
              "      <td>0.229211</td>\n",
              "      <td>0.192048</td>\n",
              "      <td>-5.420518e-01</td>\n",
              "      <td>0.009204</td>\n",
              "      <td>0.166853</td>\n",
              "      <td>1.007797</td>\n",
              "      <td>4.882418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_17</th>\n",
              "      <td>147759.0</td>\n",
              "      <td>0.681198</td>\n",
              "      <td>0.415114</td>\n",
              "      <td>1.197814e-06</td>\n",
              "      <td>0.000451</td>\n",
              "      <td>0.939358</td>\n",
              "      <td>1.009725</td>\n",
              "      <td>1.010000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_39</th>\n",
              "      <td>2503.0</td>\n",
              "      <td>0.196954</td>\n",
              "      <td>0.288252</td>\n",
              "      <td>-1.815520e-02</td>\n",
              "      <td>-0.003727</td>\n",
              "      <td>0.090875</td>\n",
              "      <td>1.008819</td>\n",
              "      <td>1.057267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_48</th>\n",
              "      <td>253023.0</td>\n",
              "      <td>0.366331</td>\n",
              "      <td>0.323967</td>\n",
              "      <td>-9.608557e-03</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>0.262911</td>\n",
              "      <td>0.997020</td>\n",
              "      <td>8.964546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_77</th>\n",
              "      <td>216167.0</td>\n",
              "      <td>0.218326</td>\n",
              "      <td>0.223926</td>\n",
              "      <td>1.474925e-06</td>\n",
              "      <td>0.002235</td>\n",
              "      <td>0.159015</td>\n",
              "      <td>0.998699</td>\n",
              "      <td>10.009623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_75</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.170062</td>\n",
              "      <td>0.222543</td>\n",
              "      <td>1.921960e-07</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.074414</td>\n",
              "      <td>1.002986</td>\n",
              "      <td>4.270527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_53</th>\n",
              "      <td>87144.0</td>\n",
              "      <td>0.073556</td>\n",
              "      <td>0.189549</td>\n",
              "      <td>1.134726e-07</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>0.014077</td>\n",
              "      <td>0.923819</td>\n",
              "      <td>6.171899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_132</th>\n",
              "      <td>36199.0</td>\n",
              "      <td>0.183624</td>\n",
              "      <td>0.265833</td>\n",
              "      <td>-1.454371e-02</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.120108</td>\n",
              "      <td>1.057431</td>\n",
              "      <td>5.934922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_1</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.123169</td>\n",
              "      <td>0.209989</td>\n",
              "      <td>-1.024709e+00</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>0.032070</td>\n",
              "      <td>0.998345</td>\n",
              "      <td>1.324055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_3</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.126750</td>\n",
              "      <td>0.229081</td>\n",
              "      <td>1.693698e-07</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.009641</td>\n",
              "      <td>0.991403</td>\n",
              "      <td>1.465107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_44</th>\n",
              "      <td>259328.0</td>\n",
              "      <td>0.115371</td>\n",
              "      <td>0.216164</td>\n",
              "      <td>1.027098e-07</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>0.007649</td>\n",
              "      <td>1.004204</td>\n",
              "      <td>5.634724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_2</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.624860</td>\n",
              "      <td>0.398530</td>\n",
              "      <td>4.737755e-07</td>\n",
              "      <td>0.003198</td>\n",
              "      <td>0.814353</td>\n",
              "      <td>1.009681</td>\n",
              "      <td>1.010000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_42</th>\n",
              "      <td>60938.0</td>\n",
              "      <td>0.180515</td>\n",
              "      <td>0.223688</td>\n",
              "      <td>-3.155777e-04</td>\n",
              "      <td>0.002543</td>\n",
              "      <td>0.117614</td>\n",
              "      <td>0.978335</td>\n",
              "      <td>4.190794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_9</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.189615</td>\n",
              "      <td>0.282701</td>\n",
              "      <td>3.926994e-08</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.027939</td>\n",
              "      <td>1.007247</td>\n",
              "      <td>13.974121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_49</th>\n",
              "      <td>36201.0</td>\n",
              "      <td>0.181701</td>\n",
              "      <td>0.381921</td>\n",
              "      <td>6.079387e-06</td>\n",
              "      <td>0.001951</td>\n",
              "      <td>0.118945</td>\n",
              "      <td>0.971915</td>\n",
              "      <td>45.840118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_111</th>\n",
              "      <td>2490.0</td>\n",
              "      <td>0.869496</td>\n",
              "      <td>0.274212</td>\n",
              "      <td>3.187095e-05</td>\n",
              "      <td>0.001394</td>\n",
              "      <td>1.003666</td>\n",
              "      <td>1.009892</td>\n",
              "      <td>1.009995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_45</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.245879</td>\n",
              "      <td>0.243041</td>\n",
              "      <td>7.336260e-07</td>\n",
              "      <td>0.003214</td>\n",
              "      <td>0.171394</td>\n",
              "      <td>1.004107</td>\n",
              "      <td>1.594811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_88</th>\n",
              "      <td>1474.0</td>\n",
              "      <td>0.181093</td>\n",
              "      <td>0.232830</td>\n",
              "      <td>3.955652e-04</td>\n",
              "      <td>0.002828</td>\n",
              "      <td>0.093810</td>\n",
              "      <td>1.094812</td>\n",
              "      <td>2.669250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_5</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.081871</td>\n",
              "      <td>0.393577</td>\n",
              "      <td>8.843823e-08</td>\n",
              "      <td>0.000404</td>\n",
              "      <td>0.014929</td>\n",
              "      <td>0.998441</td>\n",
              "      <td>68.707425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_66</th>\n",
              "      <td>35652.0</td>\n",
              "      <td>0.890890</td>\n",
              "      <td>0.311782</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_38</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>2.674239</td>\n",
              "      <td>1.574335</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S_23</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.176719</td>\n",
              "      <td>0.895719</td>\n",
              "      <td>-1.305714e+02</td>\n",
              "      <td>0.011962</td>\n",
              "      <td>0.136196</td>\n",
              "      <td>0.976839</td>\n",
              "      <td>423.436222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_43</th>\n",
              "      <td>214617.0</td>\n",
              "      <td>0.152664</td>\n",
              "      <td>0.211613</td>\n",
              "      <td>1.154550e-07</td>\n",
              "      <td>0.002251</td>\n",
              "      <td>0.087089</td>\n",
              "      <td>0.999963</td>\n",
              "      <td>8.560369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_63_CO</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.748591</td>\n",
              "      <td>0.433824</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_41</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.052705</td>\n",
              "      <td>0.181690</td>\n",
              "      <td>6.039876e-08</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.005679</td>\n",
              "      <td>0.879120</td>\n",
              "      <td>7.139698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_51</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.141732</td>\n",
              "      <td>0.239573</td>\n",
              "      <td>5.615784e-08</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.007216</td>\n",
              "      <td>1.006601</td>\n",
              "      <td>2.342054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P_2</th>\n",
              "      <td>268249.0</td>\n",
              "      <td>0.653899</td>\n",
              "      <td>0.240682</td>\n",
              "      <td>-3.842365e-01</td>\n",
              "      <td>0.028479</td>\n",
              "      <td>0.686045</td>\n",
              "      <td>1.005737</td>\n",
              "      <td>1.009999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_7</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.185880</td>\n",
              "      <td>0.230025</td>\n",
              "      <td>-2.702491e-01</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.077522</td>\n",
              "      <td>1.011665</td>\n",
              "      <td>1.252750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>R_26</th>\n",
              "      <td>31519.0</td>\n",
              "      <td>0.085856</td>\n",
              "      <td>0.243151</td>\n",
              "      <td>1.994525e-07</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.037300</td>\n",
              "      <td>1.038282</td>\n",
              "      <td>9.184288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_74</th>\n",
              "      <td>268647.0</td>\n",
              "      <td>0.152802</td>\n",
              "      <td>0.218103</td>\n",
              "      <td>9.117276e-08</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>0.075749</td>\n",
              "      <td>1.002641</td>\n",
              "      <td>4.506363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_62</th>\n",
              "      <td>252487.0</td>\n",
              "      <td>0.187195</td>\n",
              "      <td>0.229514</td>\n",
              "      <td>-2.373225e-03</td>\n",
              "      <td>0.003517</td>\n",
              "      <td>0.092012</td>\n",
              "      <td>0.987957</td>\n",
              "      <td>10.857220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_4</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.168284</td>\n",
              "      <td>0.217046</td>\n",
              "      <td>2.796505e-07</td>\n",
              "      <td>0.000722</td>\n",
              "      <td>0.080901</td>\n",
              "      <td>0.980753</td>\n",
              "      <td>3.591951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_56</th>\n",
              "      <td>146288.0</td>\n",
              "      <td>0.196957</td>\n",
              "      <td>0.218137</td>\n",
              "      <td>-1.698373e-02</td>\n",
              "      <td>0.002402</td>\n",
              "      <td>0.142620</td>\n",
              "      <td>0.987219</td>\n",
              "      <td>10.956514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_10</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.239842</td>\n",
              "      <td>8.494480</td>\n",
              "      <td>-2.955690e-03</td>\n",
              "      <td>0.003761</td>\n",
              "      <td>0.109502</td>\n",
              "      <td>1.028737</td>\n",
              "      <td>4097.440729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_8</th>\n",
              "      <td>269122.0</td>\n",
              "      <td>0.464343</td>\n",
              "      <td>0.498265</td>\n",
              "      <td>1.179623e-07</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.009245</td>\n",
              "      <td>1.009562</td>\n",
              "      <td>1.012301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_46</th>\n",
              "      <td>231193.0</td>\n",
              "      <td>0.476199</td>\n",
              "      <td>0.167152</td>\n",
              "      <td>-6.931337e+00</td>\n",
              "      <td>0.028248</td>\n",
              "      <td>0.460192</td>\n",
              "      <td>0.986337</td>\n",
              "      <td>16.319901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_76</th>\n",
              "      <td>33375.0</td>\n",
              "      <td>0.138690</td>\n",
              "      <td>0.279716</td>\n",
              "      <td>2.859264e-06</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.057868</td>\n",
              "      <td>0.966506</td>\n",
              "      <td>15.131427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_110</th>\n",
              "      <td>2490.0</td>\n",
              "      <td>0.739950</td>\n",
              "      <td>0.307269</td>\n",
              "      <td>-2.361460e-02</td>\n",
              "      <td>0.002106</td>\n",
              "      <td>0.875735</td>\n",
              "      <td>1.009672</td>\n",
              "      <td>1.009997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P_3</th>\n",
              "      <td>266694.0</td>\n",
              "      <td>0.599790</td>\n",
              "      <td>0.172868</td>\n",
              "      <td>-1.165741e+00</td>\n",
              "      <td>0.003759</td>\n",
              "      <td>0.617731</td>\n",
              "      <td>1.008099</td>\n",
              "      <td>2.100572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_64_O</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.536530</td>\n",
              "      <td>0.498665</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S_7</th>\n",
              "      <td>236579.0</td>\n",
              "      <td>0.225074</td>\n",
              "      <td>0.204621</td>\n",
              "      <td>-3.972371e-01</td>\n",
              "      <td>0.008108</td>\n",
              "      <td>0.145958</td>\n",
              "      <td>1.004786</td>\n",
              "      <td>3.516453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>R_1</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.075871</td>\n",
              "      <td>0.220068</td>\n",
              "      <td>2.066058e-08</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.005779</td>\n",
              "      <td>1.008782</td>\n",
              "      <td>2.756292</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00456b8f-afd0-4c68-839c-c8bb4f9a89bb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-00456b8f-afd0-4c68-839c-c8bb4f9a89bb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-00456b8f-afd0-4c68-839c-c8bb4f9a89bb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#remove values less than 1% and greater than 99%.\n",
        "#checking the 1% and 99% of each\n",
        "xtrain.describe(percentiles = [.01, .99]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgkifXh96kE5",
        "outputId": "701caa4e-8409-4bce-b38e-6367978ee4c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "D_50       1.025614\n",
              "R_27       1.009884\n",
              "S_3        1.007797\n",
              "B_17       1.009725\n",
              "B_39       1.008819\n",
              "D_48       0.997020\n",
              "D_77       0.998699\n",
              "D_75       1.002986\n",
              "D_53       0.923819\n",
              "D_132      1.057431\n",
              "B_1        0.998345\n",
              "B_3        0.991403\n",
              "D_44       1.004204\n",
              "B_2        1.009681\n",
              "D_42       0.978335\n",
              "B_9        1.007247\n",
              "D_49       0.971915\n",
              "D_111      1.009892\n",
              "D_45       1.004107\n",
              "D_88       1.094812\n",
              "B_5        0.998441\n",
              "D_66       1.000000\n",
              "B_38       7.000000\n",
              "S_23       0.976839\n",
              "D_43       0.999963\n",
              "D_63_CO    1.000000\n",
              "D_41       0.879120\n",
              "D_51       1.006601\n",
              "P_2        1.005737\n",
              "B_7        1.011665\n",
              "R_26       1.038282\n",
              "D_74       1.002641\n",
              "D_62       0.987957\n",
              "B_4        0.980753\n",
              "D_56       0.987219\n",
              "B_10       1.028737\n",
              "B_8        1.009562\n",
              "D_46       0.986337\n",
              "D_76       0.966506\n",
              "D_110      1.009672\n",
              "P_3        1.008099\n",
              "D_64_O     1.000000\n",
              "S_7        1.004786\n",
              "R_1        1.008782\n",
              "Name: 0.99, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "xtrain.quantile(0.99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3Vu7rKX6kE5"
      },
      "outputs": [],
      "source": [
        "for i in xtrain.columns:\n",
        "  #column value > 99%, cap it else keep the same value.\n",
        "  #column value < 1%, cap it else keep the same value.\n",
        "    xtrain[i] = np.where(xtrain[i] > xtrain[i].quantile(0.99), xtrain[i].quantile(0.99), xtrain[i] )\n",
        "    xtrain[i] = np.where(xtrain[i] < xtrain[i].quantile(0.01), xtrain[i].quantile(0.01), xtrain[i] )\n",
        "\n",
        "for i in xtest1.columns:\n",
        "    xtest1[i] = np.where(xtest1[i] > xtest1[i].quantile(0.99), xtest1[i].quantile(0.99), xtest1[i] )\n",
        "    xtest1[i] = np.where(xtest1[i] < xtest1[i].quantile(0.01), xtest1[i].quantile(0.01), xtest1[i] )\n",
        "\n",
        "for i in xtest2.columns:\n",
        "    xtest2[i] = np.where(xtest2[i] > xtest2[i].quantile(0.99), xtest2[i].quantile(0.99), xtest2[i] )\n",
        "    xtest2[i] = np.where(xtest2[i] < xtest2[i].quantile(0.01), xtest2[i].quantile(0.01), xtest2[i] )\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y4gGE6Jj6kE5",
        "outputId": "1ea765fa-6e61-4488-c758-0b41bce259cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            count      mean       std       min        1%       50%       99%  \\\n",
              "D_50     124737.0  0.153605  0.158885  0.001119  0.001120  0.106474  1.025548   \n",
              "R_27     264585.0  0.889623  0.317189  0.004221  0.004221  1.004331  1.009884   \n",
              "S_3      236579.0  0.226974  0.176180  0.009204  0.009209  0.166853  1.007790   \n",
              "B_17     147759.0  0.681199  0.415109  0.000451  0.000452  0.939358  1.009725   \n",
              "B_39       2503.0  0.196938  0.288015 -0.003727 -0.003708  0.090875  1.008818   \n",
              "D_48     253023.0  0.365333  0.320872  0.000314  0.000314  0.262911  0.997016   \n",
              "D_77     216167.0  0.215917  0.206674  0.002235  0.002235  0.159015  0.998698   \n",
              "D_75     269243.0  0.167581  0.209838  0.000262  0.000262  0.074414  1.002986   \n",
              "D_53      87144.0  0.068415  0.146353  0.000266  0.000266  0.014077  0.923781   \n",
              "D_132     36199.0  0.173358  0.178509  0.001326  0.001326  0.120108  1.057199   \n",
              "B_1      269243.0  0.121533  0.201810  0.000493  0.000493  0.032070  0.998309   \n",
              "B_3      269243.0  0.125715  0.224736  0.000250  0.000250  0.009641  0.991396   \n",
              "D_44     259328.0  0.112697  0.200733  0.000154  0.000154  0.007649  1.004202   \n",
              "B_2      269243.0  0.624874  0.398504  0.003198  0.003198  0.814353  1.009681   \n",
              "D_42      60938.0  0.175491  0.189123  0.002543  0.002543  0.117614  0.978218   \n",
              "B_9      269243.0  0.184991  0.252886  0.000234  0.000234  0.027939  1.007231   \n",
              "D_49      36201.0  0.174271  0.176588  0.001951  0.001951  0.118945  0.971915   \n",
              "D_111      2490.0  0.869503  0.274187  0.001394  0.001401  1.003666  1.009892   \n",
              "D_45     269243.0  0.244513  0.238003  0.003214  0.003214  0.171394  1.004087   \n",
              "D_88       1474.0  0.178168  0.215176  0.002828  0.002852  0.093810  1.093215   \n",
              "B_5      269243.0  0.069529  0.152094  0.000404  0.000404  0.014929  0.998406   \n",
              "D_66      35652.0  0.890890  0.311782  0.000000  0.000000  1.000000  1.000000   \n",
              "B_38     269243.0  2.674239  1.574335  1.000000  1.000000  2.000000  7.000000   \n",
              "S_23     269243.0  0.171605  0.144725  0.011962  0.011978  0.136196  0.976825   \n",
              "D_43     214617.0  0.148017  0.176411  0.002251  0.002251  0.087089  0.999952   \n",
              "D_63_CO  269243.0  0.748591  0.433824  0.000000  0.000000  1.000000  1.000000   \n",
              "D_41     269243.0  0.048130  0.142972  0.000112  0.000112  0.005679  0.879111   \n",
              "D_51     269243.0  0.139788  0.230461  0.000147  0.000147  0.007216  1.006600   \n",
              "P_2      268249.0  0.654755  0.238098  0.028479  0.028494  0.686045  1.005737   \n",
              "B_7      269243.0  0.184464  0.224130  0.002560  0.002560  0.077522  1.011636   \n",
              "R_26      31519.0  0.077095  0.152884  0.000214  0.000214  0.037300  1.038223   \n",
              "D_74     268647.0  0.150198  0.204020  0.000227  0.000227  0.075749  1.002636   \n",
              "D_62     252487.0  0.184298  0.209689  0.003517  0.003517  0.092012  0.987918   \n",
              "B_4      269243.0  0.165670  0.203391  0.000722  0.000722  0.080901  0.980725   \n",
              "D_56     146288.0  0.191911  0.171834  0.002402  0.002403  0.142620  0.987213   \n",
              "B_10     269243.0  0.164748  0.163361  0.003761  0.003761  0.109502  1.028682   \n",
              "B_8      269122.0  0.464342  0.498263  0.000185  0.000185  0.009245  1.009562   \n",
              "D_46     231193.0  0.475961  0.130316  0.028248  0.028254  0.460192  0.986328   \n",
              "D_76      33375.0  0.129400  0.183836  0.000936  0.000936  0.057868  0.966464   \n",
              "D_110      2490.0  0.740133  0.306819  0.002106  0.002183  0.875735  1.009672   \n",
              "P_3      266694.0  0.600383  0.160077  0.003759  0.003764  0.617731  1.008096   \n",
              "D_64_O   269243.0  0.536530  0.498665  0.000000  0.000000  1.000000  1.000000   \n",
              "S_7      236579.0  0.223133  0.192829  0.008108  0.008114  0.145958  1.004784   \n",
              "R_1      269243.0  0.071964  0.196982  0.000115  0.000115  0.005779  1.008781   \n",
              "\n",
              "              max  \n",
              "D_50     1.025614  \n",
              "R_27     1.009884  \n",
              "S_3      1.007797  \n",
              "B_17     1.009725  \n",
              "B_39     1.008819  \n",
              "D_48     0.997020  \n",
              "D_77     0.998699  \n",
              "D_75     1.002986  \n",
              "D_53     0.923819  \n",
              "D_132    1.057431  \n",
              "B_1      0.998345  \n",
              "B_3      0.991403  \n",
              "D_44     1.004204  \n",
              "B_2      1.009681  \n",
              "D_42     0.978335  \n",
              "B_9      1.007247  \n",
              "D_49     0.971915  \n",
              "D_111    1.009892  \n",
              "D_45     1.004107  \n",
              "D_88     1.094812  \n",
              "B_5      0.998441  \n",
              "D_66     1.000000  \n",
              "B_38     7.000000  \n",
              "S_23     0.976839  \n",
              "D_43     0.999963  \n",
              "D_63_CO  1.000000  \n",
              "D_41     0.879120  \n",
              "D_51     1.006601  \n",
              "P_2      1.005737  \n",
              "B_7      1.011665  \n",
              "R_26     1.038282  \n",
              "D_74     1.002641  \n",
              "D_62     0.987957  \n",
              "B_4      0.980753  \n",
              "D_56     0.987219  \n",
              "B_10     1.028737  \n",
              "B_8      1.009562  \n",
              "D_46     0.986337  \n",
              "D_76     0.966506  \n",
              "D_110    1.009672  \n",
              "P_3      1.008099  \n",
              "D_64_O   1.000000  \n",
              "S_7      1.004786  \n",
              "R_1      1.008782  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c79f5985-cceb-4931-9d5f-c9f0f2b0109b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>1%</th>\n",
              "      <th>50%</th>\n",
              "      <th>99%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>D_50</th>\n",
              "      <td>124737.0</td>\n",
              "      <td>0.153605</td>\n",
              "      <td>0.158885</td>\n",
              "      <td>0.001119</td>\n",
              "      <td>0.001120</td>\n",
              "      <td>0.106474</td>\n",
              "      <td>1.025548</td>\n",
              "      <td>1.025614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>R_27</th>\n",
              "      <td>264585.0</td>\n",
              "      <td>0.889623</td>\n",
              "      <td>0.317189</td>\n",
              "      <td>0.004221</td>\n",
              "      <td>0.004221</td>\n",
              "      <td>1.004331</td>\n",
              "      <td>1.009884</td>\n",
              "      <td>1.009884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S_3</th>\n",
              "      <td>236579.0</td>\n",
              "      <td>0.226974</td>\n",
              "      <td>0.176180</td>\n",
              "      <td>0.009204</td>\n",
              "      <td>0.009209</td>\n",
              "      <td>0.166853</td>\n",
              "      <td>1.007790</td>\n",
              "      <td>1.007797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_17</th>\n",
              "      <td>147759.0</td>\n",
              "      <td>0.681199</td>\n",
              "      <td>0.415109</td>\n",
              "      <td>0.000451</td>\n",
              "      <td>0.000452</td>\n",
              "      <td>0.939358</td>\n",
              "      <td>1.009725</td>\n",
              "      <td>1.009725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_39</th>\n",
              "      <td>2503.0</td>\n",
              "      <td>0.196938</td>\n",
              "      <td>0.288015</td>\n",
              "      <td>-0.003727</td>\n",
              "      <td>-0.003708</td>\n",
              "      <td>0.090875</td>\n",
              "      <td>1.008818</td>\n",
              "      <td>1.008819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_48</th>\n",
              "      <td>253023.0</td>\n",
              "      <td>0.365333</td>\n",
              "      <td>0.320872</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>0.262911</td>\n",
              "      <td>0.997016</td>\n",
              "      <td>0.997020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_77</th>\n",
              "      <td>216167.0</td>\n",
              "      <td>0.215917</td>\n",
              "      <td>0.206674</td>\n",
              "      <td>0.002235</td>\n",
              "      <td>0.002235</td>\n",
              "      <td>0.159015</td>\n",
              "      <td>0.998698</td>\n",
              "      <td>0.998699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_75</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.167581</td>\n",
              "      <td>0.209838</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.074414</td>\n",
              "      <td>1.002986</td>\n",
              "      <td>1.002986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_53</th>\n",
              "      <td>87144.0</td>\n",
              "      <td>0.068415</td>\n",
              "      <td>0.146353</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>0.014077</td>\n",
              "      <td>0.923781</td>\n",
              "      <td>0.923819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_132</th>\n",
              "      <td>36199.0</td>\n",
              "      <td>0.173358</td>\n",
              "      <td>0.178509</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.120108</td>\n",
              "      <td>1.057199</td>\n",
              "      <td>1.057431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_1</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.121533</td>\n",
              "      <td>0.201810</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>0.032070</td>\n",
              "      <td>0.998309</td>\n",
              "      <td>0.998345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_3</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.125715</td>\n",
              "      <td>0.224736</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.009641</td>\n",
              "      <td>0.991396</td>\n",
              "      <td>0.991403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_44</th>\n",
              "      <td>259328.0</td>\n",
              "      <td>0.112697</td>\n",
              "      <td>0.200733</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>0.007649</td>\n",
              "      <td>1.004202</td>\n",
              "      <td>1.004204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_2</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.624874</td>\n",
              "      <td>0.398504</td>\n",
              "      <td>0.003198</td>\n",
              "      <td>0.003198</td>\n",
              "      <td>0.814353</td>\n",
              "      <td>1.009681</td>\n",
              "      <td>1.009681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_42</th>\n",
              "      <td>60938.0</td>\n",
              "      <td>0.175491</td>\n",
              "      <td>0.189123</td>\n",
              "      <td>0.002543</td>\n",
              "      <td>0.002543</td>\n",
              "      <td>0.117614</td>\n",
              "      <td>0.978218</td>\n",
              "      <td>0.978335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_9</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.184991</td>\n",
              "      <td>0.252886</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.027939</td>\n",
              "      <td>1.007231</td>\n",
              "      <td>1.007247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_49</th>\n",
              "      <td>36201.0</td>\n",
              "      <td>0.174271</td>\n",
              "      <td>0.176588</td>\n",
              "      <td>0.001951</td>\n",
              "      <td>0.001951</td>\n",
              "      <td>0.118945</td>\n",
              "      <td>0.971915</td>\n",
              "      <td>0.971915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_111</th>\n",
              "      <td>2490.0</td>\n",
              "      <td>0.869503</td>\n",
              "      <td>0.274187</td>\n",
              "      <td>0.001394</td>\n",
              "      <td>0.001401</td>\n",
              "      <td>1.003666</td>\n",
              "      <td>1.009892</td>\n",
              "      <td>1.009892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_45</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.244513</td>\n",
              "      <td>0.238003</td>\n",
              "      <td>0.003214</td>\n",
              "      <td>0.003214</td>\n",
              "      <td>0.171394</td>\n",
              "      <td>1.004087</td>\n",
              "      <td>1.004107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_88</th>\n",
              "      <td>1474.0</td>\n",
              "      <td>0.178168</td>\n",
              "      <td>0.215176</td>\n",
              "      <td>0.002828</td>\n",
              "      <td>0.002852</td>\n",
              "      <td>0.093810</td>\n",
              "      <td>1.093215</td>\n",
              "      <td>1.094812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_5</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.069529</td>\n",
              "      <td>0.152094</td>\n",
              "      <td>0.000404</td>\n",
              "      <td>0.000404</td>\n",
              "      <td>0.014929</td>\n",
              "      <td>0.998406</td>\n",
              "      <td>0.998441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_66</th>\n",
              "      <td>35652.0</td>\n",
              "      <td>0.890890</td>\n",
              "      <td>0.311782</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_38</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>2.674239</td>\n",
              "      <td>1.574335</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S_23</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.171605</td>\n",
              "      <td>0.144725</td>\n",
              "      <td>0.011962</td>\n",
              "      <td>0.011978</td>\n",
              "      <td>0.136196</td>\n",
              "      <td>0.976825</td>\n",
              "      <td>0.976839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_43</th>\n",
              "      <td>214617.0</td>\n",
              "      <td>0.148017</td>\n",
              "      <td>0.176411</td>\n",
              "      <td>0.002251</td>\n",
              "      <td>0.002251</td>\n",
              "      <td>0.087089</td>\n",
              "      <td>0.999952</td>\n",
              "      <td>0.999963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_63_CO</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.748591</td>\n",
              "      <td>0.433824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_41</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.048130</td>\n",
              "      <td>0.142972</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.005679</td>\n",
              "      <td>0.879111</td>\n",
              "      <td>0.879120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_51</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.139788</td>\n",
              "      <td>0.230461</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.007216</td>\n",
              "      <td>1.006600</td>\n",
              "      <td>1.006601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P_2</th>\n",
              "      <td>268249.0</td>\n",
              "      <td>0.654755</td>\n",
              "      <td>0.238098</td>\n",
              "      <td>0.028479</td>\n",
              "      <td>0.028494</td>\n",
              "      <td>0.686045</td>\n",
              "      <td>1.005737</td>\n",
              "      <td>1.005737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_7</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.184464</td>\n",
              "      <td>0.224130</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.077522</td>\n",
              "      <td>1.011636</td>\n",
              "      <td>1.011665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>R_26</th>\n",
              "      <td>31519.0</td>\n",
              "      <td>0.077095</td>\n",
              "      <td>0.152884</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.037300</td>\n",
              "      <td>1.038223</td>\n",
              "      <td>1.038282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_74</th>\n",
              "      <td>268647.0</td>\n",
              "      <td>0.150198</td>\n",
              "      <td>0.204020</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>0.075749</td>\n",
              "      <td>1.002636</td>\n",
              "      <td>1.002641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_62</th>\n",
              "      <td>252487.0</td>\n",
              "      <td>0.184298</td>\n",
              "      <td>0.209689</td>\n",
              "      <td>0.003517</td>\n",
              "      <td>0.003517</td>\n",
              "      <td>0.092012</td>\n",
              "      <td>0.987918</td>\n",
              "      <td>0.987957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_4</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.165670</td>\n",
              "      <td>0.203391</td>\n",
              "      <td>0.000722</td>\n",
              "      <td>0.000722</td>\n",
              "      <td>0.080901</td>\n",
              "      <td>0.980725</td>\n",
              "      <td>0.980753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_56</th>\n",
              "      <td>146288.0</td>\n",
              "      <td>0.191911</td>\n",
              "      <td>0.171834</td>\n",
              "      <td>0.002402</td>\n",
              "      <td>0.002403</td>\n",
              "      <td>0.142620</td>\n",
              "      <td>0.987213</td>\n",
              "      <td>0.987219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_10</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.164748</td>\n",
              "      <td>0.163361</td>\n",
              "      <td>0.003761</td>\n",
              "      <td>0.003761</td>\n",
              "      <td>0.109502</td>\n",
              "      <td>1.028682</td>\n",
              "      <td>1.028737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B_8</th>\n",
              "      <td>269122.0</td>\n",
              "      <td>0.464342</td>\n",
              "      <td>0.498263</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.009245</td>\n",
              "      <td>1.009562</td>\n",
              "      <td>1.009562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_46</th>\n",
              "      <td>231193.0</td>\n",
              "      <td>0.475961</td>\n",
              "      <td>0.130316</td>\n",
              "      <td>0.028248</td>\n",
              "      <td>0.028254</td>\n",
              "      <td>0.460192</td>\n",
              "      <td>0.986328</td>\n",
              "      <td>0.986337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_76</th>\n",
              "      <td>33375.0</td>\n",
              "      <td>0.129400</td>\n",
              "      <td>0.183836</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.057868</td>\n",
              "      <td>0.966464</td>\n",
              "      <td>0.966506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_110</th>\n",
              "      <td>2490.0</td>\n",
              "      <td>0.740133</td>\n",
              "      <td>0.306819</td>\n",
              "      <td>0.002106</td>\n",
              "      <td>0.002183</td>\n",
              "      <td>0.875735</td>\n",
              "      <td>1.009672</td>\n",
              "      <td>1.009672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P_3</th>\n",
              "      <td>266694.0</td>\n",
              "      <td>0.600383</td>\n",
              "      <td>0.160077</td>\n",
              "      <td>0.003759</td>\n",
              "      <td>0.003764</td>\n",
              "      <td>0.617731</td>\n",
              "      <td>1.008096</td>\n",
              "      <td>1.008099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D_64_O</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.536530</td>\n",
              "      <td>0.498665</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S_7</th>\n",
              "      <td>236579.0</td>\n",
              "      <td>0.223133</td>\n",
              "      <td>0.192829</td>\n",
              "      <td>0.008108</td>\n",
              "      <td>0.008114</td>\n",
              "      <td>0.145958</td>\n",
              "      <td>1.004784</td>\n",
              "      <td>1.004786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>R_1</th>\n",
              "      <td>269243.0</td>\n",
              "      <td>0.071964</td>\n",
              "      <td>0.196982</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.005779</td>\n",
              "      <td>1.008781</td>\n",
              "      <td>1.008782</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c79f5985-cceb-4931-9d5f-c9f0f2b0109b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c79f5985-cceb-4931-9d5f-c9f0f2b0109b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c79f5985-cceb-4931-9d5f-c9f0f2b0109b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "xtrain.describe(percentiles = [.01, .99]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoUs63bx6kE4"
      },
      "source": [
        "### Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWnl-WnT6kE4"
      },
      "outputs": [],
      "source": [
        "#normalise according to xtrain(take that as a base) \n",
        "#varying from 0 to 100. If to be kept between 0 and 1 then it will adjust accordingly then 99 = 0.99\n",
        "#now apply the same to xtest2 and xtest2 if it is between 1 and 200. As we have to fix the parameters i.e. sd or mean\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "sc = StandardScaler()\n",
        "sc.fit(xtrain)\n",
        "\n",
        "xtrain_n = sc.transform(xtrain)\n",
        "xtest1_n  = sc.transform(xtest1)\n",
        "xtest2_n  = sc.transform(xtest2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57slY3NQ6kE4"
      },
      "outputs": [],
      "source": [
        "# convert to Pandas DF\n",
        "xtrain_n_df = pd.DataFrame(xtrain_n, columns=xtrain.columns)\n",
        "xtest1_n_df = pd.DataFrame(xtest1_n, columns=xtest1.columns)\n",
        "xtest2_n_df = pd.DataFrame(xtest2_n, columns=xtest2.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdP7N84J6kE5"
      },
      "source": [
        "### Missing Value Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5mEfExE6kE5"
      },
      "outputs": [],
      "source": [
        "#replace NaNs with 0\n",
        "xtrain_n_df.fillna(0,inplace=True)\n",
        "xtest1_n_df.fillna(0,inplace=True)\n",
        "xtest2_n_df.fillna(0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "k5bLdkE46kE6",
        "outputId": "4b6f2b28-bdc1-4689-8955-d367310718bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       D_50      R_27       S_3      B_17  B_39      D_48      D_77      D_75  \\\n",
              "0  0.000000  0.377631 -0.383062  0.000000   0.0 -1.107039  0.040327 -0.755381   \n",
              "1  1.843198  0.372141  0.502247  0.017745   0.0 -1.097875  0.911857 -0.784362   \n",
              "2 -0.437529 -2.728504 -0.272799  0.000000   0.0  1.842888 -0.950551  1.453366   \n",
              "3  0.000000  0.000000 -0.367267  0.000000   0.0  0.000000 -1.017417 -0.761060   \n",
              "4 -0.483413  0.372753 -0.687531 -1.479957   0.0  0.611751  0.890597 -0.470751   \n",
              "\n",
              "       D_53     D_132       B_1       B_3      D_44       B_2      D_42  \\\n",
              "0  0.000000  0.000000 -0.430972 -0.520536 -0.560638  0.954936  0.000000   \n",
              "1 -0.423522  0.000000 -0.529924 -0.538016 -0.560638  0.965565  0.000000   \n",
              "2  0.787194  0.000000 -0.322426 -0.531865  3.821934  0.467214  0.000000   \n",
              "3  0.664321  0.880645 -0.117347 -0.515198  0.000000  0.960354 -0.639383   \n",
              "4  0.000000  0.000000  0.207770  1.176883 -0.530640 -0.081702  0.000000   \n",
              "\n",
              "        B_9      D_49  D_111      D_45  D_88       B_5      D_66      B_38  \\\n",
              "0 -0.685118  0.000000    0.0  0.080766   0.0 -0.390372  0.000000 -0.428269   \n",
              "1 -0.595661  0.000000    0.0 -0.712508   0.0  0.305248  0.000000 -0.428269   \n",
              "2  0.549902  0.000000    0.0 -0.710620   0.0 -0.392342  0.000000 -1.063459   \n",
              "3 -0.171118 -0.959478    0.0 -0.971711   0.0 -0.425938  0.000000 -1.063459   \n",
              "4 -0.617596  0.000000    0.0 -0.480064   0.0 -0.426827  0.349962  0.206920   \n",
              "\n",
              "       S_23      D_43   D_63_CO      D_41      D_51       P_2       B_7  \\\n",
              "0 -0.216696 -0.487437  0.579519 -0.292756  0.868774  0.941215 -0.674991   \n",
              "1 -0.225507 -0.532062  0.579519 -0.279013 -0.571794 -0.131726 -0.767857   \n",
              "2 -0.264870 -0.282129  0.579519 -0.295348  0.862312 -1.058486  3.441066   \n",
              "3 -0.241175  0.000000  0.579519 -0.281609 -0.598149 -1.680961 -0.408557   \n",
              "4 -0.249114  0.073433  0.579519 -0.294765  0.851496  1.008137 -0.649541   \n",
              "\n",
              "       R_26      D_74      D_62       B_4      D_56      B_10       B_8  \\\n",
              "0  0.000000 -0.730125  0.258596 -0.711899  2.931983  0.837827 -0.922301   \n",
              "1  0.000000 -0.713646  1.196163 -0.772814  2.008569  1.451580  1.086175   \n",
              "2 -0.018105  1.730283 -0.812073  2.190999 -0.314562 -0.947911  1.077944   \n",
              "3  0.000000 -0.704164  0.000000 -0.748924  0.000000  0.841608  1.080546   \n",
              "4  0.000000 -0.732697  1.151915 -0.479474  3.811727  0.833941  1.075952   \n",
              "\n",
              "       D_46  D_76  D_110       P_3    D_64_O       S_7       R_1  \n",
              "0 -0.044030   0.0    0.0 -0.515629  0.929423 -0.450937 -0.328536  \n",
              "1 -0.380396   0.0    0.0  0.516465  0.929423  0.427514 -0.346202  \n",
              "2  2.354170   0.0    0.0 -0.806894  0.929423  0.076566  2.184979  \n",
              "3  0.000000   0.0    0.0  0.000000 -1.075936  0.003371  4.731583  \n",
              "4 -1.264847   0.0    0.0  0.819710  0.929423 -0.698450 -0.318218  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c41c3cc7-3211-4b21-b833-9c6c19006549\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D_50</th>\n",
              "      <th>R_27</th>\n",
              "      <th>S_3</th>\n",
              "      <th>B_17</th>\n",
              "      <th>B_39</th>\n",
              "      <th>D_48</th>\n",
              "      <th>D_77</th>\n",
              "      <th>D_75</th>\n",
              "      <th>D_53</th>\n",
              "      <th>D_132</th>\n",
              "      <th>B_1</th>\n",
              "      <th>B_3</th>\n",
              "      <th>D_44</th>\n",
              "      <th>B_2</th>\n",
              "      <th>D_42</th>\n",
              "      <th>B_9</th>\n",
              "      <th>D_49</th>\n",
              "      <th>D_111</th>\n",
              "      <th>D_45</th>\n",
              "      <th>D_88</th>\n",
              "      <th>B_5</th>\n",
              "      <th>D_66</th>\n",
              "      <th>B_38</th>\n",
              "      <th>S_23</th>\n",
              "      <th>D_43</th>\n",
              "      <th>D_63_CO</th>\n",
              "      <th>D_41</th>\n",
              "      <th>D_51</th>\n",
              "      <th>P_2</th>\n",
              "      <th>B_7</th>\n",
              "      <th>R_26</th>\n",
              "      <th>D_74</th>\n",
              "      <th>D_62</th>\n",
              "      <th>B_4</th>\n",
              "      <th>D_56</th>\n",
              "      <th>B_10</th>\n",
              "      <th>B_8</th>\n",
              "      <th>D_46</th>\n",
              "      <th>D_76</th>\n",
              "      <th>D_110</th>\n",
              "      <th>P_3</th>\n",
              "      <th>D_64_O</th>\n",
              "      <th>S_7</th>\n",
              "      <th>R_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.377631</td>\n",
              "      <td>-0.383062</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.107039</td>\n",
              "      <td>0.040327</td>\n",
              "      <td>-0.755381</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.430972</td>\n",
              "      <td>-0.520536</td>\n",
              "      <td>-0.560638</td>\n",
              "      <td>0.954936</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.685118</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.080766</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.390372</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.428269</td>\n",
              "      <td>-0.216696</td>\n",
              "      <td>-0.487437</td>\n",
              "      <td>0.579519</td>\n",
              "      <td>-0.292756</td>\n",
              "      <td>0.868774</td>\n",
              "      <td>0.941215</td>\n",
              "      <td>-0.674991</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.730125</td>\n",
              "      <td>0.258596</td>\n",
              "      <td>-0.711899</td>\n",
              "      <td>2.931983</td>\n",
              "      <td>0.837827</td>\n",
              "      <td>-0.922301</td>\n",
              "      <td>-0.044030</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.515629</td>\n",
              "      <td>0.929423</td>\n",
              "      <td>-0.450937</td>\n",
              "      <td>-0.328536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.843198</td>\n",
              "      <td>0.372141</td>\n",
              "      <td>0.502247</td>\n",
              "      <td>0.017745</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.097875</td>\n",
              "      <td>0.911857</td>\n",
              "      <td>-0.784362</td>\n",
              "      <td>-0.423522</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.529924</td>\n",
              "      <td>-0.538016</td>\n",
              "      <td>-0.560638</td>\n",
              "      <td>0.965565</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.595661</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.712508</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.305248</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.428269</td>\n",
              "      <td>-0.225507</td>\n",
              "      <td>-0.532062</td>\n",
              "      <td>0.579519</td>\n",
              "      <td>-0.279013</td>\n",
              "      <td>-0.571794</td>\n",
              "      <td>-0.131726</td>\n",
              "      <td>-0.767857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.713646</td>\n",
              "      <td>1.196163</td>\n",
              "      <td>-0.772814</td>\n",
              "      <td>2.008569</td>\n",
              "      <td>1.451580</td>\n",
              "      <td>1.086175</td>\n",
              "      <td>-0.380396</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.516465</td>\n",
              "      <td>0.929423</td>\n",
              "      <td>0.427514</td>\n",
              "      <td>-0.346202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.437529</td>\n",
              "      <td>-2.728504</td>\n",
              "      <td>-0.272799</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.842888</td>\n",
              "      <td>-0.950551</td>\n",
              "      <td>1.453366</td>\n",
              "      <td>0.787194</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.322426</td>\n",
              "      <td>-0.531865</td>\n",
              "      <td>3.821934</td>\n",
              "      <td>0.467214</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.549902</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.710620</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.392342</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.063459</td>\n",
              "      <td>-0.264870</td>\n",
              "      <td>-0.282129</td>\n",
              "      <td>0.579519</td>\n",
              "      <td>-0.295348</td>\n",
              "      <td>0.862312</td>\n",
              "      <td>-1.058486</td>\n",
              "      <td>3.441066</td>\n",
              "      <td>-0.018105</td>\n",
              "      <td>1.730283</td>\n",
              "      <td>-0.812073</td>\n",
              "      <td>2.190999</td>\n",
              "      <td>-0.314562</td>\n",
              "      <td>-0.947911</td>\n",
              "      <td>1.077944</td>\n",
              "      <td>2.354170</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.806894</td>\n",
              "      <td>0.929423</td>\n",
              "      <td>0.076566</td>\n",
              "      <td>2.184979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.367267</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.017417</td>\n",
              "      <td>-0.761060</td>\n",
              "      <td>0.664321</td>\n",
              "      <td>0.880645</td>\n",
              "      <td>-0.117347</td>\n",
              "      <td>-0.515198</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.960354</td>\n",
              "      <td>-0.639383</td>\n",
              "      <td>-0.171118</td>\n",
              "      <td>-0.959478</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.971711</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.425938</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.063459</td>\n",
              "      <td>-0.241175</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.579519</td>\n",
              "      <td>-0.281609</td>\n",
              "      <td>-0.598149</td>\n",
              "      <td>-1.680961</td>\n",
              "      <td>-0.408557</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.704164</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.748924</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.841608</td>\n",
              "      <td>1.080546</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.075936</td>\n",
              "      <td>0.003371</td>\n",
              "      <td>4.731583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.483413</td>\n",
              "      <td>0.372753</td>\n",
              "      <td>-0.687531</td>\n",
              "      <td>-1.479957</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.611751</td>\n",
              "      <td>0.890597</td>\n",
              "      <td>-0.470751</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.207770</td>\n",
              "      <td>1.176883</td>\n",
              "      <td>-0.530640</td>\n",
              "      <td>-0.081702</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.617596</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.480064</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.426827</td>\n",
              "      <td>0.349962</td>\n",
              "      <td>0.206920</td>\n",
              "      <td>-0.249114</td>\n",
              "      <td>0.073433</td>\n",
              "      <td>0.579519</td>\n",
              "      <td>-0.294765</td>\n",
              "      <td>0.851496</td>\n",
              "      <td>1.008137</td>\n",
              "      <td>-0.649541</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.732697</td>\n",
              "      <td>1.151915</td>\n",
              "      <td>-0.479474</td>\n",
              "      <td>3.811727</td>\n",
              "      <td>0.833941</td>\n",
              "      <td>1.075952</td>\n",
              "      <td>-1.264847</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.819710</td>\n",
              "      <td>0.929423</td>\n",
              "      <td>-0.698450</td>\n",
              "      <td>-0.318218</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c41c3cc7-3211-4b21-b833-9c6c19006549')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c41c3cc7-3211-4b21-b833-9c6c19006549 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c41c3cc7-3211-4b21-b833-9c6c19006549');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "xtest2_n_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build a NN and run a grid search\n",
        "def build_classifier(activation = 'relu', dropout_rate = .5, neurons = 4):\n",
        "    # first step: create a Sequential object, as a sequence of layers. B/C NN is a sequence of layers.\n",
        "    classifier = Sequential()\n",
        "    # add the first hidden layer\n",
        "    classifier.add(Dense(units=neurons,kernel_initializer='glorot_uniform',\n",
        "                    activation = activation))\n",
        "    classifier.add(Dropout(dropout_rate))\n",
        "    # add the second hidden layer\n",
        "    classifier.add(Dense(units=neurons,kernel_initializer='glorot_uniform',\n",
        "                    activation = activation))\n",
        "    # add the output layer\n",
        "    classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',\n",
        "                    activation = 'sigmoid'))\n",
        "    # compiling the NN\n",
        "    classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "    return classifier\n",
        "\n",
        "\n",
        "classifier = KerasClassifier(build_fn=build_classifier, batch_size = 100)\n",
        "\n",
        "parameters = dict(activation = ['relu', 'tanh'], batch_size = [100, 10000], )\n",
        "grid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = 'roc_auc', cv=10, return_train_score=True)\n",
        "grid_search = grid_search.fit(xtrain_n_df, ytrain)\n",
        "\n",
        "best_parameters = grid_search.best_params_ \n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "P1KTIfTfJ7do",
        "outputId": "e355669a-2df3-4225-dc2a-c06c12f347d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-82-62d358e82ec6>:19: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  classifier = KerasClassifier(build_fn=build_classifier, batch_size = 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2424/2424 [==============================] - 10s 4ms/step - loss: 0.3973 - accuracy: 0.7959\n",
            "842/842 [==============================] - 2s 2ms/step\n",
            "7573/7573 [==============================] - 16s 2ms/step\n",
            "2424/2424 [==============================] - 9s 3ms/step - loss: 0.4429 - accuracy: 0.8191\n",
            "842/842 [==============================] - 2s 2ms/step\n",
            "7573/7573 [==============================] - 15s 2ms/step\n",
            "2424/2424 [==============================] - 10s 4ms/step - loss: 0.3996 - accuracy: 0.8179\n",
            "842/842 [==============================] - 2s 2ms/step\n",
            "7573/7573 [==============================] - 15s 2ms/step\n",
            "2424/2424 [==============================] - 9s 4ms/step - loss: 0.4054 - accuracy: 0.8164\n",
            "842/842 [==============================] - 2s 2ms/step\n",
            "7573/7573 [==============================] - 16s 2ms/step\n",
            "2424/2424 [==============================] - 10s 4ms/step - loss: 0.3946 - accuracy: 0.7835\n",
            "842/842 [==============================] - 2s 2ms/step\n",
            "7573/7573 [==============================] - 15s 2ms/step\n",
            "2424/2424 [==============================] - 10s 3ms/step - loss: 0.4469 - accuracy: 0.8145\n",
            "842/842 [==============================] - 2s 2ms/step\n",
            "7573/7573 [==============================] - 15s 2ms/step\n",
            "2424/2424 [==============================] - 8s 3ms/step - loss: 0.4155 - accuracy: 0.7699\n",
            "842/842 [==============================] - 2s 2ms/step\n",
            "7573/7573 [==============================] - 15s 2ms/step\n",
            "2424/2424 [==============================] - 9s 3ms/step - loss: 0.3977 - accuracy: 0.7813\n",
            "842/842 [==============================] - 2s 2ms/step\n",
            "4422/7573 [================>.............] - ETA: 5s"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(self, method_caller, clf, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod_caller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"decision_function\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_cached_call\u001b[0;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KerasClassifier' object has no attribute 'decision_function'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-62d358e82ec6>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tanh'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain_n_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mbest_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mtrain_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_MultimetricScorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mScore\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mapplied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0mon\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         return self._score(\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cached_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(self, method_caller, clf, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod_caller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predict_proba\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_cached_call\u001b[0;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;34m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mKeras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# check if binary classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2348\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2349\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2350\u001b[0;31m                         \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2351\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2352\u001b[0m                             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking values\n",
        "means = grid_search.cv_results_['mean_test_score']\n",
        "stds = grid_search.cv_results_['std_test_score']\n",
        "params = grid_search.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSqpl5iUQXKd",
        "outputId": "a1487303-ee51-4064-b40d-960294a167ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.924460 (0.003092) with: {'activation': 'relu', 'batch_size': 100}\n",
            "0.617315 (0.122796) with: {'activation': 'relu', 'batch_size': 10000}\n",
            "0.925803 (0.001744) with: {'activation': 'tanh', 'batch_size': 100}\n",
            "0.655991 (0.130437) with: {'activation': 'tanh', 'batch_size': 10000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ2VzV9KWYPY",
        "outputId": "293449db-1232-40af-e854-89a2921c4a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FREwu7UJYWGF",
        "outputId": "64b731f2-4093-42c8-b08f-fd106f84764a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'tanh'}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pred = grid_search.predict_proba(xtrain_n_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fal6QoJSYcXm",
        "outputId": "a0ae02d1-103d-416b-b2c9-ef977c195c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8414/8414 [==============================] - 43s 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ytrain.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF0QiaEQuS3x",
        "outputId": "ce52db2a-92a0-4973-a23d-d7fdc32fb012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "target    69028\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbkQeqe9tDWy",
        "outputId": "24d17cbc-07fd-421f-c10f-0621cb4c0a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9736629 , 0.02633711],\n",
              "       [0.970219  , 0.02978096],\n",
              "       [0.96727234, 0.03272764],\n",
              "       ...,\n",
              "       [0.49549836, 0.50450164],\n",
              "       [0.97080064, 0.02919936],\n",
              "       [0.9729349 , 0.02706511]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#roc - auc concepts by varying thresholds\n",
        "roc_auc_score(ytrain, train_pred[:, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlK083WWZd9D",
        "outputId": "ee64cbee-c2ab-4e02-b7c6-64483ca61912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9261858132078623"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test1_pred = grid_search.predict_proba(xtest1_n_df)[:,1]\n",
        "test2_pred = grid_search.predict_proba(xtest2_n_df)[:,1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIEentBFZmRd",
        "outputId": "ee96fe19-3ebd-414d-c00c-74f87a176f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "961/961 [==============================] - 1s 2ms/step\n",
            "2774/2774 [==============================] - 5s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(roc_auc_score(ytest1, test1_pred))\n",
        "print(roc_auc_score(ytest2, test2_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDL38Vp0ZuP0",
        "outputId": "94367beb-78cf-4d77-c5e4-c0dfe00b7c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9182272163901571\n",
            "0.938807971658403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8idkXpEYvtL",
        "outputId": "db30fff2-c093-4846-92fe-26288600c914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9282850734081782"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search.cv_results_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYRTdYtiZdEm",
        "outputId": "1e457199-e813-443c-8e5a-cc1b8b1f1a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([24.16289396, 25.82018354]),\n",
              " 'std_fit_time': array([6.02319552, 8.24766352]),\n",
              " 'mean_score_time': array([2.45665553, 2.47038836]),\n",
              " 'std_score_time': array([0.43214532, 0.3582605 ]),\n",
              " 'param_activation': masked_array(data=['relu', 'tanh'],\n",
              "              mask=[False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'activation': 'relu'}, {'activation': 'tanh'}],\n",
              " 'split0_test_score': array([0.92658136, 0.92623104]),\n",
              " 'split1_test_score': array([0.92750921, 0.92654195]),\n",
              " 'split2_test_score': array([0.92688978, 0.92681698]),\n",
              " 'split3_test_score': array([0.92984503, 0.92995832]),\n",
              " 'split4_test_score': array([0.93020029, 0.93007415]),\n",
              " 'split5_test_score': array([0.92676152, 0.92762218]),\n",
              " 'split6_test_score': array([0.92602759, 0.92687771]),\n",
              " 'split7_test_score': array([0.93133247, 0.93147751]),\n",
              " 'split8_test_score': array([0.92907383, 0.9293695 ]),\n",
              " 'split9_test_score': array([0.92787229, 0.9278814 ]),\n",
              " 'mean_test_score': array([0.92820934, 0.92828507]),\n",
              " 'std_test_score': array([0.00170271, 0.00171359]),\n",
              " 'rank_test_score': array([2, 1], dtype=int32)}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the AUC scores for all hyperparameters\n",
        "results = grid_search.cv_results_\n",
        "for i in range(len(results['params'])):\n",
        "    print(f\"Hyperparameters: {results['params'][i]}\")\n",
        "    #print(f\"Train AUC score: {results['mean_train_score'][i]:.3f}\")\n",
        "    print(f\"Test AUC score: {results['mean_test_score'][i]:.3f}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em0DDYHNZCzm",
        "outputId": "eae747d5-914f-4f1c-a4be-fc1d3c4ca0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'activation': 'relu', 'batch_size': 100}\n",
            "Test AUC score: 0.924\n",
            "\n",
            "Hyperparameters: {'activation': 'relu', 'batch_size': 10000}\n",
            "Test AUC score: 0.617\n",
            "\n",
            "Hyperparameters: {'activation': 'tanh', 'batch_size': 100}\n",
            "Test AUC score: 0.926\n",
            "\n",
            "Hyperparameters: {'activation': 'tanh', 'batch_size': 10000}\n",
            "Test AUC score: 0.656\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s96en6S053Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid Search sucks!"
      ],
      "metadata": {
        "id": "AuMWy5xU55TJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For 2 hidden Layers"
      ],
      "metadata": {
        "id": "PgRMkkNs6Ch_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first step: create a Sequential object, as a sequence of layers. B/C NN is a sequence of layers.\n",
        "classifier = Sequential()\n",
        "\n",
        "# add the first hidden layer. #kernel_initializer='glorot_uniform' => uniform distribution of weights w1, w2, w3.... to start off with\n",
        "classifier.add(Dense(units=6,kernel_initializer='glorot_uniform',\n",
        "                    activation = 'relu'))\n",
        "\n",
        "# add the second hidden layer\n",
        "classifier.add(Dense(units=6,kernel_initializer='glorot_uniform',\n",
        "                activation = 'relu'))\n",
        "#relu -> 0 if x<0 and x if x>0. Max(0,x). Caps negative values\n",
        "#tanh varies from -1 to 1\n",
        "#sigmoid varies from 0 to 1. Sigmoid is the output activation function for classification function as it's probability(0/1 in the output)\n",
        "#epoch - Circling the same training sample(input) n number of times. Eg: 1,00,000 is the input, batch size = 1000. Therefore iterations would be 100000/1000 = 100 cycles for one epoch.Each epoch takes 1000observations from input. next epoch next other observations. Random sampling. For the second epoch, it would be the same number of cycles for the same input. More iterations more accuracy.\n",
        "# add the output layer\n",
        "classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',\n",
        "                    activation = 'sigmoid'))\n",
        "\n",
        "# add additional parameters\n",
        "classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'FalseNegatives'])\n",
        "\n",
        "# train the model\n",
        "classifier.fit(xtrain_n_df,ytrain,batch_size=100,epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uetR94E87tq-",
        "outputId": "3c98d9f0-f8e4-4deb-ebad-c6eccf137949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2693/2693 [==============================] - 10s 3ms/step - loss: 0.3128 - accuracy: 0.8565 - false_negatives: 19575.0000\n",
            "Epoch 2/5\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2933 - accuracy: 0.8643 - false_negatives: 18017.0000\n",
            "Epoch 3/5\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2922 - accuracy: 0.8650 - false_negatives: 18013.0000\n",
            "Epoch 4/5\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2915 - accuracy: 0.8651 - false_negatives: 18089.0000\n",
            "Epoch 5/5\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2909 - accuracy: 0.8654 - false_negatives: 17989.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e981024f0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(xtrain_n_df)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20VinYRH8Ngm",
        "outputId": "09222344-7bdc-4aa7-cb05-441112bd6100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8414/8414 [==============================] - 15s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00198588],\n",
              "       [0.00736063],\n",
              "       [0.0289878 ],\n",
              "       ...,\n",
              "       [0.4429292 ],\n",
              "       [0.01241633],\n",
              "       [0.00346036]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(xtest2_n_df)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpZ5BkCcQs5I",
        "outputId": "fe701d13-4ade-4a94-d002-b8b0cfb713f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2774/2774 [==============================] - 8s 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00259744],\n",
              "       [0.02210935],\n",
              "       [0.8318695 ],\n",
              "       ...,\n",
              "       [0.01020003],\n",
              "       [0.59218216],\n",
              "       [0.08405238]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc_score(ytrain, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU7ANjCS8uly",
        "outputId": "bfd4abc3-8c40-42ca-d1cb-71b4a9cab2de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9306093281602095"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ytest2['target'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-aFXagP6M_B",
        "outputId": "ef2f3883-577b-4303-d253-b79632be3ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88750,)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred[:, 0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMWF57Xp6Wil",
        "outputId": "05989beb-03dd-4a15-a7f4-6badb1396e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88750,)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O9bQhIgj-Jn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first step: create a Sequential object, as a sequence of layers. B/C NN is a sequence of layers.\n",
        "#By increasing the batch size, over fitting decreases.\n",
        "t1 = t.time()\n",
        "grid_search_nn1 = pd.DataFrame(columns = ['hd', 'nodes', 'activation', 'dropout', 'batch_size', 'auc_train', 'auc_test1', 'auc_test2'])\n",
        "\n",
        "\n",
        "neurons = [4, 6]\n",
        "activations = ['relu', 'tanh']\n",
        "dropout = [.5, 0]\n",
        "batch_sizes = [100, 10000]\n",
        "\n",
        "row = 0\n",
        "for i in neurons:\n",
        "  for a in activations:\n",
        "    for d in dropout: #dropput : when the model is complex then it becomes overfitted. To reduce complexity and overfitting we drop the neurons. % of dropping the neurons.\n",
        "      for s in batch_sizes:\n",
        "        \n",
        "        grid_search_nn1.loc[row, 'nodes'] = i\n",
        "        grid_search_nn1.loc[row, 'activation'] = a\n",
        "        grid_search_nn1.loc[row, 'dropout'] = d\n",
        "        grid_search_nn1.loc[row, 'batch_size'] = s\n",
        "\n",
        "        classifier = Sequential()\n",
        "\n",
        "        # add the first hidden layer\n",
        "        classifier.add(Dense(units=i,kernel_initializer='glorot_uniform',\n",
        "                            activation = a))\n",
        "\n",
        "        classifier.add(Dropout(d))\n",
        "\n",
        "        # add the second hidden layer\n",
        "        classifier.add(Dense(units=i,kernel_initializer='glorot_uniform',\n",
        "                        activation = a))\n",
        "\n",
        "        classifier.add(Dropout(d))\n",
        "        # add the output layer\n",
        "        classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',\n",
        "                            activation = 'sigmoid'))\n",
        "\n",
        "        # add additional parameters\n",
        "        classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'FalseNegatives'])\n",
        "\n",
        "        # train the model\n",
        "        classifier.fit(xtrain_n_df,ytrain,batch_size=s,epochs=20)\n",
        "\n",
        "        grid_search_nn1.loc[row, 'auc_train'] = roc_auc_score(ytrain, classifier.predict(xtrain_n_df))\n",
        "        grid_search_nn1.loc[row, 'auc_test1'] = roc_auc_score(ytest1, classifier.predict(xtest1_n_df))\n",
        "        grid_search_nn1.loc[row, 'auc_test2'] = roc_auc_score(ytest2, classifier.predict(xtest2_n_df))\n",
        "        print(\"Best Parameter Iteration\",  row, \"AUC Train\", roc_auc_score(ytrain, classifier.predict(xtrain_n_df)))\n",
        "\n",
        "        row += 1\n",
        "\n",
        "grid_search_nn1['hd'] = 2\n",
        "t2 = t.time()\n",
        "print(t2 - t1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y8-RZWpurrd",
        "outputId": "f2bb994e-ae03-44f8-9602-40d4291d2b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 9s 2ms/step - loss: 0.5077 - accuracy: 0.7851 - false_negatives: 46684.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.4505 - accuracy: 0.8026 - false_negatives: 45459.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 6s 2ms/step - loss: 0.4496 - accuracy: 0.8027 - false_negatives: 45413.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.4502 - accuracy: 0.8032 - false_negatives: 45410.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.4497 - accuracy: 0.8024 - false_negatives: 45466.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.4481 - accuracy: 0.8041 - false_negatives: 45061.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.4485 - accuracy: 0.8037 - false_negatives: 45171.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.4487 - accuracy: 0.8032 - false_negatives: 45290.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.4487 - accuracy: 0.8026 - false_negatives: 45451.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.4488 - accuracy: 0.8034 - false_negatives: 45257.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.4498 - accuracy: 0.8023 - false_negatives: 45568.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.4495 - accuracy: 0.8026 - false_negatives: 45414.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.4490 - accuracy: 0.8035 - false_negatives: 45375.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.4503 - accuracy: 0.8023 - false_negatives: 45594.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.4487 - accuracy: 0.8035 - false_negatives: 45325.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.4488 - accuracy: 0.8031 - false_negatives: 45317.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.4497 - accuracy: 0.8025 - false_negatives: 45467.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.4497 - accuracy: 0.8021 - false_negatives: 45492.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.4494 - accuracy: 0.8026 - false_negatives: 45408.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.4492 - accuracy: 0.8034 - false_negatives: 45366.0000\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 5s 2ms/step\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "Best Parameter Iteration 0 AUC Train 0.9277041137794313\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 2s 12ms/step - loss: 0.8492 - accuracy: 0.5949 - false_negatives: 36432.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.7635 - accuracy: 0.6394 - false_negatives: 36794.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.7163 - accuracy: 0.6794 - false_negatives: 36979.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6886 - accuracy: 0.7101 - false_negatives: 37859.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6706 - accuracy: 0.7326 - false_negatives: 38893.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6548 - accuracy: 0.7491 - false_negatives: 40380.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6346 - accuracy: 0.7605 - false_negatives: 41367.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6083 - accuracy: 0.7642 - false_negatives: 41828.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.5785 - accuracy: 0.7582 - false_negatives: 42145.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.5557 - accuracy: 0.7614 - false_negatives: 42226.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.5415 - accuracy: 0.7646 - false_negatives: 42202.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5316 - accuracy: 0.7687 - false_negatives: 42173.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5232 - accuracy: 0.7732 - false_negatives: 42185.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.5174 - accuracy: 0.7773 - false_negatives: 42184.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5118 - accuracy: 0.7844 - false_negatives: 41790.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.5082 - accuracy: 0.7896 - false_negatives: 41670.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5050 - accuracy: 0.7951 - false_negatives: 41520.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.5016 - accuracy: 0.7996 - false_negatives: 41618.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4991 - accuracy: 0.8025 - false_negatives: 41534.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.4974 - accuracy: 0.8026 - false_negatives: 41829.0000\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 4s 1ms/step\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "Best Parameter Iteration 1 AUC Train 0.920849395803145\n",
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 20s 3ms/step - loss: 0.3351 - accuracy: 0.8410 - false_negatives: 20957.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 6s 2ms/step - loss: 0.3003 - accuracy: 0.8627 - false_negatives: 17454.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2951 - accuracy: 0.8642 - false_negatives: 18032.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 6s 2ms/step - loss: 0.2934 - accuracy: 0.8650 - false_negatives: 18100.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2925 - accuracy: 0.8653 - false_negatives: 18203.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 6s 2ms/step - loss: 0.2919 - accuracy: 0.8657 - false_negatives: 18205.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2915 - accuracy: 0.8660 - false_negatives: 18186.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.2912 - accuracy: 0.8661 - false_negatives: 18169.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2909 - accuracy: 0.8659 - false_negatives: 18227.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 6s 2ms/step - loss: 0.2907 - accuracy: 0.8659 - false_negatives: 18214.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2906 - accuracy: 0.8657 - false_negatives: 18246.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.2903 - accuracy: 0.8664 - false_negatives: 18099.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2900 - accuracy: 0.8665 - false_negatives: 17987.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2896 - accuracy: 0.8669 - false_negatives: 18046.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2893 - accuracy: 0.8667 - false_negatives: 17992.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.2892 - accuracy: 0.8666 - false_negatives: 17989.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2891 - accuracy: 0.8666 - false_negatives: 17878.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 6s 2ms/step - loss: 0.2889 - accuracy: 0.8668 - false_negatives: 17883.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2889 - accuracy: 0.8668 - false_negatives: 17961.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 6s 2ms/step - loss: 0.2888 - accuracy: 0.8667 - false_negatives: 17750.0000\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 4s 2ms/step\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "Best Parameter Iteration 2 AUC Train 0.9308565935329319\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 2s 9ms/step - loss: 0.6664 - accuracy: 0.6668 - false_negatives: 12086.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.6006 - accuracy: 0.7789 - false_negatives: 13440.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.5340 - accuracy: 0.8114 - false_negatives: 13985.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.4723 - accuracy: 0.8240 - false_negatives: 14323.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.4237 - accuracy: 0.8320 - false_negatives: 14527.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3891 - accuracy: 0.8374 - false_negatives: 14655.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3657 - accuracy: 0.8415 - false_negatives: 14566.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3501 - accuracy: 0.8450 - false_negatives: 14474.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3394 - accuracy: 0.8478 - false_negatives: 14412.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3318 - accuracy: 0.8501 - false_negatives: 14297.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3261 - accuracy: 0.8517 - false_negatives: 14323.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3218 - accuracy: 0.8531 - false_negatives: 14353.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.3184 - accuracy: 0.8544 - false_negatives: 14347.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3155 - accuracy: 0.8555 - false_negatives: 14426.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3132 - accuracy: 0.8562 - false_negatives: 14344.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3113 - accuracy: 0.8571 - false_negatives: 14490.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3096 - accuracy: 0.8577 - false_negatives: 14485.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3082 - accuracy: 0.8585 - false_negatives: 14582.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3070 - accuracy: 0.8590 - false_negatives: 14727.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.3059 - accuracy: 0.8593 - false_negatives: 14743.0000\n",
            "8414/8414 [==============================] - 15s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 4s 2ms/step\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "Best Parameter Iteration 3 AUC Train 0.9235673761444119\n",
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 10s 3ms/step - loss: 0.4214 - accuracy: 0.8133 - false_negatives: 30463.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3752 - accuracy: 0.8342 - false_negatives: 29946.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3715 - accuracy: 0.8338 - false_negatives: 31011.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3711 - accuracy: 0.8338 - false_negatives: 31349.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3706 - accuracy: 0.8332 - false_negatives: 31604.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3702 - accuracy: 0.8341 - false_negatives: 31493.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3695 - accuracy: 0.8350 - false_negatives: 31327.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3693 - accuracy: 0.8343 - false_negatives: 31509.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3693 - accuracy: 0.8346 - false_negatives: 31282.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3689 - accuracy: 0.8344 - false_negatives: 31223.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3703 - accuracy: 0.8339 - false_negatives: 31404.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3691 - accuracy: 0.8345 - false_negatives: 31290.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3693 - accuracy: 0.8344 - false_negatives: 31373.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3705 - accuracy: 0.8337 - false_negatives: 31527.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3689 - accuracy: 0.8352 - false_negatives: 31055.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3694 - accuracy: 0.8349 - false_negatives: 31150.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3697 - accuracy: 0.8349 - false_negatives: 31242.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.3696 - accuracy: 0.8348 - false_negatives: 31361.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3694 - accuracy: 0.8350 - false_negatives: 31144.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3700 - accuracy: 0.8342 - false_negatives: 31496.0000\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 4s 1ms/step\n",
            "8414/8414 [==============================] - 15s 2ms/step\n",
            "Best Parameter Iteration 4 AUC Train 0.9252213479291704\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 2s 12ms/step - loss: 0.8124 - accuracy: 0.5079 - false_negatives: 42120.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7144 - accuracy: 0.5871 - false_negatives: 38527.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.6487 - accuracy: 0.6465 - false_negatives: 35086.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.6036 - accuracy: 0.6895 - false_negatives: 32794.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.5700 - accuracy: 0.7182 - false_negatives: 31531.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5461 - accuracy: 0.7406 - false_negatives: 30716.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.5267 - accuracy: 0.7586 - false_negatives: 29924.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.5095 - accuracy: 0.7722 - false_negatives: 29880.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.4958 - accuracy: 0.7831 - false_negatives: 29764.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4832 - accuracy: 0.7918 - false_negatives: 29549.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.4719 - accuracy: 0.8004 - false_negatives: 29519.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4615 - accuracy: 0.8059 - false_negatives: 29621.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4533 - accuracy: 0.8118 - false_negatives: 29411.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4461 - accuracy: 0.8156 - false_negatives: 29597.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4388 - accuracy: 0.8195 - false_negatives: 29255.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4339 - accuracy: 0.8223 - false_negatives: 29094.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4289 - accuracy: 0.8231 - false_negatives: 29467.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4245 - accuracy: 0.8261 - false_negatives: 29140.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4206 - accuracy: 0.8263 - false_negatives: 29214.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4166 - accuracy: 0.8281 - false_negatives: 29258.0000\n",
            "8414/8414 [==============================] - 15s 2ms/step\n",
            "961/961 [==============================] - 1s 2ms/step\n",
            "2774/2774 [==============================] - 4s 2ms/step\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "Best Parameter Iteration 5 AUC Train 0.9229531918728886\n",
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 8s 2ms/step - loss: 0.3251 - accuracy: 0.8537 - false_negatives: 17820.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2969 - accuracy: 0.8638 - false_negatives: 18553.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 6s 2ms/step - loss: 0.2943 - accuracy: 0.8650 - false_negatives: 18579.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2931 - accuracy: 0.8655 - false_negatives: 18545.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 6s 2ms/step - loss: 0.2923 - accuracy: 0.8651 - false_negatives: 18598.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2917 - accuracy: 0.8654 - false_negatives: 18513.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 6s 2ms/step - loss: 0.2912 - accuracy: 0.8660 - false_negatives: 18271.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2909 - accuracy: 0.8656 - false_negatives: 18168.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 6s 2ms/step - loss: 0.2904 - accuracy: 0.8663 - false_negatives: 18103.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2901 - accuracy: 0.8664 - false_negatives: 18127.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 6s 2ms/step - loss: 0.2897 - accuracy: 0.8668 - false_negatives: 18030.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2893 - accuracy: 0.8672 - false_negatives: 17907.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.2890 - accuracy: 0.8669 - false_negatives: 18084.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2886 - accuracy: 0.8671 - false_negatives: 17856.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2885 - accuracy: 0.8673 - false_negatives: 17920.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2883 - accuracy: 0.8672 - false_negatives: 17851.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2882 - accuracy: 0.8674 - false_negatives: 17925.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2882 - accuracy: 0.8672 - false_negatives: 18005.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2881 - accuracy: 0.8674 - false_negatives: 17875.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.2881 - accuracy: 0.8674 - false_negatives: 17963.0000\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 4s 2ms/step\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "Best Parameter Iteration 6 AUC Train 0.931091685234803\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 2s 15ms/step - loss: 0.7201 - accuracy: 0.5254 - false_negatives: 38658.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.5918 - accuracy: 0.7077 - false_negatives: 28155.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.5163 - accuracy: 0.7771 - false_negatives: 21613.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.4681 - accuracy: 0.8022 - false_negatives: 18023.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.4355 - accuracy: 0.8159 - false_negatives: 16125.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.4116 - accuracy: 0.8259 - false_negatives: 15338.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3929 - accuracy: 0.8345 - false_negatives: 15165.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3778 - accuracy: 0.8414 - false_negatives: 15433.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3657 - accuracy: 0.8458 - false_negatives: 15995.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3561 - accuracy: 0.8494 - false_negatives: 16489.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3485 - accuracy: 0.8518 - false_negatives: 17071.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3424 - accuracy: 0.8535 - false_negatives: 17399.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3375 - accuracy: 0.8548 - false_negatives: 17786.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3335 - accuracy: 0.8558 - false_negatives: 18071.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3301 - accuracy: 0.8566 - false_negatives: 18309.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3272 - accuracy: 0.8575 - false_negatives: 18476.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3247 - accuracy: 0.8580 - false_negatives: 18639.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3226 - accuracy: 0.8584 - false_negatives: 18853.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 0s 9ms/step - loss: 0.3207 - accuracy: 0.8586 - false_negatives: 18870.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 0s 8ms/step - loss: 0.3190 - accuracy: 0.8590 - false_negatives: 18960.0000\n",
            "8414/8414 [==============================] - 15s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 5s 2ms/step\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "Best Parameter Iteration 7 AUC Train 0.9202265973709136\n",
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 10s 3ms/step - loss: 0.4371 - accuracy: 0.7698 - false_negatives: 50699.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3846 - accuracy: 0.7884 - false_negatives: 50183.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3822 - accuracy: 0.7882 - false_negatives: 50442.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3813 - accuracy: 0.7878 - false_negatives: 50678.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3803 - accuracy: 0.7893 - false_negatives: 50219.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3801 - accuracy: 0.7880 - false_negatives: 50622.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3798 - accuracy: 0.7891 - false_negatives: 50328.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3797 - accuracy: 0.7885 - false_negatives: 50362.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3794 - accuracy: 0.7889 - false_negatives: 50358.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3798 - accuracy: 0.7889 - false_negatives: 50344.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3800 - accuracy: 0.7884 - false_negatives: 50534.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.3800 - accuracy: 0.7889 - false_negatives: 50428.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3803 - accuracy: 0.7884 - false_negatives: 50454.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3799 - accuracy: 0.7884 - false_negatives: 50369.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3804 - accuracy: 0.7879 - false_negatives: 50538.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3790 - accuracy: 0.7881 - false_negatives: 50493.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3791 - accuracy: 0.7886 - false_negatives: 50461.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3795 - accuracy: 0.7886 - false_negatives: 50645.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3793 - accuracy: 0.7880 - false_negatives: 50672.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.3790 - accuracy: 0.7891 - false_negatives: 50248.0000\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "961/961 [==============================] - 1s 2ms/step\n",
            "2774/2774 [==============================] - 4s 2ms/step\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "Best Parameter Iteration 8 AUC Train 0.927967824611432\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 2s 14ms/step - loss: 0.9079 - accuracy: 0.5287 - false_negatives: 25196.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.7882 - accuracy: 0.6018 - false_negatives: 30100.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.7159 - accuracy: 0.6547 - false_negatives: 32639.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.6607 - accuracy: 0.7038 - false_negatives: 33815.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.6177 - accuracy: 0.7383 - false_negatives: 35791.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.5849 - accuracy: 0.7582 - false_negatives: 36987.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.5605 - accuracy: 0.7672 - false_negatives: 38401.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.5423 - accuracy: 0.7753 - false_negatives: 38906.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5295 - accuracy: 0.7809 - false_negatives: 39514.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.5160 - accuracy: 0.7877 - false_negatives: 39435.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.5074 - accuracy: 0.7894 - false_negatives: 39847.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.4991 - accuracy: 0.7924 - false_negatives: 40014.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4907 - accuracy: 0.7950 - false_negatives: 39713.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4834 - accuracy: 0.7967 - false_negatives: 39435.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.4760 - accuracy: 0.7973 - false_negatives: 39540.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.4680 - accuracy: 0.7992 - false_negatives: 39418.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4599 - accuracy: 0.8000 - false_negatives: 39034.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4538 - accuracy: 0.8016 - false_negatives: 39004.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.4474 - accuracy: 0.8027 - false_negatives: 38927.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.4416 - accuracy: 0.8045 - false_negatives: 38889.0000\n",
            "8414/8414 [==============================] - 14s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 4s 2ms/step\n",
            "8414/8414 [==============================] - 15s 2ms/step\n",
            "Best Parameter Iteration 9 AUC Train 0.9240539448429266\n",
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 8s 2ms/step - loss: 0.3140 - accuracy: 0.8584 - false_negatives: 18160.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2927 - accuracy: 0.8645 - false_negatives: 17966.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.2914 - accuracy: 0.8650 - false_negatives: 18169.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2904 - accuracy: 0.8654 - false_negatives: 18167.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.2895 - accuracy: 0.8660 - false_negatives: 18087.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2887 - accuracy: 0.8667 - false_negatives: 17960.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2878 - accuracy: 0.8668 - false_negatives: 17836.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2871 - accuracy: 0.8671 - false_negatives: 17592.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2864 - accuracy: 0.8675 - false_negatives: 17548.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2859 - accuracy: 0.8676 - false_negatives: 17496.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2857 - accuracy: 0.8678 - false_negatives: 17436.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2854 - accuracy: 0.8676 - false_negatives: 17419.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2851 - accuracy: 0.8678 - false_negatives: 17324.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2850 - accuracy: 0.8679 - false_negatives: 17387.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2847 - accuracy: 0.8683 - false_negatives: 17258.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2846 - accuracy: 0.8682 - false_negatives: 17359.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2844 - accuracy: 0.8684 - false_negatives: 17246.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2844 - accuracy: 0.8683 - false_negatives: 17371.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2842 - accuracy: 0.8683 - false_negatives: 17282.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 7s 2ms/step - loss: 0.2842 - accuracy: 0.8684 - false_negatives: 17240.0000\n",
            "8414/8414 [==============================] - 19s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 6s 2ms/step\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "Best Parameter Iteration 10 AUC Train 0.9328806047391967\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 2s 18ms/step - loss: 0.6679 - accuracy: 0.5765 - false_negatives: 48711.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.5456 - accuracy: 0.7421 - false_negatives: 38624.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.4529 - accuracy: 0.8050 - false_negatives: 29912.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.3920 - accuracy: 0.8268 - false_negatives: 23744.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.3582 - accuracy: 0.8357 - false_negatives: 19712.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3407 - accuracy: 0.8407 - false_negatives: 17214.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.3313 - accuracy: 0.8442 - false_negatives: 15894.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.3256 - accuracy: 0.8472 - false_negatives: 15381.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3215 - accuracy: 0.8499 - false_negatives: 15424.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.3180 - accuracy: 0.8526 - false_negatives: 15717.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3151 - accuracy: 0.8542 - false_negatives: 15727.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3127 - accuracy: 0.8556 - false_negatives: 15693.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3107 - accuracy: 0.8568 - false_negatives: 15872.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3088 - accuracy: 0.8579 - false_negatives: 16016.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3072 - accuracy: 0.8586 - false_negatives: 16185.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3057 - accuracy: 0.8595 - false_negatives: 16345.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3043 - accuracy: 0.8602 - false_negatives: 16573.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3031 - accuracy: 0.8607 - false_negatives: 16698.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3020 - accuracy: 0.8612 - false_negatives: 16987.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3011 - accuracy: 0.8618 - false_negatives: 17142.0000\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "961/961 [==============================] - 3s 3ms/step\n",
            "2774/2774 [==============================] - 5s 2ms/step\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "Best Parameter Iteration 11 AUC Train 0.9242780873645378\n",
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 12s 4ms/step - loss: 0.3941 - accuracy: 0.8264 - false_negatives: 25497.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3544 - accuracy: 0.8389 - false_negatives: 26821.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.3504 - accuracy: 0.8402 - false_negatives: 27436.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3461 - accuracy: 0.8422 - false_negatives: 27477.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3460 - accuracy: 0.8422 - false_negatives: 27738.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.3452 - accuracy: 0.8434 - false_negatives: 27392.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3448 - accuracy: 0.8425 - false_negatives: 27591.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.3457 - accuracy: 0.8419 - false_negatives: 27617.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3454 - accuracy: 0.8420 - false_negatives: 27355.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.3452 - accuracy: 0.8428 - false_negatives: 27249.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3461 - accuracy: 0.8422 - false_negatives: 27388.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3460 - accuracy: 0.8425 - false_negatives: 27324.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.3452 - accuracy: 0.8424 - false_negatives: 27263.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.3451 - accuracy: 0.8431 - false_negatives: 27125.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.3459 - accuracy: 0.8420 - false_negatives: 27370.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.3455 - accuracy: 0.8424 - false_negatives: 27183.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3472 - accuracy: 0.8419 - false_negatives: 27316.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.3456 - accuracy: 0.8424 - false_negatives: 26961.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3456 - accuracy: 0.8422 - false_negatives: 27113.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.3451 - accuracy: 0.8422 - false_negatives: 27029.0000\n",
            "8414/8414 [==============================] - 16s 2ms/step\n",
            "961/961 [==============================] - 3s 3ms/step\n",
            "2774/2774 [==============================] - 5s 2ms/step\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "Best Parameter Iteration 12 AUC Train 0.9257044003506048\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 2s 14ms/step - loss: 0.7461 - accuracy: 0.5321 - false_negatives: 37247.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.6430 - accuracy: 0.6551 - false_negatives: 30386.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.5730 - accuracy: 0.7381 - false_negatives: 25463.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.5325 - accuracy: 0.7739 - false_negatives: 22878.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.5044 - accuracy: 0.7923 - false_negatives: 21731.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4835 - accuracy: 0.8049 - false_negatives: 21300.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.4653 - accuracy: 0.8147 - false_negatives: 21193.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4512 - accuracy: 0.8204 - false_negatives: 21548.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.4377 - accuracy: 0.8264 - false_negatives: 21702.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4275 - accuracy: 0.8306 - false_negatives: 22049.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4169 - accuracy: 0.8346 - false_negatives: 22152.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4103 - accuracy: 0.8364 - false_negatives: 22593.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.4043 - accuracy: 0.8390 - false_negatives: 22305.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3980 - accuracy: 0.8412 - false_negatives: 22583.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3940 - accuracy: 0.8424 - false_negatives: 22705.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3909 - accuracy: 0.8424 - false_negatives: 22897.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3870 - accuracy: 0.8436 - false_negatives: 22954.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3847 - accuracy: 0.8440 - false_negatives: 23420.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3813 - accuracy: 0.8451 - false_negatives: 23345.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3790 - accuracy: 0.8457 - false_negatives: 23455.0000\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 7s 2ms/step\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "Best Parameter Iteration 13 AUC Train 0.9257244738055399\n",
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 11s 3ms/step - loss: 0.3244 - accuracy: 0.8509 - false_negatives: 18310.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2969 - accuracy: 0.8641 - false_negatives: 17766.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2934 - accuracy: 0.8644 - false_negatives: 18211.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2918 - accuracy: 0.8652 - false_negatives: 18120.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2907 - accuracy: 0.8656 - false_negatives: 18038.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 9s 4ms/step - loss: 0.2898 - accuracy: 0.8662 - false_negatives: 17871.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2890 - accuracy: 0.8666 - false_negatives: 17854.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2884 - accuracy: 0.8672 - false_negatives: 17775.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2876 - accuracy: 0.8675 - false_negatives: 17604.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2872 - accuracy: 0.8683 - false_negatives: 17487.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2866 - accuracy: 0.8682 - false_negatives: 17439.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2862 - accuracy: 0.8688 - false_negatives: 17318.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2861 - accuracy: 0.8687 - false_negatives: 17321.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2858 - accuracy: 0.8690 - false_negatives: 17297.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2857 - accuracy: 0.8690 - false_negatives: 17244.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 7s 3ms/step - loss: 0.2857 - accuracy: 0.8687 - false_negatives: 17270.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2855 - accuracy: 0.8692 - false_negatives: 17182.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2854 - accuracy: 0.8691 - false_negatives: 17224.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2854 - accuracy: 0.8688 - false_negatives: 17198.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2853 - accuracy: 0.8690 - false_negatives: 17169.0000\n",
            "8414/8414 [==============================] - 16s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 6s 2ms/step\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "Best Parameter Iteration 14 AUC Train 0.9324551425566592\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 2s 11ms/step - loss: 0.7626 - accuracy: 0.3886 - false_negatives: 48139.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.6239 - accuracy: 0.6555 - false_negatives: 35720.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.5205 - accuracy: 0.7835 - false_negatives: 21650.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.4514 - accuracy: 0.8186 - false_negatives: 14265.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.4103 - accuracy: 0.8328 - false_negatives: 12418.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3840 - accuracy: 0.8417 - false_negatives: 12845.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3657 - accuracy: 0.8478 - false_negatives: 13885.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3527 - accuracy: 0.8522 - false_negatives: 14903.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3432 - accuracy: 0.8546 - false_negatives: 15762.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.3361 - accuracy: 0.8564 - false_negatives: 16201.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3304 - accuracy: 0.8578 - false_negatives: 16673.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3259 - accuracy: 0.8588 - false_negatives: 16881.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3221 - accuracy: 0.8596 - false_negatives: 17121.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3190 - accuracy: 0.8605 - false_negatives: 17277.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3163 - accuracy: 0.8612 - false_negatives: 17435.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.3140 - accuracy: 0.8618 - false_negatives: 17624.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.3121 - accuracy: 0.8624 - false_negatives: 17737.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3105 - accuracy: 0.8626 - false_negatives: 17857.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3090 - accuracy: 0.8629 - false_negatives: 18052.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3078 - accuracy: 0.8630 - false_negatives: 18158.0000\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 5s 2ms/step\n",
            "8414/8414 [==============================] - 16s 2ms/step\n",
            "Best Parameter Iteration 15 AUC Train 0.9249216235575672\n",
            "2283.143643140793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search_nn1['hd'] = 2"
      ],
      "metadata": {
        "id": "3Vo40d-C_p_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search_nn1.to_csv('grid_1.csv', index = False)"
      ],
      "metadata": {
        "id": "zmRbiTcgD8Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ijFVCFjWGJO",
        "outputId": "58623149-41ca-4d1e-f463-23256baa1a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while isn"
      ],
      "metadata": {
        "id": "BDJjdTTEXJtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first step: create a Sequential object, as a sequence of layers. B/C NN is a sequence of layers.\n",
        "t1 = t.time()\n",
        "grid_search_nn2 = pd.DataFrame(columns = ['hd', 'nodes', 'activation', 'dropout', 'batch_size', 'auc_train', 'auc_test1', 'auc_test2'])\n",
        "\n",
        "\n",
        "neurons = [4, 6]\n",
        "activations = ['relu', 'tanh']\n",
        "dropout = [.5, 0]\n",
        "batch_sizes = [100, 10000]\n",
        "\n",
        "row = 0\n",
        "for i in neurons:\n",
        "  for a in activations:\n",
        "    for d in dropout:\n",
        "      for s in batch_sizes:\n",
        "        \n",
        "        grid_search_nn1.loc[row, 'nodes'] = i\n",
        "        grid_search_nn1.loc[row, 'activation'] = a\n",
        "        grid_search_nn1.loc[row, 'dropout'] = d\n",
        "        grid_search_nn1.loc[row, 'batch_size'] = s\n",
        "\n",
        "        classifier = Sequential()\n",
        "\n",
        "        # add the first hidden layer\n",
        "        classifier.add(Dense(units=i,kernel_initializer='glorot_uniform',\n",
        "                            activation = a))\n",
        "        classifier.add(Dropout(d))\n",
        "\n",
        "        # add the second hidden layer\n",
        "        classifier.add(Dense(units=i,kernel_initializer='glorot_uniform',\n",
        "                        activation = a))\n",
        "        classifier.add(Dropout(d))\n",
        "\n",
        "        # add the third hidden layer\n",
        "        classifier.add(Dense(units=i,kernel_initializer='glorot_uniform',\n",
        "                        activation = a))\n",
        "        classifier.add(Dropout(d))\n",
        "\n",
        "        # add the fourth hidden layer\n",
        "        classifier.add(Dense(units=i,kernel_initializer='glorot_uniform',\n",
        "                        activation = a))\n",
        "        classifier.add(Dropout(d))\n",
        "\n",
        "        # add the output layer\n",
        "        classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',\n",
        "                            activation = 'sigmoid'))\n",
        "\n",
        "        # add additional parameters\n",
        "        classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy', 'FalseNegatives'])\n",
        "\n",
        "        # train the model\n",
        "        classifier.fit(xtrain_n_df,ytrain,batch_size=s,epochs=20)\n",
        "\n",
        "        grid_search_nn1.loc[row, 'auc_train'] = roc_auc_score(ytrain, classifier.predict(xtrain_n_df))\n",
        "        grid_search_nn1.loc[row, 'auc_test1'] = roc_auc_score(ytest1, classifier.predict(xtest1_n_df))\n",
        "        grid_search_nn1.loc[row, 'auc_test2'] = roc_auc_score(ytest2, classifier.predict(xtest2_n_df))\n",
        "        print(\"Best Parameter Iteration\",  row, \"AUC Train\", roc_auc_score(ytrain, classifier.predict(xtrain_n_df)))\n",
        "\n",
        "        row += 1\n",
        "        \n",
        "grid_search_nn2['hd'] = 4\n",
        "t2 = t.time()\n",
        "print(t2 - t1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlWimQUOEEQi",
        "outputId": "0a1ee917-00ed-4ec0-ba44-8600834586bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 19s 4ms/step - loss: 0.5641 - accuracy: 0.7436 - false_negatives: 68994.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 19s 7ms/step - loss: 0.5254 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 14s 5ms/step - loss: 0.5229 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.5233 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.5234 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 13s 5ms/step - loss: 0.5228 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 13s 5ms/step - loss: 0.5218 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 13s 5ms/step - loss: 0.5230 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.5231 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.5225 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.5233 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.5229 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.5226 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 12s 4ms/step - loss: 0.5227 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 12s 4ms/step - loss: 0.5222 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.5232 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.5229 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.5226 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.5227 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.5224 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "8414/8414 [==============================] - 18s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 7s 2ms/step\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "Best Parameter Iteration 0 AUC Train 0.880967772257097\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 3s 17ms/step - loss: 0.6697 - accuracy: 0.7410 - false_negatives: 68873.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.6404 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.6154 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.5979 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.5830 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.5700 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.5616 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.5529 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.5473 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.5428 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.5378 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.5345 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.5302 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.5267 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.5225 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.5186 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.5143 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.5117 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.5100 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 0.5073 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "8414/8414 [==============================] - 18s 2ms/step\n",
            "961/961 [==============================] - 3s 3ms/step\n",
            "2774/2774 [==============================] - 5s 2ms/step\n",
            "8414/8414 [==============================] - 27s 3ms/step\n",
            "Best Parameter Iteration 1 AUC Train 0.8836523721512904\n",
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 16s 5ms/step - loss: 0.3495 - accuracy: 0.8501 - false_negatives: 24024.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2963 - accuracy: 0.8640 - false_negatives: 19206.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 14s 5ms/step - loss: 0.2930 - accuracy: 0.8651 - false_negatives: 19120.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 15s 6ms/step - loss: 0.2915 - accuracy: 0.8656 - false_negatives: 19051.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 16s 6ms/step - loss: 0.2906 - accuracy: 0.8657 - false_negatives: 18977.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.2900 - accuracy: 0.8663 - false_negatives: 18968.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2896 - accuracy: 0.8665 - false_negatives: 18874.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.2894 - accuracy: 0.8667 - false_negatives: 18854.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.2890 - accuracy: 0.8663 - false_negatives: 18880.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2888 - accuracy: 0.8671 - false_negatives: 18626.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2883 - accuracy: 0.8672 - false_negatives: 18585.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2882 - accuracy: 0.8674 - false_negatives: 18527.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2880 - accuracy: 0.8671 - false_negatives: 18563.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2880 - accuracy: 0.8672 - false_negatives: 18620.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2880 - accuracy: 0.8673 - false_negatives: 18560.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2878 - accuracy: 0.8675 - false_negatives: 18489.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 17s 6ms/step - loss: 0.2878 - accuracy: 0.8674 - false_negatives: 18545.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 13s 5ms/step - loss: 0.2877 - accuracy: 0.8674 - false_negatives: 18481.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 17s 6ms/step - loss: 0.2876 - accuracy: 0.8675 - false_negatives: 18468.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2877 - accuracy: 0.8675 - false_negatives: 18534.0000\n",
            "8414/8414 [==============================] - 24s 3ms/step\n",
            "961/961 [==============================] - 5s 5ms/step\n",
            "2774/2774 [==============================] - 7s 3ms/step\n",
            "8414/8414 [==============================] - 22s 3ms/step\n",
            "Best Parameter Iteration 2 AUC Train 0.9311359380917933\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 4s 25ms/step - loss: 0.6361 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 1s 24ms/step - loss: 0.5459 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.4659 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.4193 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.3959 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3814 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3706 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3620 - accuracy: 0.7436 - false_negatives: 69028.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3563 - accuracy: 0.8146 - false_negatives: 35534.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3506 - accuracy: 0.8514 - false_negatives: 18342.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3417 - accuracy: 0.8525 - false_negatives: 17411.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3338 - accuracy: 0.8535 - false_negatives: 16655.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3277 - accuracy: 0.8544 - false_negatives: 16574.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3231 - accuracy: 0.8555 - false_negatives: 16430.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.3196 - accuracy: 0.8569 - false_negatives: 16862.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3165 - accuracy: 0.8582 - false_negatives: 17635.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3134 - accuracy: 0.8593 - false_negatives: 18450.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 10ms/step - loss: 0.3103 - accuracy: 0.8604 - false_negatives: 19072.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3076 - accuracy: 0.8611 - false_negatives: 19235.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3054 - accuracy: 0.8616 - false_negatives: 18929.0000\n",
            "8414/8414 [==============================] - 16s 2ms/step\n",
            "961/961 [==============================] - 2s 3ms/step\n",
            "2774/2774 [==============================] - 6s 2ms/step\n",
            "8414/8414 [==============================] - 16s 2ms/step\n",
            "Best Parameter Iteration 3 AUC Train 0.9228106352426662\n",
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 12s 4ms/step - loss: 0.4633 - accuracy: 0.7894 - false_negatives: 38436.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.4148 - accuracy: 0.8049 - false_negatives: 38441.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.4091 - accuracy: 0.8078 - false_negatives: 38490.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.4090 - accuracy: 0.8073 - false_negatives: 39117.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.4063 - accuracy: 0.8077 - false_negatives: 39043.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.4067 - accuracy: 0.8078 - false_negatives: 39097.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.4058 - accuracy: 0.8080 - false_negatives: 38765.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.4059 - accuracy: 0.8076 - false_negatives: 38996.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 20s 8ms/step - loss: 0.4056 - accuracy: 0.8080 - false_negatives: 38660.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.4049 - accuracy: 0.8084 - false_negatives: 38846.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.4065 - accuracy: 0.8068 - false_negatives: 39068.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.4047 - accuracy: 0.8084 - false_negatives: 38874.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.4050 - accuracy: 0.8088 - false_negatives: 38785.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 15s 5ms/step - loss: 0.4038 - accuracy: 0.8097 - false_negatives: 38530.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 16s 6ms/step - loss: 0.4067 - accuracy: 0.8072 - false_negatives: 39267.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 15s 6ms/step - loss: 0.4064 - accuracy: 0.8078 - false_negatives: 39198.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 15s 6ms/step - loss: 0.4041 - accuracy: 0.8088 - false_negatives: 39041.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 15s 5ms/step - loss: 0.4051 - accuracy: 0.8087 - false_negatives: 38980.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 14s 5ms/step - loss: 0.4051 - accuracy: 0.8083 - false_negatives: 39111.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.4041 - accuracy: 0.8095 - false_negatives: 38727.0000\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "961/961 [==============================] - 2s 3ms/step\n",
            "2774/2774 [==============================] - 4s 2ms/step\n",
            "8414/8414 [==============================] - 16s 2ms/step\n",
            "Best Parameter Iteration 4 AUC Train 0.9253916358379712\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 4s 27ms/step - loss: 0.7676 - accuracy: 0.5592 - false_negatives: 45456.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 1s 25ms/step - loss: 0.7138 - accuracy: 0.6023 - false_negatives: 47852.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.6804 - accuracy: 0.6332 - false_negatives: 50671.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.6579 - accuracy: 0.6560 - false_negatives: 53123.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.6336 - accuracy: 0.6801 - false_negatives: 54662.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.6125 - accuracy: 0.6988 - false_negatives: 54943.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.5942 - accuracy: 0.7133 - false_negatives: 54730.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.5778 - accuracy: 0.7280 - false_negatives: 53923.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.5649 - accuracy: 0.7380 - false_negatives: 53276.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.5514 - accuracy: 0.7484 - false_negatives: 52550.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.5395 - accuracy: 0.7565 - false_negatives: 51370.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.5272 - accuracy: 0.7635 - false_negatives: 49951.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.5146 - accuracy: 0.7713 - false_negatives: 48037.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.5035 - accuracy: 0.7781 - false_negatives: 46147.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.4930 - accuracy: 0.7841 - false_negatives: 44526.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.4851 - accuracy: 0.7896 - false_negatives: 42930.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.4783 - accuracy: 0.7926 - false_negatives: 42085.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.4730 - accuracy: 0.7966 - false_negatives: 41268.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 0s 16ms/step - loss: 0.4675 - accuracy: 0.8003 - false_negatives: 40404.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.4629 - accuracy: 0.8029 - false_negatives: 39526.0000\n",
            "8414/8414 [==============================] - 16s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 6s 2ms/step\n",
            "8414/8414 [==============================] - 16s 2ms/step\n",
            "Best Parameter Iteration 5 AUC Train 0.9245643028691135\n",
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 12s 4ms/step - loss: 0.3300 - accuracy: 0.8565 - false_negatives: 16894.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2987 - accuracy: 0.8630 - false_negatives: 18280.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2954 - accuracy: 0.8637 - false_negatives: 18517.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.2938 - accuracy: 0.8645 - false_negatives: 18425.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2926 - accuracy: 0.8654 - false_negatives: 18214.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2918 - accuracy: 0.8656 - false_negatives: 18057.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.2913 - accuracy: 0.8658 - false_negatives: 18115.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2908 - accuracy: 0.8660 - false_negatives: 18013.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 13s 5ms/step - loss: 0.2906 - accuracy: 0.8658 - false_negatives: 18049.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 15s 5ms/step - loss: 0.2902 - accuracy: 0.8662 - false_negatives: 17959.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.2897 - accuracy: 0.8665 - false_negatives: 17868.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 16s 6ms/step - loss: 0.2890 - accuracy: 0.8666 - false_negatives: 17777.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 15s 6ms/step - loss: 0.2887 - accuracy: 0.8671 - false_negatives: 17659.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 13s 5ms/step - loss: 0.2885 - accuracy: 0.8667 - false_negatives: 17722.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.2884 - accuracy: 0.8663 - false_negatives: 17849.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 12s 4ms/step - loss: 0.2882 - accuracy: 0.8669 - false_negatives: 17809.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.2881 - accuracy: 0.8667 - false_negatives: 17806.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.2879 - accuracy: 0.8670 - false_negatives: 17835.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 12s 4ms/step - loss: 0.2878 - accuracy: 0.8670 - false_negatives: 17792.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 13s 5ms/step - loss: 0.2878 - accuracy: 0.8668 - false_negatives: 17804.0000\n",
            "8414/8414 [==============================] - 19s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 5s 2ms/step\n",
            "8414/8414 [==============================] - 18s 2ms/step\n",
            "Best Parameter Iteration 6 AUC Train 0.931316759492238\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 3s 14ms/step - loss: 0.6998 - accuracy: 0.5807 - false_negatives: 30116.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.5380 - accuracy: 0.7432 - false_negatives: 18460.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 1s 20ms/step - loss: 0.4700 - accuracy: 0.7944 - false_negatives: 14089.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 1s 21ms/step - loss: 0.4345 - accuracy: 0.8152 - false_negatives: 13334.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.4103 - accuracy: 0.8278 - false_negatives: 13896.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.3921 - accuracy: 0.8363 - false_negatives: 15048.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.3783 - accuracy: 0.8430 - false_negatives: 16184.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.3680 - accuracy: 0.8476 - false_negatives: 16953.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.3600 - accuracy: 0.8505 - false_negatives: 17407.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.3535 - accuracy: 0.8526 - false_negatives: 17594.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 1s 20ms/step - loss: 0.3479 - accuracy: 0.8543 - false_negatives: 17737.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 14ms/step - loss: 0.3430 - accuracy: 0.8558 - false_negatives: 17780.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.3385 - accuracy: 0.8569 - false_negatives: 17683.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.3345 - accuracy: 0.8576 - false_negatives: 17795.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.3309 - accuracy: 0.8583 - false_negatives: 17745.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.3277 - accuracy: 0.8588 - false_negatives: 17708.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.3249 - accuracy: 0.8593 - false_negatives: 17624.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 13ms/step - loss: 0.3223 - accuracy: 0.8597 - false_negatives: 17707.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.3200 - accuracy: 0.8600 - false_negatives: 17595.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.3180 - accuracy: 0.8603 - false_negatives: 17619.0000\n",
            "8414/8414 [==============================] - 18s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 6s 2ms/step\n",
            "8414/8414 [==============================] - 18s 2ms/step\n",
            "Best Parameter Iteration 7 AUC Train 0.9224844999917376\n",
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 14s 4ms/step - loss: 0.5017 - accuracy: 0.7529 - false_negatives: 63872.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.4402 - accuracy: 0.7591 - false_negatives: 56820.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.4337 - accuracy: 0.7594 - false_negatives: 52179.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.4326 - accuracy: 0.7604 - false_negatives: 51833.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.4333 - accuracy: 0.7600 - false_negatives: 52036.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 9s 4ms/step - loss: 0.4313 - accuracy: 0.7613 - false_negatives: 51791.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.4317 - accuracy: 0.7620 - false_negatives: 51988.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.4307 - accuracy: 0.7617 - false_negatives: 51711.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.4329 - accuracy: 0.7609 - false_negatives: 52182.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.4321 - accuracy: 0.7618 - false_negatives: 51833.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.4317 - accuracy: 0.7615 - false_negatives: 51839.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.4297 - accuracy: 0.7612 - false_negatives: 52065.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.4309 - accuracy: 0.7617 - false_negatives: 51951.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.4308 - accuracy: 0.7612 - false_negatives: 52220.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.4318 - accuracy: 0.7614 - false_negatives: 52319.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.4298 - accuracy: 0.7622 - false_negatives: 52101.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.4313 - accuracy: 0.7620 - false_negatives: 52063.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.4314 - accuracy: 0.7621 - false_negatives: 52124.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.4294 - accuracy: 0.7617 - false_negatives: 52089.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.4307 - accuracy: 0.7611 - false_negatives: 52193.0000\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "961/961 [==============================] - 3s 3ms/step\n",
            "2774/2774 [==============================] - 5s 2ms/step\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "Best Parameter Iteration 8 AUC Train 0.9274966065084369\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 4s 22ms/step - loss: 0.8103 - accuracy: 0.5883 - false_negatives: 47965.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 1s 20ms/step - loss: 0.7264 - accuracy: 0.6654 - false_negatives: 53675.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.6911 - accuracy: 0.7044 - false_negatives: 56930.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 1s 20ms/step - loss: 0.6712 - accuracy: 0.7270 - false_negatives: 59396.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 1s 21ms/step - loss: 0.6575 - accuracy: 0.7382 - false_negatives: 60970.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 1s 18ms/step - loss: 0.6455 - accuracy: 0.7448 - false_negatives: 62060.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 1s 18ms/step - loss: 0.6356 - accuracy: 0.7480 - false_negatives: 62760.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.6264 - accuracy: 0.7503 - false_negatives: 63194.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 1s 20ms/step - loss: 0.6183 - accuracy: 0.7516 - false_negatives: 63458.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 1s 22ms/step - loss: 0.6095 - accuracy: 0.7532 - false_negatives: 63438.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 1s 33ms/step - loss: 0.6014 - accuracy: 0.7556 - false_negatives: 62725.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 1s 27ms/step - loss: 0.5941 - accuracy: 0.7575 - false_negatives: 61824.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 1s 31ms/step - loss: 0.5837 - accuracy: 0.7623 - false_negatives: 60084.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 1s 34ms/step - loss: 0.5729 - accuracy: 0.7687 - false_negatives: 57283.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 1s 42ms/step - loss: 0.5644 - accuracy: 0.7727 - false_negatives: 55357.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 1s 47ms/step - loss: 0.5566 - accuracy: 0.7764 - false_negatives: 54294.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 1s 55ms/step - loss: 0.5487 - accuracy: 0.7790 - false_negatives: 53319.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 1s 37ms/step - loss: 0.5428 - accuracy: 0.7823 - false_negatives: 52162.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 1s 41ms/step - loss: 0.5361 - accuracy: 0.7851 - false_negatives: 51037.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 1s 43ms/step - loss: 0.5305 - accuracy: 0.7874 - false_negatives: 50186.0000\n",
            "8414/8414 [==============================] - 23s 3ms/step\n",
            "961/961 [==============================] - 4s 4ms/step\n",
            "2774/2774 [==============================] - 7s 3ms/step\n",
            "8414/8414 [==============================] - 29s 3ms/step\n",
            "Best Parameter Iteration 9 AUC Train 0.8968160023304381\n",
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 17s 6ms/step - loss: 0.3232 - accuracy: 0.8516 - false_negatives: 22378.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 13s 5ms/step - loss: 0.2948 - accuracy: 0.8647 - false_negatives: 18616.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 12s 5ms/step - loss: 0.2909 - accuracy: 0.8652 - false_negatives: 18392.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2898 - accuracy: 0.8661 - false_negatives: 18164.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2892 - accuracy: 0.8667 - false_negatives: 18063.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.2884 - accuracy: 0.8668 - false_negatives: 17985.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2879 - accuracy: 0.8674 - false_negatives: 17869.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2872 - accuracy: 0.8679 - false_negatives: 17625.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2865 - accuracy: 0.8680 - false_negatives: 17472.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2858 - accuracy: 0.8679 - false_negatives: 17607.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2855 - accuracy: 0.8686 - false_negatives: 17482.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2853 - accuracy: 0.8684 - false_negatives: 17336.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2850 - accuracy: 0.8684 - false_negatives: 17243.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2848 - accuracy: 0.8688 - false_negatives: 17250.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2847 - accuracy: 0.8688 - false_negatives: 17360.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2847 - accuracy: 0.8687 - false_negatives: 17146.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.2845 - accuracy: 0.8690 - false_negatives: 17105.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2845 - accuracy: 0.8689 - false_negatives: 17216.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 8s 3ms/step - loss: 0.2843 - accuracy: 0.8692 - false_negatives: 17160.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.2843 - accuracy: 0.8687 - false_negatives: 17227.0000\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 7s 2ms/step\n",
            "8414/8414 [==============================] - 17s 2ms/step\n",
            "Best Parameter Iteration 10 AUC Train 0.933012127568126\n",
            "Epoch 1/20\n",
            "27/27 [==============================] - 3s 21ms/step - loss: 0.7056 - accuracy: 0.4169 - false_negatives: 27236.0000\n",
            "Epoch 2/20\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.6822 - accuracy: 0.7241 - false_negatives: 46240.0000\n",
            "Epoch 3/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.6627 - accuracy: 0.7658 - false_negatives: 46768.0000\n",
            "Epoch 4/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.6291 - accuracy: 0.8050 - false_negatives: 33481.0000\n",
            "Epoch 5/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.5641 - accuracy: 0.8326 - false_negatives: 21030.0000\n",
            "Epoch 6/20\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.4537 - accuracy: 0.8449 - false_negatives: 19327.0000\n",
            "Epoch 7/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.3555 - accuracy: 0.8526 - false_negatives: 20152.0000\n",
            "Epoch 8/20\n",
            "27/27 [==============================] - 1s 19ms/step - loss: 0.3212 - accuracy: 0.8567 - false_negatives: 20067.0000\n",
            "Epoch 9/20\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.3113 - accuracy: 0.8585 - false_negatives: 19920.0000\n",
            "Epoch 10/20\n",
            "27/27 [==============================] - 0s 18ms/step - loss: 0.3066 - accuracy: 0.8601 - false_negatives: 19483.0000\n",
            "Epoch 11/20\n",
            "27/27 [==============================] - 0s 17ms/step - loss: 0.3037 - accuracy: 0.8614 - false_negatives: 19118.0000\n",
            "Epoch 12/20\n",
            "27/27 [==============================] - 0s 15ms/step - loss: 0.3017 - accuracy: 0.8621 - false_negatives: 19056.0000\n",
            "Epoch 13/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.3002 - accuracy: 0.8625 - false_negatives: 19165.0000\n",
            "Epoch 14/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.2990 - accuracy: 0.8630 - false_negatives: 18984.0000\n",
            "Epoch 15/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.2981 - accuracy: 0.8635 - false_negatives: 18917.0000\n",
            "Epoch 16/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.2974 - accuracy: 0.8636 - false_negatives: 18959.0000\n",
            "Epoch 17/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.2968 - accuracy: 0.8640 - false_negatives: 18868.0000\n",
            "Epoch 18/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.2964 - accuracy: 0.8641 - false_negatives: 18893.0000\n",
            "Epoch 19/20\n",
            "27/27 [==============================] - 0s 12ms/step - loss: 0.2960 - accuracy: 0.8644 - false_negatives: 18908.0000\n",
            "Epoch 20/20\n",
            "27/27 [==============================] - 0s 11ms/step - loss: 0.2956 - accuracy: 0.8645 - false_negatives: 19122.0000\n",
            "8414/8414 [==============================] - 22s 3ms/step\n",
            "961/961 [==============================] - 2s 2ms/step\n",
            "2774/2774 [==============================] - 5s 2ms/step\n",
            "8414/8414 [==============================] - 18s 2ms/step\n",
            "Best Parameter Iteration 11 AUC Train 0.9269767537056498\n",
            "Epoch 1/20\n",
            "2693/2693 [==============================] - 22s 7ms/step - loss: 0.4403 - accuracy: 0.7954 - false_negatives: 33285.0000\n",
            "Epoch 2/20\n",
            "2693/2693 [==============================] - 12s 4ms/step - loss: 0.3798 - accuracy: 0.8255 - false_negatives: 27563.0000\n",
            "Epoch 3/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.3728 - accuracy: 0.8304 - false_negatives: 27460.0000\n",
            "Epoch 4/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.3710 - accuracy: 0.8315 - false_negatives: 27619.0000\n",
            "Epoch 5/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.3692 - accuracy: 0.8324 - false_negatives: 27742.0000\n",
            "Epoch 6/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.3687 - accuracy: 0.8315 - false_negatives: 27944.0000\n",
            "Epoch 7/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.3678 - accuracy: 0.8321 - false_negatives: 28049.0000\n",
            "Epoch 8/20\n",
            "2693/2693 [==============================] - 12s 4ms/step - loss: 0.3668 - accuracy: 0.8333 - false_negatives: 27732.0000\n",
            "Epoch 9/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.3676 - accuracy: 0.8327 - false_negatives: 28005.0000\n",
            "Epoch 10/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3666 - accuracy: 0.8322 - false_negatives: 27879.0000\n",
            "Epoch 11/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.3674 - accuracy: 0.8327 - false_negatives: 27897.0000\n",
            "Epoch 12/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.3673 - accuracy: 0.8329 - false_negatives: 27773.0000\n",
            "Epoch 13/20\n",
            "2693/2693 [==============================] - 9s 3ms/step - loss: 0.3666 - accuracy: 0.8325 - false_negatives: 27832.0000\n",
            "Epoch 14/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.3663 - accuracy: 0.8332 - false_negatives: 27612.0000\n",
            "Epoch 15/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.3659 - accuracy: 0.8327 - false_negatives: 27680.0000\n",
            "Epoch 16/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.3663 - accuracy: 0.8330 - false_negatives: 27466.0000\n",
            "Epoch 17/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.3673 - accuracy: 0.8324 - false_negatives: 27826.0000\n",
            "Epoch 18/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.3666 - accuracy: 0.8322 - false_negatives: 27858.0000\n",
            "Epoch 19/20\n",
            "2693/2693 [==============================] - 11s 4ms/step - loss: 0.3656 - accuracy: 0.8337 - false_negatives: 27416.0000\n",
            "Epoch 20/20\n",
            "2693/2693 [==============================] - 10s 4ms/step - loss: 0.3668 - accuracy: 0.8321 - false_negatives: 27630.0000\n",
            "3867/8414 [============>.................] - ETA: 10s"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search_nn2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "BjDD5sLJrcnX",
        "outputId": "2cfe71bd-a11e-4b3e-e8e8-de67e0ec77e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9380e80ae2bd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_search_nn2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'grid_search_nn2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtest2 = pd.read_csv('xtest2.csv')\n",
        "ytest2 = pd.read_csv('ytest2.csv')"
      ],
      "metadata": {
        "id": "rh2fjbdjMjn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in xtest2.columns: \n",
        "  if ('B_' in i ) or ('S_' in i ):\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7z-hAGCj0Cq",
        "outputId": "301f928a-98cd-4a8d-c640-5275eefc5222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S_3\n",
            "B_17\n",
            "B_39\n",
            "B_1\n",
            "B_3\n",
            "B_2\n",
            "B_9\n",
            "B_5\n",
            "B_38\n",
            "S_23\n",
            "B_7\n",
            "B_4\n",
            "B_10\n",
            "B_8\n",
            "S_7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def default_rate(y, y_pred, threshold):\n",
        "  ydf = pd.DataFrame({'actual': y, 'pred': y_pred})\n",
        "  return ydf.loc[ydf['pred'] < threshold, 'actual'].mean()\n",
        "\n",
        "default_rate(ytrain['target'], y_pred[:, 0], 0.3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENlr5m502cDv",
        "outputId": "13caf16e-8536-4237-c903-ce730b21e47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04838354711103838"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([xtest2, ytest2, y], axis = 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "-PKexV7-rSoo",
        "outputId": "2ee01989-9b1c-4c0c-87f2-82856272ac71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           D_50      R_27       S_3      B_17  B_39      D_48      D_77  \\\n",
              "0           NaN  1.009403  0.159486       NaN   NaN  0.010117  0.224252   \n",
              "1      0.446461  1.007662  0.315459  0.688565   NaN  0.013057  0.404375   \n",
              "2      0.084088  0.024173  0.178912       NaN   NaN  0.956663  0.019463   \n",
              "3           NaN       NaN  0.162269       NaN   NaN       NaN  0.005644   \n",
              "4      0.076798  1.007856  0.105845  0.066857   NaN  0.561627  0.399981   \n",
              "...         ...       ...       ...       ...   ...       ...       ...   \n",
              "88745       NaN       NaN  0.169682       NaN   NaN  0.514328  0.228322   \n",
              "88746       NaN  1.004962  0.152418       NaN   NaN  0.033365  0.105513   \n",
              "88747       NaN       NaN  0.171598       NaN   NaN  0.116904  0.289863   \n",
              "88748  0.022301  1.003392  0.481997  0.822720   NaN  0.780991  0.023681   \n",
              "88749  0.129539  1.004079  0.329960  0.793540   NaN  0.164867  0.446279   \n",
              "\n",
              "           D_75      D_53     D_132       B_1       B_3      D_44       B_2  \\\n",
              "0      0.009073       NaN       NaN  0.034558  0.008733  0.000123  1.005419   \n",
              "1      0.002992  0.006432       NaN  0.014589  0.004804  0.000058  1.009999   \n",
              "2      0.472552  0.183622       NaN  0.056464  0.006187  0.879886  0.811060   \n",
              "3      0.007882  0.165639  0.330559  0.097851  0.009932       NaN  1.007578   \n",
              "4      0.068800       NaN       NaN  0.163463  0.390202  0.006180  0.592315   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "88745  0.207449       NaN       NaN  0.007184  0.002129  0.134245  0.815350   \n",
              "88746  0.006795       NaN       NaN  0.010148  0.007318  0.002964  0.810644   \n",
              "88747  0.009093       NaN       NaN  0.046630  0.009283  0.002747  0.818442   \n",
              "88748  0.401302  0.054901  0.276069  0.522726  0.397499  0.254395  0.031466   \n",
              "88749  0.272967       NaN  0.340736  0.306075  0.231867  0.125308  0.052778   \n",
              "\n",
              "           D_42       B_9      D_49  D_111      D_45  D_88       B_5  D_66  \\\n",
              "0           NaN  0.011734       NaN    NaN  0.263736   NaN  0.010155   NaN   \n",
              "1           NaN  0.034357       NaN    NaN  0.074935   NaN  0.115955   NaN   \n",
              "2           NaN  0.324053       NaN    NaN  0.075384   NaN  0.009856   NaN   \n",
              "3      0.054570  0.141718  0.004841    NaN  0.013244   NaN  0.004746   NaN   \n",
              "4           NaN  0.028810       NaN    NaN  0.130257   NaN  0.004611   1.0   \n",
              "...         ...       ...       ...    ...       ...   ...       ...   ...   \n",
              "88745  0.227061  0.008811       NaN    NaN  0.015907   NaN  0.007407   1.0   \n",
              "88746       NaN  0.002438       NaN    NaN  0.377822   NaN  0.027734   NaN   \n",
              "88747       NaN  0.039968       NaN    NaN  0.070072   NaN  0.008785   NaN   \n",
              "88748       NaN  0.546089  0.356834    NaN  0.266749   NaN  0.022907   NaN   \n",
              "88749       NaN  0.150276  0.020616    NaN  0.613615   NaN  0.016168   NaN   \n",
              "\n",
              "       B_38      S_23      D_43  D_63_CO      D_41      D_51       P_2  \\\n",
              "0       2.0  0.140244  0.062028        1  0.006274  0.340006  0.878856   \n",
              "1       2.0  0.138968  0.054156        1  0.008239  0.008012  0.623392   \n",
              "2       1.0  0.133271  0.098247        1  0.005904  0.338517  0.402733   \n",
              "3       1.0  0.136701       NaN        1  0.007868  0.001938  0.254523   \n",
              "4       3.0  0.135552  0.160972        1  0.005987  0.336024  0.894790   \n",
              "...     ...       ...       ...      ...       ...       ...       ...   \n",
              "88745   1.0  0.131963       NaN        0  0.008801  0.005826  0.581485   \n",
              "88746   2.0  0.132864       NaN        1  0.003385  0.334047  0.878375   \n",
              "88747   1.0  0.137276       NaN        1  0.009255  0.007494  0.839199   \n",
              "88748   7.0  0.136507  0.099630        1  0.007432  0.002057  0.370313   \n",
              "88749   7.0  0.132480  0.520499        1  0.005761  0.342507  0.838860   \n",
              "\n",
              "            B_7      R_26      D_74      D_62       B_4      D_56      B_10  \\\n",
              "0      0.033179       NaN  0.001238  0.238523  0.020876  0.695723  0.301616   \n",
              "1      0.012364       NaN  0.004600  0.435120  0.008486  0.537050  0.401879   \n",
              "2      0.955709  0.074327  0.503210  0.014015  0.611299  0.137859  0.009896   \n",
              "3      0.092894       NaN  0.006534       NaN  0.013345       NaN  0.302234   \n",
              "4      0.038883       NaN  0.000713  0.425841  0.068149  0.846892  0.300981   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "88745  0.242050       NaN  0.216103  0.049188  0.325682       NaN  0.062427   \n",
              "88746  0.017615       NaN  0.000708  0.112248  0.007046  0.170417  0.302207   \n",
              "88747  0.047381       NaN  0.002057  0.302879  0.020095       NaN  0.296129   \n",
              "88748  0.433236       NaN  0.294725  0.005452  0.261137       NaN  0.018993   \n",
              "88749  0.268719       NaN  0.222828  0.484418  0.249961       NaN  0.083528   \n",
              "\n",
              "            B_8      D_46  D_76  D_110       P_3  D_64_O       S_7       R_1  \\\n",
              "0      0.004795  0.470223   NaN    NaN  0.517843       1  0.136180  0.007248   \n",
              "1      1.005542  0.426389   NaN    NaN  0.683057       1  0.305570  0.003768   \n",
              "2      1.001441  0.782748   NaN    NaN  0.471218       1  0.237897  0.502366   \n",
              "3      1.002738       NaN   NaN    NaN       NaN       0  0.223783  1.004001   \n",
              "4      1.000449  0.311131   NaN    NaN  0.731599       1  0.088452  0.009281   \n",
              "...         ...       ...   ...    ...       ...     ...       ...       ...   \n",
              "88745  1.005892  0.605229   NaN    NaN  0.721763       0  0.102842  0.001619   \n",
              "88746  0.008649  0.384367   NaN    NaN  0.634077       1  0.096922  0.005595   \n",
              "88747       NaN       NaN   NaN    NaN       NaN       1  0.109090  0.005582   \n",
              "88748  0.005598  0.451677   NaN    NaN  0.674371       1  0.450288  0.000768   \n",
              "88749  0.008778  0.529416   NaN    NaN  0.541872       0  0.267289  0.007359   \n",
              "\n",
              "       target  \n",
              "0           0  \n",
              "1           0  \n",
              "2           1  \n",
              "3           1  \n",
              "4           0  \n",
              "...       ...  \n",
              "88745       1  \n",
              "88746       0  \n",
              "88747       0  \n",
              "88748       0  \n",
              "88749       0  \n",
              "\n",
              "[88750 rows x 45 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-52990726-436d-4658-bd6f-0ea1a56d364e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D_50</th>\n",
              "      <th>R_27</th>\n",
              "      <th>S_3</th>\n",
              "      <th>B_17</th>\n",
              "      <th>B_39</th>\n",
              "      <th>D_48</th>\n",
              "      <th>D_77</th>\n",
              "      <th>D_75</th>\n",
              "      <th>D_53</th>\n",
              "      <th>D_132</th>\n",
              "      <th>B_1</th>\n",
              "      <th>B_3</th>\n",
              "      <th>D_44</th>\n",
              "      <th>B_2</th>\n",
              "      <th>D_42</th>\n",
              "      <th>B_9</th>\n",
              "      <th>D_49</th>\n",
              "      <th>D_111</th>\n",
              "      <th>D_45</th>\n",
              "      <th>D_88</th>\n",
              "      <th>B_5</th>\n",
              "      <th>D_66</th>\n",
              "      <th>B_38</th>\n",
              "      <th>S_23</th>\n",
              "      <th>D_43</th>\n",
              "      <th>D_63_CO</th>\n",
              "      <th>D_41</th>\n",
              "      <th>D_51</th>\n",
              "      <th>P_2</th>\n",
              "      <th>B_7</th>\n",
              "      <th>R_26</th>\n",
              "      <th>D_74</th>\n",
              "      <th>D_62</th>\n",
              "      <th>B_4</th>\n",
              "      <th>D_56</th>\n",
              "      <th>B_10</th>\n",
              "      <th>B_8</th>\n",
              "      <th>D_46</th>\n",
              "      <th>D_76</th>\n",
              "      <th>D_110</th>\n",
              "      <th>P_3</th>\n",
              "      <th>D_64_O</th>\n",
              "      <th>S_7</th>\n",
              "      <th>R_1</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.009403</td>\n",
              "      <td>0.159486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010117</td>\n",
              "      <td>0.224252</td>\n",
              "      <td>0.009073</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.034558</td>\n",
              "      <td>0.008733</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>1.005419</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.011734</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.263736</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010155</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.140244</td>\n",
              "      <td>0.062028</td>\n",
              "      <td>1</td>\n",
              "      <td>0.006274</td>\n",
              "      <td>0.340006</td>\n",
              "      <td>0.878856</td>\n",
              "      <td>0.033179</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.238523</td>\n",
              "      <td>0.020876</td>\n",
              "      <td>0.695723</td>\n",
              "      <td>0.301616</td>\n",
              "      <td>0.004795</td>\n",
              "      <td>0.470223</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.517843</td>\n",
              "      <td>1</td>\n",
              "      <td>0.136180</td>\n",
              "      <td>0.007248</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.446461</td>\n",
              "      <td>1.007662</td>\n",
              "      <td>0.315459</td>\n",
              "      <td>0.688565</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.013057</td>\n",
              "      <td>0.404375</td>\n",
              "      <td>0.002992</td>\n",
              "      <td>0.006432</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.014589</td>\n",
              "      <td>0.004804</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>1.009999</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.034357</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.074935</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.115955</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.138968</td>\n",
              "      <td>0.054156</td>\n",
              "      <td>1</td>\n",
              "      <td>0.008239</td>\n",
              "      <td>0.008012</td>\n",
              "      <td>0.623392</td>\n",
              "      <td>0.012364</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004600</td>\n",
              "      <td>0.435120</td>\n",
              "      <td>0.008486</td>\n",
              "      <td>0.537050</td>\n",
              "      <td>0.401879</td>\n",
              "      <td>1.005542</td>\n",
              "      <td>0.426389</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.683057</td>\n",
              "      <td>1</td>\n",
              "      <td>0.305570</td>\n",
              "      <td>0.003768</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.084088</td>\n",
              "      <td>0.024173</td>\n",
              "      <td>0.178912</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.956663</td>\n",
              "      <td>0.019463</td>\n",
              "      <td>0.472552</td>\n",
              "      <td>0.183622</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.056464</td>\n",
              "      <td>0.006187</td>\n",
              "      <td>0.879886</td>\n",
              "      <td>0.811060</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.324053</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.075384</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009856</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.133271</td>\n",
              "      <td>0.098247</td>\n",
              "      <td>1</td>\n",
              "      <td>0.005904</td>\n",
              "      <td>0.338517</td>\n",
              "      <td>0.402733</td>\n",
              "      <td>0.955709</td>\n",
              "      <td>0.074327</td>\n",
              "      <td>0.503210</td>\n",
              "      <td>0.014015</td>\n",
              "      <td>0.611299</td>\n",
              "      <td>0.137859</td>\n",
              "      <td>0.009896</td>\n",
              "      <td>1.001441</td>\n",
              "      <td>0.782748</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.471218</td>\n",
              "      <td>1</td>\n",
              "      <td>0.237897</td>\n",
              "      <td>0.502366</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.162269</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005644</td>\n",
              "      <td>0.007882</td>\n",
              "      <td>0.165639</td>\n",
              "      <td>0.330559</td>\n",
              "      <td>0.097851</td>\n",
              "      <td>0.009932</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.007578</td>\n",
              "      <td>0.054570</td>\n",
              "      <td>0.141718</td>\n",
              "      <td>0.004841</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.013244</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004746</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.136701</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.007868</td>\n",
              "      <td>0.001938</td>\n",
              "      <td>0.254523</td>\n",
              "      <td>0.092894</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.006534</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.013345</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.302234</td>\n",
              "      <td>1.002738</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.223783</td>\n",
              "      <td>1.004001</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.076798</td>\n",
              "      <td>1.007856</td>\n",
              "      <td>0.105845</td>\n",
              "      <td>0.066857</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.561627</td>\n",
              "      <td>0.399981</td>\n",
              "      <td>0.068800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.163463</td>\n",
              "      <td>0.390202</td>\n",
              "      <td>0.006180</td>\n",
              "      <td>0.592315</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.028810</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.130257</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004611</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.135552</td>\n",
              "      <td>0.160972</td>\n",
              "      <td>1</td>\n",
              "      <td>0.005987</td>\n",
              "      <td>0.336024</td>\n",
              "      <td>0.894790</td>\n",
              "      <td>0.038883</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000713</td>\n",
              "      <td>0.425841</td>\n",
              "      <td>0.068149</td>\n",
              "      <td>0.846892</td>\n",
              "      <td>0.300981</td>\n",
              "      <td>1.000449</td>\n",
              "      <td>0.311131</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.731599</td>\n",
              "      <td>1</td>\n",
              "      <td>0.088452</td>\n",
              "      <td>0.009281</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88745</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.169682</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.514328</td>\n",
              "      <td>0.228322</td>\n",
              "      <td>0.207449</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007184</td>\n",
              "      <td>0.002129</td>\n",
              "      <td>0.134245</td>\n",
              "      <td>0.815350</td>\n",
              "      <td>0.227061</td>\n",
              "      <td>0.008811</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.015907</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.007407</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.131963</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.008801</td>\n",
              "      <td>0.005826</td>\n",
              "      <td>0.581485</td>\n",
              "      <td>0.242050</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.216103</td>\n",
              "      <td>0.049188</td>\n",
              "      <td>0.325682</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.062427</td>\n",
              "      <td>1.005892</td>\n",
              "      <td>0.605229</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.721763</td>\n",
              "      <td>0</td>\n",
              "      <td>0.102842</td>\n",
              "      <td>0.001619</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88746</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.004962</td>\n",
              "      <td>0.152418</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.033365</td>\n",
              "      <td>0.105513</td>\n",
              "      <td>0.006795</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010148</td>\n",
              "      <td>0.007318</td>\n",
              "      <td>0.002964</td>\n",
              "      <td>0.810644</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002438</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.377822</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.027734</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.132864</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.003385</td>\n",
              "      <td>0.334047</td>\n",
              "      <td>0.878375</td>\n",
              "      <td>0.017615</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000708</td>\n",
              "      <td>0.112248</td>\n",
              "      <td>0.007046</td>\n",
              "      <td>0.170417</td>\n",
              "      <td>0.302207</td>\n",
              "      <td>0.008649</td>\n",
              "      <td>0.384367</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.634077</td>\n",
              "      <td>1</td>\n",
              "      <td>0.096922</td>\n",
              "      <td>0.005595</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88747</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.171598</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.116904</td>\n",
              "      <td>0.289863</td>\n",
              "      <td>0.009093</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.046630</td>\n",
              "      <td>0.009283</td>\n",
              "      <td>0.002747</td>\n",
              "      <td>0.818442</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.039968</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.070072</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.008785</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.137276</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.009255</td>\n",
              "      <td>0.007494</td>\n",
              "      <td>0.839199</td>\n",
              "      <td>0.047381</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.002057</td>\n",
              "      <td>0.302879</td>\n",
              "      <td>0.020095</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.296129</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.109090</td>\n",
              "      <td>0.005582</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88748</th>\n",
              "      <td>0.022301</td>\n",
              "      <td>1.003392</td>\n",
              "      <td>0.481997</td>\n",
              "      <td>0.822720</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.780991</td>\n",
              "      <td>0.023681</td>\n",
              "      <td>0.401302</td>\n",
              "      <td>0.054901</td>\n",
              "      <td>0.276069</td>\n",
              "      <td>0.522726</td>\n",
              "      <td>0.397499</td>\n",
              "      <td>0.254395</td>\n",
              "      <td>0.031466</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.546089</td>\n",
              "      <td>0.356834</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.266749</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.022907</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.136507</td>\n",
              "      <td>0.099630</td>\n",
              "      <td>1</td>\n",
              "      <td>0.007432</td>\n",
              "      <td>0.002057</td>\n",
              "      <td>0.370313</td>\n",
              "      <td>0.433236</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.294725</td>\n",
              "      <td>0.005452</td>\n",
              "      <td>0.261137</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.018993</td>\n",
              "      <td>0.005598</td>\n",
              "      <td>0.451677</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.674371</td>\n",
              "      <td>1</td>\n",
              "      <td>0.450288</td>\n",
              "      <td>0.000768</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88749</th>\n",
              "      <td>0.129539</td>\n",
              "      <td>1.004079</td>\n",
              "      <td>0.329960</td>\n",
              "      <td>0.793540</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.164867</td>\n",
              "      <td>0.446279</td>\n",
              "      <td>0.272967</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.340736</td>\n",
              "      <td>0.306075</td>\n",
              "      <td>0.231867</td>\n",
              "      <td>0.125308</td>\n",
              "      <td>0.052778</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.150276</td>\n",
              "      <td>0.020616</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.613615</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.016168</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.132480</td>\n",
              "      <td>0.520499</td>\n",
              "      <td>1</td>\n",
              "      <td>0.005761</td>\n",
              "      <td>0.342507</td>\n",
              "      <td>0.838860</td>\n",
              "      <td>0.268719</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.222828</td>\n",
              "      <td>0.484418</td>\n",
              "      <td>0.249961</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.083528</td>\n",
              "      <td>0.008778</td>\n",
              "      <td>0.529416</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.541872</td>\n",
              "      <td>0</td>\n",
              "      <td>0.267289</td>\n",
              "      <td>0.007359</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>88750 rows × 45 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52990726-436d-4658-bd6f-0ea1a56d364e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-52990726-436d-4658-bd6f-0ea1a56d364e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-52990726-436d-4658-bd6f-0ea1a56d364e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Default Rate {} & Revenue {}\".format(0.001, 23.23))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDb8IQsjQ8Zm",
        "outputId": "91b663cd-5aea-4af8-b45d-aa8269b51b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default Rate 0.001 & Revenue 23.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtest2.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "hDr-KOguSCkC",
        "outputId": "4ce9afcf-994b-4135-d42e-7799ed8c5816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       D_50      R_27       S_3      B_17  B_39      D_48      D_77      D_75  \\\n",
              "0       NaN  1.009403  0.159486       NaN   NaN  0.010117  0.224252  0.009073   \n",
              "1  0.446461  1.007662  0.315459  0.688565   NaN  0.013057  0.404375  0.002992   \n",
              "2  0.084088  0.024173  0.178912       NaN   NaN  0.956663  0.019463  0.472552   \n",
              "3       NaN       NaN  0.162269       NaN   NaN       NaN  0.005644  0.007882   \n",
              "4  0.076798  1.007856  0.105845  0.066857   NaN  0.561627  0.399981  0.068800   \n",
              "\n",
              "       D_53     D_132       B_1       B_3      D_44       B_2     D_42  \\\n",
              "0       NaN       NaN  0.034558  0.008733  0.000159  1.005419      NaN   \n",
              "1  0.006432       NaN  0.014589  0.004804  0.000159  1.009655      NaN   \n",
              "2  0.183622       NaN  0.056464  0.006187  0.879886  0.811060      NaN   \n",
              "3  0.165639  0.330559  0.097851  0.009932       NaN  1.007578  0.05457   \n",
              "4       NaN       NaN  0.163463  0.390202  0.006180  0.592315      NaN   \n",
              "\n",
              "        B_9      D_49  D_111      D_45  D_88       B_5  D_66  B_38      S_23  \\\n",
              "0  0.011734       NaN    NaN  0.263736   NaN  0.010155   NaN   2.0  0.140244   \n",
              "1  0.034357       NaN    NaN  0.074935   NaN  0.115955   NaN   2.0  0.138968   \n",
              "2  0.324053       NaN    NaN  0.075384   NaN  0.009856   NaN   1.0  0.133271   \n",
              "3  0.141718  0.004841    NaN  0.013244   NaN  0.004746   NaN   1.0  0.136701   \n",
              "4  0.028810       NaN    NaN  0.130257   NaN  0.004611   1.0   3.0  0.135552   \n",
              "\n",
              "       D_43  D_63_CO      D_41      D_51       P_2       B_7      R_26  \\\n",
              "0  0.062028      1.0  0.006274  0.340006  0.878856  0.033179       NaN   \n",
              "1  0.054156      1.0  0.008239  0.008012  0.623392  0.012364       NaN   \n",
              "2  0.098247      1.0  0.005904  0.338517  0.402733  0.955709  0.074327   \n",
              "3       NaN      1.0  0.007868  0.001938  0.254523  0.092894       NaN   \n",
              "4  0.160972      1.0  0.005987  0.336024  0.894790  0.038883       NaN   \n",
              "\n",
              "       D_74      D_62       B_4      D_56      B_10       B_8      D_46  D_76  \\\n",
              "0  0.001238  0.238523  0.020876  0.695723  0.301616  0.004795  0.470223   NaN   \n",
              "1  0.004600  0.435120  0.008486  0.537050  0.401879  1.005542  0.426389   NaN   \n",
              "2  0.503210  0.014015  0.611299  0.137859  0.009896  1.001441  0.782748   NaN   \n",
              "3  0.006534       NaN  0.013345       NaN  0.302234  1.002738       NaN   NaN   \n",
              "4  0.000713  0.425841  0.068149  0.846892  0.300981  1.000449  0.311131   NaN   \n",
              "\n",
              "   D_110       P_3  D_64_O       S_7       R_1  \n",
              "0    NaN  0.517843     1.0  0.136180  0.007248  \n",
              "1    NaN  0.683057     1.0  0.305570  0.003768  \n",
              "2    NaN  0.471218     1.0  0.237897  0.502366  \n",
              "3    NaN       NaN     0.0  0.223783  1.004001  \n",
              "4    NaN  0.731599     1.0  0.088452  0.009281  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b7de271-0aa5-47bb-baa2-39e5641e83a5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D_50</th>\n",
              "      <th>R_27</th>\n",
              "      <th>S_3</th>\n",
              "      <th>B_17</th>\n",
              "      <th>B_39</th>\n",
              "      <th>D_48</th>\n",
              "      <th>D_77</th>\n",
              "      <th>D_75</th>\n",
              "      <th>D_53</th>\n",
              "      <th>D_132</th>\n",
              "      <th>B_1</th>\n",
              "      <th>B_3</th>\n",
              "      <th>D_44</th>\n",
              "      <th>B_2</th>\n",
              "      <th>D_42</th>\n",
              "      <th>B_9</th>\n",
              "      <th>D_49</th>\n",
              "      <th>D_111</th>\n",
              "      <th>D_45</th>\n",
              "      <th>D_88</th>\n",
              "      <th>B_5</th>\n",
              "      <th>D_66</th>\n",
              "      <th>B_38</th>\n",
              "      <th>S_23</th>\n",
              "      <th>D_43</th>\n",
              "      <th>D_63_CO</th>\n",
              "      <th>D_41</th>\n",
              "      <th>D_51</th>\n",
              "      <th>P_2</th>\n",
              "      <th>B_7</th>\n",
              "      <th>R_26</th>\n",
              "      <th>D_74</th>\n",
              "      <th>D_62</th>\n",
              "      <th>B_4</th>\n",
              "      <th>D_56</th>\n",
              "      <th>B_10</th>\n",
              "      <th>B_8</th>\n",
              "      <th>D_46</th>\n",
              "      <th>D_76</th>\n",
              "      <th>D_110</th>\n",
              "      <th>P_3</th>\n",
              "      <th>D_64_O</th>\n",
              "      <th>S_7</th>\n",
              "      <th>R_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.009403</td>\n",
              "      <td>0.159486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010117</td>\n",
              "      <td>0.224252</td>\n",
              "      <td>0.009073</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.034558</td>\n",
              "      <td>0.008733</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>1.005419</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.011734</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.263736</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010155</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.140244</td>\n",
              "      <td>0.062028</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.006274</td>\n",
              "      <td>0.340006</td>\n",
              "      <td>0.878856</td>\n",
              "      <td>0.033179</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.238523</td>\n",
              "      <td>0.020876</td>\n",
              "      <td>0.695723</td>\n",
              "      <td>0.301616</td>\n",
              "      <td>0.004795</td>\n",
              "      <td>0.470223</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.517843</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.136180</td>\n",
              "      <td>0.007248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.446461</td>\n",
              "      <td>1.007662</td>\n",
              "      <td>0.315459</td>\n",
              "      <td>0.688565</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.013057</td>\n",
              "      <td>0.404375</td>\n",
              "      <td>0.002992</td>\n",
              "      <td>0.006432</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.014589</td>\n",
              "      <td>0.004804</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>1.009655</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.034357</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.074935</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.115955</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.138968</td>\n",
              "      <td>0.054156</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.008239</td>\n",
              "      <td>0.008012</td>\n",
              "      <td>0.623392</td>\n",
              "      <td>0.012364</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004600</td>\n",
              "      <td>0.435120</td>\n",
              "      <td>0.008486</td>\n",
              "      <td>0.537050</td>\n",
              "      <td>0.401879</td>\n",
              "      <td>1.005542</td>\n",
              "      <td>0.426389</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.683057</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.305570</td>\n",
              "      <td>0.003768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.084088</td>\n",
              "      <td>0.024173</td>\n",
              "      <td>0.178912</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.956663</td>\n",
              "      <td>0.019463</td>\n",
              "      <td>0.472552</td>\n",
              "      <td>0.183622</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.056464</td>\n",
              "      <td>0.006187</td>\n",
              "      <td>0.879886</td>\n",
              "      <td>0.811060</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.324053</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.075384</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009856</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.133271</td>\n",
              "      <td>0.098247</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.005904</td>\n",
              "      <td>0.338517</td>\n",
              "      <td>0.402733</td>\n",
              "      <td>0.955709</td>\n",
              "      <td>0.074327</td>\n",
              "      <td>0.503210</td>\n",
              "      <td>0.014015</td>\n",
              "      <td>0.611299</td>\n",
              "      <td>0.137859</td>\n",
              "      <td>0.009896</td>\n",
              "      <td>1.001441</td>\n",
              "      <td>0.782748</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.471218</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.237897</td>\n",
              "      <td>0.502366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.162269</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005644</td>\n",
              "      <td>0.007882</td>\n",
              "      <td>0.165639</td>\n",
              "      <td>0.330559</td>\n",
              "      <td>0.097851</td>\n",
              "      <td>0.009932</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.007578</td>\n",
              "      <td>0.05457</td>\n",
              "      <td>0.141718</td>\n",
              "      <td>0.004841</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.013244</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004746</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.136701</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.007868</td>\n",
              "      <td>0.001938</td>\n",
              "      <td>0.254523</td>\n",
              "      <td>0.092894</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.006534</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.013345</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.302234</td>\n",
              "      <td>1.002738</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.223783</td>\n",
              "      <td>1.004001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.076798</td>\n",
              "      <td>1.007856</td>\n",
              "      <td>0.105845</td>\n",
              "      <td>0.066857</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.561627</td>\n",
              "      <td>0.399981</td>\n",
              "      <td>0.068800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.163463</td>\n",
              "      <td>0.390202</td>\n",
              "      <td>0.006180</td>\n",
              "      <td>0.592315</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.028810</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.130257</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004611</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.135552</td>\n",
              "      <td>0.160972</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.005987</td>\n",
              "      <td>0.336024</td>\n",
              "      <td>0.894790</td>\n",
              "      <td>0.038883</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000713</td>\n",
              "      <td>0.425841</td>\n",
              "      <td>0.068149</td>\n",
              "      <td>0.846892</td>\n",
              "      <td>0.300981</td>\n",
              "      <td>1.000449</td>\n",
              "      <td>0.311131</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.731599</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.088452</td>\n",
              "      <td>0.009281</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b7de271-0aa5-47bb-baa2-39e5641e83a5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b7de271-0aa5-47bb-baa2-39e5641e83a5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b7de271-0aa5-47bb-baa2-39e5641e83a5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtest2['trial'] = xtest2.apply(lambda x : x['B_2']*0.02+ x['S_3']*0.001, axis =1)\n",
        "xtest2.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "uDHbm-JUSFEG",
        "outputId": "47428164-6b26-4534-8897-0e1548ad86b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       D_50      R_27       S_3      B_17  B_39      D_48      D_77      D_75  \\\n",
              "0       NaN  1.009403  0.159486       NaN   NaN  0.010117  0.224252  0.009073   \n",
              "1  0.446461  1.007662  0.315459  0.688565   NaN  0.013057  0.404375  0.002992   \n",
              "2  0.084088  0.024173  0.178912       NaN   NaN  0.956663  0.019463  0.472552   \n",
              "3       NaN       NaN  0.162269       NaN   NaN       NaN  0.005644  0.007882   \n",
              "4  0.076798  1.007856  0.105845  0.066857   NaN  0.561627  0.399981  0.068800   \n",
              "\n",
              "       D_53     D_132       B_1       B_3      D_44       B_2     D_42  \\\n",
              "0       NaN       NaN  0.034558  0.008733  0.000159  1.005419      NaN   \n",
              "1  0.006432       NaN  0.014589  0.004804  0.000159  1.009655      NaN   \n",
              "2  0.183622       NaN  0.056464  0.006187  0.879886  0.811060      NaN   \n",
              "3  0.165639  0.330559  0.097851  0.009932       NaN  1.007578  0.05457   \n",
              "4       NaN       NaN  0.163463  0.390202  0.006180  0.592315      NaN   \n",
              "\n",
              "        B_9      D_49  D_111      D_45  D_88       B_5  D_66  B_38      S_23  \\\n",
              "0  0.011734       NaN    NaN  0.263736   NaN  0.010155   NaN   2.0  0.140244   \n",
              "1  0.034357       NaN    NaN  0.074935   NaN  0.115955   NaN   2.0  0.138968   \n",
              "2  0.324053       NaN    NaN  0.075384   NaN  0.009856   NaN   1.0  0.133271   \n",
              "3  0.141718  0.004841    NaN  0.013244   NaN  0.004746   NaN   1.0  0.136701   \n",
              "4  0.028810       NaN    NaN  0.130257   NaN  0.004611   1.0   3.0  0.135552   \n",
              "\n",
              "       D_43  D_63_CO      D_41      D_51       P_2       B_7      R_26  \\\n",
              "0  0.062028      1.0  0.006274  0.340006  0.878856  0.033179       NaN   \n",
              "1  0.054156      1.0  0.008239  0.008012  0.623392  0.012364       NaN   \n",
              "2  0.098247      1.0  0.005904  0.338517  0.402733  0.955709  0.074327   \n",
              "3       NaN      1.0  0.007868  0.001938  0.254523  0.092894       NaN   \n",
              "4  0.160972      1.0  0.005987  0.336024  0.894790  0.038883       NaN   \n",
              "\n",
              "       D_74      D_62       B_4      D_56      B_10       B_8      D_46  D_76  \\\n",
              "0  0.001238  0.238523  0.020876  0.695723  0.301616  0.004795  0.470223   NaN   \n",
              "1  0.004600  0.435120  0.008486  0.537050  0.401879  1.005542  0.426389   NaN   \n",
              "2  0.503210  0.014015  0.611299  0.137859  0.009896  1.001441  0.782748   NaN   \n",
              "3  0.006534       NaN  0.013345       NaN  0.302234  1.002738       NaN   NaN   \n",
              "4  0.000713  0.425841  0.068149  0.846892  0.300981  1.000449  0.311131   NaN   \n",
              "\n",
              "   D_110       P_3  D_64_O       S_7       R_1     trial  \n",
              "0    NaN  0.517843     1.0  0.136180  0.007248  0.020268  \n",
              "1    NaN  0.683057     1.0  0.305570  0.003768  0.020509  \n",
              "2    NaN  0.471218     1.0  0.237897  0.502366  0.016400  \n",
              "3    NaN       NaN     0.0  0.223783  1.004001  0.020314  \n",
              "4    NaN  0.731599     1.0  0.088452  0.009281  0.011952  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-37619ef6-1e49-4ee4-8da6-f2dfef0b59fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D_50</th>\n",
              "      <th>R_27</th>\n",
              "      <th>S_3</th>\n",
              "      <th>B_17</th>\n",
              "      <th>B_39</th>\n",
              "      <th>D_48</th>\n",
              "      <th>D_77</th>\n",
              "      <th>D_75</th>\n",
              "      <th>D_53</th>\n",
              "      <th>D_132</th>\n",
              "      <th>B_1</th>\n",
              "      <th>B_3</th>\n",
              "      <th>D_44</th>\n",
              "      <th>B_2</th>\n",
              "      <th>D_42</th>\n",
              "      <th>B_9</th>\n",
              "      <th>D_49</th>\n",
              "      <th>D_111</th>\n",
              "      <th>D_45</th>\n",
              "      <th>D_88</th>\n",
              "      <th>B_5</th>\n",
              "      <th>D_66</th>\n",
              "      <th>B_38</th>\n",
              "      <th>S_23</th>\n",
              "      <th>D_43</th>\n",
              "      <th>D_63_CO</th>\n",
              "      <th>D_41</th>\n",
              "      <th>D_51</th>\n",
              "      <th>P_2</th>\n",
              "      <th>B_7</th>\n",
              "      <th>R_26</th>\n",
              "      <th>D_74</th>\n",
              "      <th>D_62</th>\n",
              "      <th>B_4</th>\n",
              "      <th>D_56</th>\n",
              "      <th>B_10</th>\n",
              "      <th>B_8</th>\n",
              "      <th>D_46</th>\n",
              "      <th>D_76</th>\n",
              "      <th>D_110</th>\n",
              "      <th>P_3</th>\n",
              "      <th>D_64_O</th>\n",
              "      <th>S_7</th>\n",
              "      <th>R_1</th>\n",
              "      <th>trial</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.009403</td>\n",
              "      <td>0.159486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010117</td>\n",
              "      <td>0.224252</td>\n",
              "      <td>0.009073</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.034558</td>\n",
              "      <td>0.008733</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>1.005419</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.011734</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.263736</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.010155</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.140244</td>\n",
              "      <td>0.062028</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.006274</td>\n",
              "      <td>0.340006</td>\n",
              "      <td>0.878856</td>\n",
              "      <td>0.033179</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.238523</td>\n",
              "      <td>0.020876</td>\n",
              "      <td>0.695723</td>\n",
              "      <td>0.301616</td>\n",
              "      <td>0.004795</td>\n",
              "      <td>0.470223</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.517843</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.136180</td>\n",
              "      <td>0.007248</td>\n",
              "      <td>0.020268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.446461</td>\n",
              "      <td>1.007662</td>\n",
              "      <td>0.315459</td>\n",
              "      <td>0.688565</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.013057</td>\n",
              "      <td>0.404375</td>\n",
              "      <td>0.002992</td>\n",
              "      <td>0.006432</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.014589</td>\n",
              "      <td>0.004804</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>1.009655</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.034357</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.074935</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.115955</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.138968</td>\n",
              "      <td>0.054156</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.008239</td>\n",
              "      <td>0.008012</td>\n",
              "      <td>0.623392</td>\n",
              "      <td>0.012364</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004600</td>\n",
              "      <td>0.435120</td>\n",
              "      <td>0.008486</td>\n",
              "      <td>0.537050</td>\n",
              "      <td>0.401879</td>\n",
              "      <td>1.005542</td>\n",
              "      <td>0.426389</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.683057</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.305570</td>\n",
              "      <td>0.003768</td>\n",
              "      <td>0.020509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.084088</td>\n",
              "      <td>0.024173</td>\n",
              "      <td>0.178912</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.956663</td>\n",
              "      <td>0.019463</td>\n",
              "      <td>0.472552</td>\n",
              "      <td>0.183622</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.056464</td>\n",
              "      <td>0.006187</td>\n",
              "      <td>0.879886</td>\n",
              "      <td>0.811060</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.324053</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.075384</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009856</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.133271</td>\n",
              "      <td>0.098247</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.005904</td>\n",
              "      <td>0.338517</td>\n",
              "      <td>0.402733</td>\n",
              "      <td>0.955709</td>\n",
              "      <td>0.074327</td>\n",
              "      <td>0.503210</td>\n",
              "      <td>0.014015</td>\n",
              "      <td>0.611299</td>\n",
              "      <td>0.137859</td>\n",
              "      <td>0.009896</td>\n",
              "      <td>1.001441</td>\n",
              "      <td>0.782748</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.471218</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.237897</td>\n",
              "      <td>0.502366</td>\n",
              "      <td>0.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.162269</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005644</td>\n",
              "      <td>0.007882</td>\n",
              "      <td>0.165639</td>\n",
              "      <td>0.330559</td>\n",
              "      <td>0.097851</td>\n",
              "      <td>0.009932</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.007578</td>\n",
              "      <td>0.05457</td>\n",
              "      <td>0.141718</td>\n",
              "      <td>0.004841</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.013244</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004746</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.136701</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.007868</td>\n",
              "      <td>0.001938</td>\n",
              "      <td>0.254523</td>\n",
              "      <td>0.092894</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.006534</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.013345</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.302234</td>\n",
              "      <td>1.002738</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.223783</td>\n",
              "      <td>1.004001</td>\n",
              "      <td>0.020314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.076798</td>\n",
              "      <td>1.007856</td>\n",
              "      <td>0.105845</td>\n",
              "      <td>0.066857</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.561627</td>\n",
              "      <td>0.399981</td>\n",
              "      <td>0.068800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.163463</td>\n",
              "      <td>0.390202</td>\n",
              "      <td>0.006180</td>\n",
              "      <td>0.592315</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.028810</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.130257</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.004611</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.135552</td>\n",
              "      <td>0.160972</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.005987</td>\n",
              "      <td>0.336024</td>\n",
              "      <td>0.894790</td>\n",
              "      <td>0.038883</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000713</td>\n",
              "      <td>0.425841</td>\n",
              "      <td>0.068149</td>\n",
              "      <td>0.846892</td>\n",
              "      <td>0.300981</td>\n",
              "      <td>1.000449</td>\n",
              "      <td>0.311131</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.731599</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.088452</td>\n",
              "      <td>0.009281</td>\n",
              "      <td>0.011952</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37619ef6-1e49-4ee4-8da6-f2dfef0b59fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-37619ef6-1e49-4ee4-8da6-f2dfef0b59fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-37619ef6-1e49-4ee4-8da6-f2dfef0b59fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def strategy(x, actual, pred, threshold, balance = 'B_2', spend = 'S_3'):\n",
        "  ydf = pd.DataFrame({'actual': actual, 'pred': pred})\n",
        "  df = pd.concat([x, ydf], axis = 1)\n",
        "  default = df.loc[ydf['pred'] < threshold, 'actual'].mean()\n",
        "\n",
        "  df['revenue'] = df.apply(lambda x: x[balance]*0.02 + x[spend]*.001 if x['actual'] != 1 else 0, axis = 1)\n",
        "\n",
        "  revenue = df.loc[df['pred'] < threshold, 'revenue'].sum()\n",
        "\n",
        "  return \"Default Rate {} & Revenue {}\".format(default, revenue)\n",
        "\n",
        "\n",
        "strategy(xtest2, ytest2['target'], y_pred[:,0], 0.7, balance = 'B_2', spend = 'S_3')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WT9ZB-FEj9Qk",
        "outputId": "4035e44d-4a17-4eaa-accc-57e95fb599a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Default Rate 0.14032093342100477 & Revenue 763.6994176726243'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dcdBjtYSRymV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}